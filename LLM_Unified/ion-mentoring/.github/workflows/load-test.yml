name: 부하 테스트 (Scheduled Load Testing)

on:
    # 매일 오전 3시(KST 12시) UTC 기준 실행
    schedule:
        - cron: "0 3 * * *"

    # 수동 실행 지원
    workflow_dispatch:
        inputs:
            test_duration:
                description: "테스트 지속 시간 (분)"
                required: false
                default: "5"
            max_users:
                description: "최대 동시 사용자 수"
                required: false
                default: "200"

env:
    API_HOST: https://ion-api-64076350717.us-central1.run.app
    PYTHON_VERSION: "3.13"

jobs:
    load-test:
        name: Locust 부하 테스트
        runs-on: ubuntu-latest
        timeout-minutes: 30

        steps:
            - name: 코드 체크아웃
              uses: actions/checkout@v4

            - name: Python ${{ env.PYTHON_VERSION }} 설정
              uses: actions/setup-python@v4
              with:
                  python-version: ${{ env.PYTHON_VERSION }}
                  cache: "pip"

            - name: 의존성 설치
              run: |
                  python -m pip install --upgrade pip
                  pip install -r ion-mentoring/requirements-api.txt

            - name: 출력 디렉토리 생성
              run: mkdir -p ion-mentoring/outputs

            - name: Light 시나리오 실행
              working-directory: ion-mentoring
              run: |
                  python -m locust -f load_test.py \
                    --host=${{ env.API_HOST }} \
                    --users 10 \
                    --spawn-rate 1 \
                    --run-time 2m \
                    --headless \
                    --csv=outputs/load_test_light \
                    --html=outputs/load_test_light.html

            - name: Medium 시나리오 실행
              working-directory: ion-mentoring
              run: |
                  sleep 30
                  python -m locust -f load_test.py \
                    --host=${{ env.API_HOST }} \
                    --users 50 \
                    --spawn-rate 5 \
                    --run-time 3m \
                    --headless \
                    --csv=outputs/load_test_medium \
                    --html=outputs/load_test_medium.html

            - name: Heavy 시나리오 실행
              working-directory: ion-mentoring
              run: |
                  sleep 30
                  python -m locust -f load_test.py \
                    --host=${{ env.API_HOST }} \
                    --users 100 \
                    --spawn-rate 10 \
                    --run-time 5m \
                    --headless \
                    --csv=outputs/load_test_heavy \
                    --html=outputs/load_test_heavy.html

            - name: Stress 시나리오 실행
              working-directory: ion-mentoring
              run: |
                  sleep 30
                  python -m locust -f load_test.py \
                    --host=${{ env.API_HOST }} \
                    --users ${{ github.event.inputs.max_users || '200' }} \
                    --spawn-rate 20 \
                    --run-time ${{ github.event.inputs.test_duration || '10' }}m \
                    --headless \
                    --csv=outputs/load_test_stress \
                    --html=outputs/load_test_stress.html

            - name: 테스트 결과 요약 생성
              if: always()
              working-directory: ion-mentoring
              run: |
                  cat > outputs/test_summary.md << 'EOF'
                  # 부하 테스트 결과 요약

                  **실행 시간**: $(date -u +'%Y-%m-%d %H:%M:%S UTC')
                  **브랜치**: ${{ github.ref_name }}
                  **커밋**: ${{ github.sha }}

                  ## 시나리오 결과

                  ### Light (10 users, 2min)
                  - CSV: [load_test_light_stats.csv](./load_test_light_stats.csv)
                  - HTML: [load_test_light.html](./load_test_light.html)

                  ### Medium (50 users, 3min)
                  - CSV: [load_test_medium_stats.csv](./load_test_medium_stats.csv)
                  - HTML: [load_test_medium.html](./load_test_medium.html)

                  ### Heavy (100 users, 5min)
                  - CSV: [load_test_heavy_stats.csv](./load_test_heavy_stats.csv)
                  - HTML: [load_test_heavy.html](./load_test_heavy.html)

                  ### Stress (${{ github.event.inputs.max_users || '200' }} users, ${{ github.event.inputs.test_duration || '10' }}min)
                  - CSV: [load_test_stress_stats.csv](./load_test_stress_stats.csv)
                  - HTML: [load_test_stress.html](./load_test_stress.html)

                  ## 다음 단계

                  1. HTML 리포트에서 상세 지표 확인
                  2. P95/P99 latency가 목표치 초과 시 성능 최적화
                  3. Cloud Monitoring 대시보드에서 인스턴스 스케일링 검토
                  EOF

            - name: 테스트 결과 아티팩트 업로드
              if: always()
              uses: actions/upload-artifact@v4
              with:
                  name: load-test-results-${{ github.run_number }}
                  path: |
                      ion-mentoring/outputs/*.csv
                      ion-mentoring/outputs/*.html
                      ion-mentoring/outputs/test_summary.md
                  retention-days: 30

            - name: 실패 시 Slack 알림 (선택사항)
              if: failure()
              run: |
                  echo "부하 테스트 실패! 상세 내용은 GitHub Actions 로그를 확인하세요."
                  # Slack webhook 설정 시 활성화:
                  # curl -X POST -H 'Content-type: application/json' \
                  #   --data '{"text":"⚠️ Ion API 부하 테스트 실패\n워크플로우: ${{ github.workflow }}\n실행: ${{ github.run_id }}"}' \
                  #   ${{ secrets.SLACK_WEBHOOK_URL }}

    analyze:
        name: 결과 분석 및 리포트
        runs-on: ubuntu-latest
        needs: load-test
        if: always()

        steps:
            - name: 아티팩트 다운로드
              uses: actions/download-artifact@v4
              with:
                  name: load-test-results-${{ github.run_number }}
                  path: results

            - name: 성능 지표 추출 (Python)
              run: |
                  cat > analyze.py << 'EOF'
                  import csv
                  import json
                  from pathlib import Path

                  results_dir = Path("results")
                  scenarios = ["light", "medium", "heavy", "stress"]

                  summary = {"scenarios": {}}

                  for scenario in scenarios:
                      stats_file = results_dir / f"load_test_{scenario}_stats.csv"
                      if not stats_file.exists():
                          continue
                          
                      with open(stats_file, 'r') as f:
                          reader = csv.DictReader(f)
                          rows = list(reader)
                          if not rows:
                              continue
                          
                          # Aggregated row (마지막 행)
                          agg = rows[-1]
                          summary["scenarios"][scenario] = {
                              "total_requests": int(agg.get("Request Count", 0)),
                              "failures": int(agg.get("Failure Count", 0)),
                              "avg_response_time": float(agg.get("Average Response Time", 0)),
                              "p95_response_time": float(agg.get("95%", 0)),
                              "p99_response_time": float(agg.get("99%", 0)),
                              "requests_per_sec": float(agg.get("Requests/s", 0))
                          }

                  print(json.dumps(summary, indent=2))

                  with open("results/metrics.json", "w") as f:
                      json.dump(summary, f, indent=2)
                  EOF

                  python analyze.py

            - name: 성능 리포트 업로드
              uses: actions/upload-artifact@v4
              with:
                  name: performance-metrics-${{ github.run_number }}
                  path: results/metrics.json
                  retention-days: 90
