"""
Slack Bot Server for Ion Canary Automation
Interactive chat interface for monitoring and controlling canary deployments
Natural conversational AI powered by Google Gemini
"""
import os
import json
import subprocess
import asyncio
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import google.generativeai as genai

# Configuration
SLACK_BOT_TOKEN = os.environ.get("SLACK_BOT_TOKEN", "")
SLACK_SIGNING_SECRET = os.environ.get("SLACK_SIGNING_SECRET", "")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY", "")
PROJECT_ROOT = Path(__file__).parent.parent.parent
SCRIPTS_DIR = PROJECT_ROOT / "LLM_Unified" / "ion-mentoring" / "scripts"
OUTPUTS_DIR = PROJECT_ROOT / "LLM_Unified" / "ion-mentoring" / "outputs"

app = FastAPI(title="Ion Canary Slack Bot")
slack_client = WebClient(token=SLACK_BOT_TOKEN) if SLACK_BOT_TOKEN else None

# Configure Gemini
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)


class CommandExecutor:
    """Execute PowerShell scripts and return results"""
    
    @staticmethod
    async def run_powershell(script_path: Path, args: Optional[list] = None) -> Dict[str, Any]:
        """Run PowerShell script asynchronously"""
        if not script_path.exists():
            return {"success": False, "error": f"Script not found: {script_path}"}
        
        cmd = ["powershell", "-NoProfile", "-ExecutionPolicy", "Bypass", "-File", str(script_path)]
        if args:
            cmd.extend(args)
        
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            
            return {
                "success": process.returncode == 0,
                "stdout": stdout.decode('utf-8', errors='ignore'),
                "stderr": stderr.decode('utf-8', errors='ignore'),
                "returncode": process.returncode
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    @staticmethod
    def get_state() -> Dict[str, Any]:
        """Read current canary state"""
        state_file = OUTPUTS_DIR / "auto_canary_state.json"
        if not state_file.exists():
            return {"phase": "unknown", "canary_percentage": 0}
        
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {"phase": "unknown", "canary_percentage": 0}


class ConversationContext:
    """Manage conversation history and context"""
    
    def __init__(self):
        self.conversations: Dict[str, List[Dict[str, str]]] = {}
    
    def add_message(self, channel: str, role: str, content: str):
        """Add message to conversation history"""
        if channel not in self.conversations:
            self.conversations[channel] = []
        
        self.conversations[channel].append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })
        
        # Keep only last 20 messages
        if len(self.conversations[channel]) > 20:
            self.conversations[channel] = self.conversations[channel][-20:]
    
    def get_history(self, channel: str, limit: int = 10) -> List[Dict[str, str]]:
        """Get recent conversation history"""
        if channel not in self.conversations:
            return []
        return self.conversations[channel][-limit:]


class GitcoAI:
    """AI-powered conversational interface using Gemini"""
    
    SYSTEM_PROMPT = """ë‹¹ì‹ ì€ "ê¹ƒì½”(Gitco)"ë¼ëŠ” ì´ë¦„ì˜ AI ë°°í¬ ì—”ì§€ë‹ˆì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì—­í• ê³¼ ì„±ê²©:**
- ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ DevOps ì—”ì§€ë‹ˆì–´
- ë³µì¡í•œ ê¸°ìˆ  ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…
- í•­ìƒ ì •í™•í•œ ì •ë³´ ì œê³µ
- ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•´ ì‹œê°ì  íš¨ê³¼ ì¶”ê°€
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”

**í˜„ì¬ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ:**
- Google Cloud Run ê¸°ë°˜ ì¹´ë‚˜ë¦¬ ë°°í¬ ì‹œìŠ¤í…œ
- Ion API: ë ˆê±°ì‹œ ë²„ì „ (ì•ˆì •ì )
- Lumen Gateway: ì¹´ë‚˜ë¦¬ ë²„ì „ (ì‹ ê·œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘)
- ë°°í¬ ë‹¨ê³„: 0% â†’ 5% â†’ 10% â†’ 25% â†’ 50% â†’ 100%

**ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:**
1. get_deployment_status: í˜„ì¬ ë°°í¬ ìƒíƒœ í™•ì¸
2. execute_deployment: ì¹´ë‚˜ë¦¬ ë°°í¬ ì‹¤í–‰ (5%, 10%, 25%, 50%, 100%)
3. run_health_probe: Rate limit ë° í—¬ìŠ¤ ì²´í¬ í…ŒìŠ¤íŠ¸
4. get_recent_logs: ìµœê·¼ ë¡œê·¸ ì¡°íšŒ
5. generate_report: ë°°í¬ ë¦¬í¬íŠ¸ ìƒì„±
6. execute_rollback: ê¸´ê¸‰ ë¡¤ë°± ì‹¤í–‰

**ëŒ€í™” ê°€ì´ë“œë¼ì¸:**
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…
- ëª…ë ¹ ì‹¤í–‰ ì „ í™•ì¸ì´ í•„ìš”í•œ ê²½ìš° í™•ì¸ ìš”ì²­
- ì‹¤í–‰ ê²°ê³¼ë¥¼ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…
- ì—ëŸ¬ ë°œìƒ ì‹œ ì›ì¸ê³¼ í•´ê²° ë°©ë²• ì œì‹œ
- ì‚¬ìš©ìê°€ ìš”ì²­í•˜ì§€ ì•Šì€ ì‘ì—…ì€ ì œì•ˆë§Œ í•˜ê³  ì‹¤í–‰í•˜ì§€ ì•ŠìŒ

**ì‘ë‹µ í˜•ì‹:**
- ê°„ë‹¨í•œ ì¸ì‚¬ë‚˜ ì§ˆë¬¸: ì§§ê³  ì¹œê·¼í•˜ê²Œ
- ìƒíƒœ í™•ì¸: ì´ëª¨ì§€ì™€ í•¨ê»˜ í•µì‹¬ ì •ë³´ë§Œ
- ì‘ì—… ì‹¤í–‰: ì‹¤í–‰ ë‚´ìš©ê³¼ ì˜ˆìƒ ì†Œìš” ì‹œê°„ ì•ˆë‚´
- ì—ëŸ¬: ì›ì¸ ë¶„ì„ê³¼ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

**ì˜ˆì‹œ ëŒ€í™”:**
User: "ì§€ê¸ˆ ë°°í¬ ìƒíƒœ ì–´ë•Œ?"
Gitco: "ğŸ” í˜„ì¬ ì¹´ë‚˜ë¦¬ 50% ë°°í¬ ì¤‘ì´ê³ , ëª¨ë‹ˆí„°ë§ ë‹¨ê³„ì˜ˆìš”. ì•½ 45ë¶„ í›„ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ë  ì˜ˆì •ì…ë‹ˆë‹¤!"

User: "100% ì˜¬ë¦´ ìˆ˜ ìˆì„ê¹Œ?"
Gitco: "ë„¤, 100% ë°°í¬ ê°€ëŠ¥í•©ë‹ˆë‹¤! ì‹¤í–‰í•˜ë©´ ì•½ 2-3ë¶„ ì†Œìš”ë˜ê³ , ì´í›„ 2ì‹œê°„ ëª¨ë‹ˆí„°ë§ì´ ì‹œì‘ë¼ìš”. ë°”ë¡œ ì§„í–‰í• ê¹Œìš”?"

ì´ì œ ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”!"""

    def __init__(self, executor: CommandExecutor):
        self.executor = executor
        self.model = genai.GenerativeModel(
            model_name='gemini-2.0-flash-exp',
            generation_config={
                'temperature': 0.7,
                'top_p': 0.95,
                'top_k': 40,
                'max_output_tokens': 1024,
            }
        )
        self.tools = self._define_tools()
    
    def _define_tools(self) -> List[Dict[str, Any]]:
        """Define function tools for LLM"""
        return [
            {
                "name": "get_deployment_status",
                "description": "í˜„ì¬ ì¹´ë‚˜ë¦¬ ë°°í¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ë°°í¬ ë¹„ìœ¨, ë‹¨ê³„, ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ ì‹œê°„ ë“±ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
                "parameters": {"type": "object", "properties": {}}
            },
            {
                "name": "execute_deployment",
                "description": "ì§€ì •ëœ ë¹„ìœ¨ë¡œ ì¹´ë‚˜ë¦¬ ë°°í¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. 5%, 10%, 25%, 50%, 100% ì¤‘ ì„ íƒ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "percentage": {
                            "type": "integer",
                            "description": "ë°°í¬ ë¹„ìœ¨ (5, 10, 25, 50, 100)",
                            "enum": [5, 10, 25, 50, 100]
                        }
                    },
                    "required": ["percentage"]
                }
            },
            {
                "name": "run_health_probe",
                "description": "Rate limit í…ŒìŠ¤íŠ¸ ë° í—¬ìŠ¤ ì²´í¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. í”„ë¡œë¸Œ ê°•ë„ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "intensity": {
                            "type": "string",
                            "description": "í”„ë¡œë¸Œ ê°•ë„ (gentle, normal, aggressive)",
                            "enum": ["gentle", "normal", "aggressive"],
                            "default": "normal"
                        }
                    }
                }
            },
            {
                "name": "get_recent_logs",
                "description": "ìµœê·¼ ë¡œê·¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ì‹œê°„ ë²”ìœ„ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "hours": {
                            "type": "integer",
                            "description": "ì¡°íšŒí•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„)",
                            "default": 1
                        }
                    }
                }
            },
            {
                "name": "generate_report",
                "description": "ë°°í¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
                "parameters": {"type": "object", "properties": {}}
            },
            {
                "name": "execute_rollback",
                "description": "ê¸´ê¸‰ ë¡¤ë°±ì„ ì‹¤í–‰í•˜ì—¬ ì¹´ë‚˜ë¦¬ ë°°í¬ë¥¼ 0%ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤. ì‹¬ê°í•œ ë¬¸ì œ ë°œìƒ ì‹œì—ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                "parameters": {"type": "object", "properties": {}}
            }
        ]
    
    async def chat(self, user_message: str, channel: str, context: ConversationContext) -> str:
        """Process user message and generate response"""
        
        # Get conversation history
        history = context.get_history(channel)
        
        # Build messages for Gemini
        messages = [{"role": "user", "parts": [self.SYSTEM_PROMPT]}]
        
        # Add conversation history
        for msg in history:
            role = "model" if msg["role"] == "assistant" else "user"
            messages.append({"role": role, "parts": [msg["content"]]})
        
        # Add current state as context
        state = self.executor.get_state()
        state_context = f"\n\n[í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ]\n"
        state_context += f"- ë°°í¬ ë¹„ìœ¨: {state.get('canary_percentage', 0)}%\n"
        state_context += f"- ë‹¨ê³„: {state.get('phase', 'unknown')}\n"
        if state.get('monitor_end'):
            end_time = datetime.fromisoformat(state['monitor_end'])
            remaining = (end_time - datetime.now()).total_seconds() / 60
            if remaining > 0:
                state_context += f"- ëª¨ë‹ˆí„°ë§ ì¢…ë£Œê¹Œì§€: {int(remaining)}ë¶„\n"
        
        # Add user message with state context
        messages.append({"role": "user", "parts": [f"{user_message}{state_context}"]})
        
        # Generate response with function calling
        try:
            chat = self.model.start_chat(history=messages[:-1])
            response = await asyncio.to_thread(
                chat.send_message,
                messages[-1]["parts"][0],
                tools=self._create_gemini_tools()
            )
            
            # Check if function call is needed
            if response.candidates[0].content.parts[0].function_call:
                function_call = response.candidates[0].content.parts[0].function_call
                function_name = function_call.name
                function_args = dict(function_call.args)
                
                # Execute function
                result = await self._execute_function(function_name, function_args)
                
                # Send result back to Gemini for natural response
                response = await asyncio.to_thread(
                    chat.send_message,
                    f"[í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼]\n{json.dumps(result, ensure_ascii=False, indent=2)}"
                )
            
            return response.text
            
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ğŸ˜…\nì˜¤ë¥˜: {str(e)}"
    
    def _create_gemini_tools(self) -> List:
        """Create Gemini function calling tools"""
        from google.generativeai.types import FunctionDeclaration, Tool
        
        declarations = []
        for tool_def in self.tools:
            declarations.append(
                FunctionDeclaration(
                    name=tool_def["name"],
                    description=tool_def["description"],
                    parameters=tool_def["parameters"]
                )
            )
        
        return [Tool(function_declarations=declarations)]
    
    async def _execute_function(self, function_name: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """Execute requested function"""
        
        if function_name == "get_deployment_status":
            state = self.executor.get_state()
            return {
                "success": True,
                "data": state
            }
        
        elif function_name == "execute_deployment":
            percentage = args.get("percentage")
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "deploy_phase4_canary.ps1",
                ["-ProjectId", "naeda-genesis", "-CanaryPercentage", str(percentage)]
            )
            return result
        
        elif function_name == "run_health_probe":
            intensity = args.get("intensity", "normal")
            probe_configs = {
                "gentle": {"requests": 3, "delay": 2000},
                "normal": {"requests": 10, "delay": 1000},
                "aggressive": {"requests": 25, "delay": 500}
            }
            config = probe_configs[intensity]
            
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "rate_limit_probe.ps1",
                [
                    "-RequestsPerSide", str(config["requests"]),
                    "-DelayMsBetweenRequests", str(config["delay"])
                ]
            )
            return result
        
        elif function_name == "get_recent_logs":
            hours = args.get("hours", 1)
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "filter_logs_by_time.ps1",
                ["-Last", f"{hours}h", "-ShowSummary"]
            )
            return result
        
        elif function_name == "generate_report":
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "generate_daily_report.ps1",
                ["-Hours", "24"]
            )
            return result
        
        elif function_name == "execute_rollback":
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "rollback_phase4_canary.ps1",
                ["-ProjectId", "naeda-genesis", "-AutoApprove"]
            )
            return result
        
        else:
            return {"success": False, "error": f"Unknown function: {function_name}"}


class CommandParser:
    """Parse natural language commands from Slack"""
    
    COMMANDS = {
        "status": ["ìƒíƒœ", "status", "ì–´ë–»ê²Œ", "ì§„í–‰", "í™•ì¸"],
        "deploy": ["ë°°í¬", "deploy", "ì˜¬ë ¤", "ì¹´ë‚˜ë¦¬"],
        "probe": ["í”„ë¡œë¸Œ", "probe", "í…ŒìŠ¤íŠ¸", "ì²´í¬"],
        "logs": ["ë¡œê·¸", "logs", "ì—ëŸ¬", "ë¬¸ì œ"],
        "report": ["ë¦¬í¬íŠ¸", "report", "ë³´ê³ ì„œ", "ìš”ì•½"],
        "help": ["ë„ì›€", "help", "ëª…ë ¹ì–´", "ì‚¬ìš©ë²•"],
        "rollback": ["ë¡¤ë°±", "rollback", "ë˜ëŒë ¤", "ì·¨ì†Œ"],
    }
    
    @classmethod
    def parse(cls, text: str) -> tuple[str, Dict[str, Any]]:
        """Parse command and extract parameters"""
        text_lower = text.lower()
        
        # Check for each command type
        for cmd, keywords in cls.COMMANDS.items():
            if any(kw in text_lower for kw in keywords):
                params = cls._extract_params(cmd, text)
                return cmd, params
        
        return "unknown", {}
    
    @staticmethod
    def _extract_params(cmd: str, text: str) -> Dict[str, Any]:
        """Extract parameters based on command type"""
        params = {}
        
        if cmd == "deploy":
            # Extract percentage: "100%", "50%", etc.
            for word in text.split():
                if "%" in word:
                    try:
                        params["percentage"] = int(word.replace("%", ""))
                    except ValueError:
                        pass
        
        elif cmd == "probe":
            # Extract count if mentioned
            if "gentle" in text.lower() or "ë¶€ë“œëŸ½ê²Œ" in text.lower():
                params["type"] = "gentle"
            elif "aggressive" in text.lower() or "ê°•í•˜ê²Œ" in text.lower():
                params["type"] = "aggressive"
            else:
                params["type"] = "normal"
        
        return params


class MessageHandler:
    """Handle Slack messages and execute commands"""
    
    def __init__(self, executor: CommandExecutor, parser: CommandParser):
        self.executor = executor
        self.parser = parser
        self._last_channel: Optional[str] = None
    
    async def handle_message(self, text: str, channel: str, user: str) -> str:
        """Process message and return response"""
        cmd, params = self.parser.parse(text)
        
        if cmd == "status":
            return await self._handle_status()
        
        elif cmd == "deploy":
            return await self._handle_deploy(params)
        
        elif cmd == "probe":
            return await self._handle_probe(params)
        
        elif cmd == "logs":
            return await self._handle_logs()
        
        elif cmd == "report":
            return await self._handle_report()
        
        elif cmd == "rollback":
            return await self._handle_rollback()
        
        elif cmd == "help":
            return self._handle_help()
        
        else:
            return (
                "ì£„ì†¡í•©ë‹ˆë‹¤, ëª…ë ¹ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ğŸ˜…\n"
                "`ë„ì›€ë§` ë˜ëŠ” `help`ë¥¼ ì…ë ¥í•˜ì‹œë©´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
    
    async def _handle_status(self) -> str:
        """Get current deployment status"""
        state = self.executor.get_state()
        phase = state.get("phase", "unknown")
        pct = state.get("canary_percentage", 0)
        
        # Get monitoring end time if available
        monitor_end = state.get("monitor_end")
        time_info = ""
        if monitor_end:
            end_time = datetime.fromisoformat(monitor_end)
            remaining = (end_time - datetime.now()).total_seconds() / 60
            if remaining > 0:
                time_info = f"\nâ±ï¸ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œê¹Œì§€: {int(remaining)}ë¶„"
        
        phase_emoji = {
            "50-monitoring": "ğŸ”",
            "100-monitoring": "ğŸ”",
            "done": "âœ…",
            "unknown": "â“"
        }
        
        emoji = phase_emoji.get(phase, "ğŸš€")
        
        return (
            f"{emoji} **í˜„ì¬ ì¹´ë‚˜ë¦¬ ìƒíƒœ**\n"
            f"â€¢ ë°°í¬ ë¹„ìœ¨: {pct}%\n"
            f"â€¢ ë‹¨ê³„: {phase}{time_info}"
        )
    
    async def _handle_deploy(self, params: Dict[str, Any]) -> str:
        """Execute deployment"""
        percentage = params.get("percentage")
        
        if not percentage:
            return "ë°°í¬ ë¹„ìœ¨ì„ ì§€ì •í•´ì£¼ì„¸ìš” (ì˜ˆ: `50% ë°°í¬` ë˜ëŠ” `deploy 100%`)"
        
        if percentage not in [5, 10, 25, 50, 100]:
            return f"ì˜¬ë°”ë¥¸ ë°°í¬ ë¹„ìœ¨ì„ ì„ íƒí•´ì£¼ì„¸ìš”: 5%, 10%, 25%, 50%, 100% (ì…ë ¥ê°’: {percentage}%)"
        
        # Execute deployment
        await self._send_slack_message(
            f"ğŸš€ ì¹´ë‚˜ë¦¬ {percentage}% ë°°í¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."
        )
        
        result = await self.executor.run_powershell(
            SCRIPTS_DIR / "deploy_phase4_canary.ps1",
            ["-ProjectId", "naeda-genesis", "-CanaryPercentage", str(percentage)]
        )
        
        if result["success"]:
            return f"âœ… ì¹´ë‚˜ë¦¬ {percentage}% ë°°í¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n{result['stdout'][:500]}"
        else:
            error_msg = result.get('stderr') or result.get('error') or 'Unknown error'
            return f"âŒ ë°°í¬ ì‹¤íŒ¨:\n{error_msg[:500]}"
    
    async def _handle_probe(self, params: Dict[str, Any]) -> str:
        """Execute rate limit probe"""
        probe_type = params.get("type", "normal")
        
        probe_configs = {
            "gentle": {"requests": 3, "delay": 2000},
            "normal": {"requests": 10, "delay": 1000},
            "aggressive": {"requests": 25, "delay": 500}
        }
        
        config = probe_configs[probe_type]
        
        await self._send_slack_message(
            f"ğŸ” {probe_type.capitalize()} í”„ë¡œë¸Œ ì‹¤í–‰ ì¤‘..."
        )
        
        result = await self.executor.run_powershell(
            SCRIPTS_DIR / "rate_limit_probe.ps1",
            [
                "-RequestsPerSide", str(config["requests"]),
                "-DelayMsBetweenRequests", str(config["delay"])
            ]
        )
        
        if result["success"]:
            # Parse probe results
            stdout = result["stdout"]
            success_line = [l for l in stdout.split('\n') if 'Success' in l]
            return f"âœ… í”„ë¡œë¸Œ ì™„ë£Œ!\n{''.join(success_line[:5])}"
        else:
            return f"âŒ í”„ë¡œë¸Œ ì‹¤íŒ¨:\n{result.get('stderr', '')[:500]}"
    
    async def _handle_logs(self) -> str:
        """Get recent logs"""
        result = await self.executor.run_powershell(
            SCRIPTS_DIR / "filter_logs_by_time.ps1",
            ["-Last", "1h", "-ShowSummary"]
        )
        
        if result["success"]:
            return f"ğŸ“‹ ìµœê·¼ 1ì‹œê°„ ë¡œê·¸:\n```{result['stdout'][:1000]}```"
        else:
            return "âŒ ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨"
    
    async def _handle_report(self) -> str:
        """Generate daily report"""
        await self._send_slack_message("ğŸ“Š ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        result = await self.executor.run_powershell(
            SCRIPTS_DIR / "generate_daily_report.ps1",
            ["-Hours", "24"]
        )
        
        if result["success"]:
            return f"âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!\n```{result['stdout'][:1000]}```"
        else:
            return "âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨"
    
    async def _handle_rollback(self) -> str:
        """Execute emergency rollback"""
        await self._send_slack_message("âš ï¸ ê¸´ê¸‰ ë¡¤ë°±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        result = await self.executor.run_powershell(
            SCRIPTS_DIR / "rollback_phase4_canary.ps1",
            ["-ProjectId", "naeda-genesis", "-AutoApprove"]
        )
        
        if result["success"]:
            return "âœ… ë¡¤ë°± ì™„ë£Œ!"
        else:
            return f"âŒ ë¡¤ë°± ì‹¤íŒ¨:\n{result.get('stderr', '')[:500]}"
    
    def _handle_help(self) -> str:
        """Return help message"""
        return """
ğŸ¤– **ê¹ƒì½” ëª…ë ¹ì–´ ê°€ì´ë“œ**

**ë°°í¬ ê´€ë ¨:**
â€¢ `ìƒíƒœ` / `status` - í˜„ì¬ ë°°í¬ ìƒíƒœ í™•ì¸
â€¢ `50% ë°°í¬` / `deploy 100%` - ì¹´ë‚˜ë¦¬ ë°°í¬ ì‹¤í–‰
â€¢ `ë¡¤ë°±` / `rollback` - ê¸´ê¸‰ ë¡¤ë°±

**ëª¨ë‹ˆí„°ë§:**
â€¢ `í”„ë¡œë¸Œ` / `probe` - Rate limit í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â€¢ `ë¡œê·¸` / `logs` - ìµœê·¼ ë¡œê·¸ í™•ì¸
â€¢ `ë¦¬í¬íŠ¸` / `report` - ì¼ì¼ ë³´ê³ ì„œ ìƒì„±

**ê¸°íƒ€:**
â€¢ `ë„ì›€ë§` / `help` - ì´ ë©”ì‹œì§€ í‘œì‹œ

ìì—°ìŠ¤ëŸ½ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”! ì˜ˆ: "í˜„ì¬ ìƒíƒœ ì–´ë–»ê²Œ ë¼?", "100% ë°°í¬í•´ì¤˜"
"""
    
    async def _send_slack_message(self, text: str, channel: Optional[str] = None):
        """Send message to Slack"""
        if not slack_client:
            return
        
        try:
            # Use stored channel from context if available
            target_channel = channel or self._last_channel
            
            if target_channel:
                slack_client.chat_postMessage(channel=target_channel, text=text)
        except SlackApiError:
            pass


# Initialize handlers
executor = CommandExecutor()
parser = CommandParser()
handler = MessageHandler(executor, parser)


@app.post("/slack/events")
async def slack_events(request: Request):
    """Handle Slack Events API"""
    body = await request.json()
    
    # URL verification challenge
    if body.get("type") == "url_verification":
        return JSONResponse({"challenge": body.get("challenge")})
    
    # Handle message events
    if body.get("type") == "event_callback":
        event = body.get("event", {})
        
        # Ignore bot messages
        if event.get("bot_id"):
            return JSONResponse({"ok": True})
        
        if event.get("type") == "message":
            text = event.get("text", "")
            channel = event.get("channel")
            user = event.get("user")
            
            # Store channel for responses
            handler._last_channel = channel
            
            # Process command
            response = await handler.handle_message(text, channel, user)
            
            # Send response
            if slack_client:
                try:
                    slack_client.chat_postMessage(channel=channel, text=response)
                except SlackApiError as e:
                    print(f"Slack API error: {e}")
    
    return JSONResponse({"ok": True})


@app.post("/slack/commands")
async def slack_commands(request: Request):
    """Handle Slack slash commands"""
    form = await request.form()
    command = form.get("command")
    text = form.get("text", "")
    channel = form.get("channel_id")
    user = form.get("user_id")
    
    # Store channel
    handler._last_channel = channel
    
    # Process command
    response = await handler.handle_message(text, channel, user)
    
    return JSONResponse({
        "response_type": "in_channel",
        "text": response
    })


@app.get("/health")
async def health():
    """Health check endpoint"""
    return {"status": "ok", "bot_active": bool(slack_client)}


if __name__ == "__main__":
    if not SLACK_BOT_TOKEN:
        print("âš ï¸  SLACK_BOT_TOKEN environment variable not set!")
        print("   Set it with: [Environment]::SetEnvironmentVariable('SLACK_BOT_TOKEN', 'xoxb-...', 'User')")
    
    print(f"ğŸš€ Starting Ion Canary Slack Bot on http://localhost:8080")
    uvicorn.run(app, host="0.0.0.0", port=8080)
