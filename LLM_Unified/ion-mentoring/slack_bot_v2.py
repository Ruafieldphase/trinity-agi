"""
Slack Bot Server for Ion Canary Automation - Natural Conversation Edition
Powered by Google Generative AI (Gemini)
"""
import os
import json
import subprocess
import asyncio
import re
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
import uvicorn
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import google.generativeai as genai
from google.generativeai import types

# Configuration
SLACK_BOT_TOKEN = os.environ.get("SLACK_BOT_TOKEN", "")
PROJECT_ROOT = Path(__file__).parent.parent.parent
SCRIPTS_DIR = PROJECT_ROOT / "LLM_Unified" / "ion-mentoring" / "scripts"
OUTPUTS_DIR = PROJECT_ROOT / "LLM_Unified" / "ion-mentoring" / "outputs"
GCP_PROJECT = "naeda-genesis"
GCP_LOCATION = "us-central1"

app = FastAPI(title="Ion Canary Slack Bot")
slack_client = WebClient(token=SLACK_BOT_TOKEN) if SLACK_BOT_TOKEN else None

# Configure Google GenAI
genai.configure()


class CommandExecutor:
    """Execute PowerShell scripts"""
    
    @staticmethod
    async def run_powershell(script_path: Path, args: Optional[List[str]] = None) -> Dict[str, Any]:
        """Run PowerShell script asynchronously"""
        if not script_path.exists():
            return {"success": False, "error": f"Script not found: {script_path}"}
        
        cmd = ["powershell", "-NoProfile", "-ExecutionPolicy", "Bypass", "-File", str(script_path)]
        if args:
            cmd.extend(args)
        
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            
            return {
                "success": process.returncode == 0,
                "stdout": stdout.decode('utf-8', errors='ignore'),
                "stderr": stderr.decode('utf-8', errors='ignore'),
                "returncode": process.returncode
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    @staticmethod
    def get_state() -> Dict[str, Any]:
        """Read current canary state"""
        state_file = OUTPUTS_DIR / "auto_canary_state.json"
        if not state_file.exists():
            return {"phase": "unknown", "canary_percentage": 0}
        
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {"phase": "unknown", "canary_percentage": 0}


class GitcoAI:
    """Natural conversational AI assistant for deployment management"""
    
    SYSTEM_INSTRUCTION = """ÎãπÏã†ÏùÄ "ÍπÉÏΩî(Gitco)"ÏûÖÎãàÎã§. ÏπúÍ∑ºÌïòÍ≥† Ïú†Îä•Ìïú AI Î∞∞Ìè¨ ÏóîÏßÄÎãàÏñ¥Î°úÏÑú ÏÇ¨Ïö©ÏûêÎ•º ÎèïÏäµÎãàÎã§.

**ÏÑ±Í≤©:**
- ÏπúÍ∑ºÌïòÍ≥† Îî∞ÎúªÌïú ÎßêÌà¨ (Ï°¥ÎåìÎßê ÏÇ¨Ïö©)
- Í∏∞Ïà†Ï†ÅÏù¥ÏßÄÎßå ÏâΩÍ≤å ÏÑ§Î™Ö
- Ïù¥Î™®ÏßÄÎ•º Ï†ÅÏ†àÌûà ÏÇ¨Ïö©
- Í∞ÑÍ≤∞ÌïòÍ≥† Î™ÖÌôïÌïú ÎãµÎ≥Ä

**Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú:**
- Google Cloud Run Ïπ¥ÎÇòÎ¶¨ Î∞∞Ìè¨
- Ion API (Î†àÍ±∞Ïãú) vs Lumen Gateway (Ïã†Í∑ú)
- Îã®Í≥ÑÎ≥Ñ Ìä∏ÎûòÌîΩ Ï¶ùÍ∞Ä: 5% ‚Üí 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%

**ÎåÄÌôî Ïä§ÌÉÄÏùº:**
User: "ÏßÄÍ∏à Ïñ¥ÎñªÍ≤å Îèº?"
Gitco: "üîç ÌòÑÏû¨ Ïπ¥ÎÇòÎ¶¨ 50% Î∞∞Ìè¨ Ï§ëÏù¥ÏóêÏöî! Î™®ÎãàÌÑ∞ÎßÅÌïòÍ≥† ÏûàÏúºÎ©∞ 45Î∂Ñ ÌõÑ Îã§Ïùå Îã®Í≥ÑÎ°ú ÎÑòÏñ¥Í∞à ÏòàÏ†ïÏûÖÎãàÎã§."

User: "Î¨∏Ï†ú ÏóÜÏñ¥?"
Gitco: "ÎÑ§, Î™®Îì† Ìó¨Ïä§ Ï≤¥ÌÅ¨Í∞Ä Ï†ïÏÉÅÏù¥ÏóêÏöî! ‚úÖ ÏóêÎü¨Ïú® 0%, Î†àÏù¥ÌÑ¥ÏãúÎèÑ ÏïàÏ†ïÏ†ÅÏûÖÎãàÎã§."

User: "100% Ïò¨Î†§Ï§ò"
Gitco: "ÏïåÍ≤†ÏäµÎãàÎã§! 100% Î∞∞Ìè¨Î•º ÏãúÏûëÌï†Í≤åÏöî. 2-3Î∂Ñ ÏÜåÏöîÎêòÍ≥† Ïù¥ÌõÑ 2ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅÏù¥ ÏßÑÌñâÎê©ÎãàÎã§. üöÄ"

Î™ÖÎ†πÏùÑ Ïã§ÌñâÌïòÍ∏∞ Ï†ÑÏóêÎäî ÏÇ¨Ïö©ÏûêÏùò ÏùòÎèÑÎ•º Î™ÖÌôïÌûà ÌååÏïÖÌïòÍ≥†, ÌïÑÏöîÏãú ÌôïÏù∏ÏùÑ Î∞õÏúºÏÑ∏Ïöî."""

    def __init__(self, executor: CommandExecutor):
        self.executor = executor
        self.model = genai.GenerativeModel(
            "gemini-2.0-flash-exp",
            system_instruction=self.SYSTEM_INSTRUCTION,
            tools=[self._create_tools()]
        )
        self.conversations: Dict[str, List] = {}
    
    def _create_tools(self) -> types.Tool:
        """Define function tools for Gemini"""
        return types.Tool(
            function_declarations=[
                types.FunctionDeclaration(
                    name="get_status",
                    description="ÌòÑÏû¨ Ïπ¥ÎÇòÎ¶¨ Î∞∞Ìè¨ ÏÉÅÌÉúÎ•º ÌôïÏù∏Ìï©ÎãàÎã§. Î∞∞Ìè¨ ÎπÑÏú®, Îã®Í≥Ñ, Î™®ÎãàÌÑ∞ÎßÅ ÎÇ®ÏùÄ ÏãúÍ∞Ñ Îì±ÏùÑ Î∞òÌôòÌï©ÎãàÎã§.",
                    parameters={
                        "type": "object",
                        "properties": {}
                    }
                ),
                types.FunctionDeclaration(
                    name="deploy_canary",
                    description="ÏßÄÏ†ïÎêú ÎπÑÏú®Î°ú Ïπ¥ÎÇòÎ¶¨ Î∞∞Ìè¨Î•º Ïã§ÌñâÌï©ÎãàÎã§. ÏÇ¨Ïö©ÏûêÍ∞Ä Î™ÖÌôïÌûà Î∞∞Ìè¨Î•º ÏöîÏ≤≠Ìïú Í≤ΩÏö∞ÏóêÎßå ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî. Í∞ÄÎä•Ìïú ÎπÑÏú®: 5, 10, 25, 50, 100",
                    parameters={
                        "type": "object",
                        "properties": {
                            "percentage": {
                                "type": "number",
                                "description": "Î∞∞Ìè¨ ÎπÑÏú® (5, 10, 25, 50, 100 Ï§ë ÌïòÎÇò)"
                            }
                        },
                        "required": ["percentage"]
                    }
                ),
                types.FunctionDeclaration(
                    name="run_probe",
                    description="Ìó¨Ïä§ Ï≤¥ÌÅ¨ Î∞è Î†àÏù¥Ìä∏ Î¶¨Î∞ã ÌÖåÏä§Ìä∏Î•º Ïã§ÌñâÌï©ÎãàÎã§.",
                    parameters={
                        "type": "object",
                        "properties": {
                            "intensity": {
                                "type": "string",
                                "description": "ÌÖåÏä§Ìä∏ Í∞ïÎèÑ - gentle(Î∂ÄÎìúÎüΩÍ≤å), normal(Î≥¥ÌÜµ), aggressive(Í∞ïÌïòÍ≤å) Ï§ë ÌïòÎÇò"
                            }
                        }
                    }
                ),
                types.FunctionDeclaration(
                    name="get_logs",
                    description="ÏµúÍ∑º Î°úÍ∑∏Î•º Ï°∞ÌöåÌï©ÎãàÎã§.",
                    parameters={
                        "type": "object",
                        "properties": {
                            "hours": {
                                "type": "number",
                                "description": "Ï°∞ÌöåÌï† ÏãúÍ∞Ñ (ÏãúÍ∞Ñ Îã®ÏúÑ, Í∏∞Î≥∏Í∞í 1ÏãúÍ∞Ñ)"
                            }
                        }
                    }
                ),
                types.FunctionDeclaration(
                    name="generate_report",
                    description="ÏùºÏùº Î∞∞Ìè¨ Î¶¨Ìè¨Ìä∏Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.",
                    parameters={
                        "type": "object",
                        "properties": {}
                    }
                ),
                types.FunctionDeclaration(
                    name="rollback",
                    description="Í∏¥Í∏â Î°§Î∞±ÏùÑ Ïã§ÌñâÌï©ÎãàÎã§. Ïπ¥ÎÇòÎ¶¨Î•º 0%Î°ú ÎêòÎèåÎ¶ΩÎãàÎã§. Îß§Ïö∞ Ïã†Ï§ëÌïòÍ≤å ÏÇ¨Ïö©Ìï¥Ïïº Ìï©ÎãàÎã§.",
                    parameters={
                        "type": "object",
                        "properties": {}
                    }
                )
            ]
        )
    
    async def chat(self, user_message: str, channel: str) -> str:
        """Process user message and generate natural response"""
        
        # Get current state for context
        state = self.executor.get_state()
        state_info = self._format_state(state)
        
        # Prepare conversation history
        if channel not in self.conversations:
            self.conversations[channel] = []
        
        # Add context to user message
        message_with_context = f"{user_message}\n\n[ÏãúÏä§ÌÖú ÏÉÅÌÉú]\n{state_info}"
        
        # Generate response
        try:
            chat_session = self.model.start_chat(history=self.conversations[channel])
            response = await asyncio.to_thread(chat_session.send_message, message_with_context)
            
            # Handle function calls
            if hasattr(response.candidates[0], 'function_calls') and response.candidates[0].function_calls:
                for function_call in response.candidates[0].function_calls:
                    result = await self._execute_function(
                        function_call.name,
                        dict(function_call.args) if function_call.args else {}
                    )
                    
                    # Send function result back to model
                    response = await asyncio.to_thread(
                        chat_session.send_message,
                        [{
                            "function_call": function_call,
                            "function_response": {"name": function_call.name, "response": result}
                        }]
                    )
            
            # Update conversation history
            self.conversations[channel].extend([
                {"role": "user", "parts": [user_message]},
                {"role": "model", "parts": [response.text]}
            ])
            
            # Keep last 20 messages
            if len(self.conversations[channel]) > 20:
                self.conversations[channel] = self.conversations[channel][-20:]
            
            return response.text
            
        except Exception as e:
            return f"Ï£ÑÏÜ°Ìï¥Ïöî, ÏùëÎãµ ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. üò¢\n```{str(e)}```"
    
    def _format_state(self, state: Dict[str, Any]) -> str:
        """Format state info for context"""
        info = f"Î∞∞Ìè¨ ÎπÑÏú®: {state.get('canary_percentage', 0)}%\n"
        info += f"ÌòÑÏû¨ Îã®Í≥Ñ: {state.get('phase', 'unknown')}\n"
        
        if state.get('monitor_end'):
            try:
                end_time = datetime.fromisoformat(state['monitor_end'])
                remaining = (end_time - datetime.now()).total_seconds() / 60
                if remaining > 0:
                    info += f"Î™®ÎãàÌÑ∞ÎßÅ Ï¢ÖÎ£åÍπåÏßÄ: {int(remaining)}Î∂Ñ\n"
            except:
                pass
        
        return info
    
    async def _execute_function(self, name: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """Execute function called by LLM"""
        
        if name == "get_status":
            state = self.executor.get_state()
            return {
                "success": True,
                "phase": state.get('phase'),
                "percentage": state.get('canary_percentage'),
                "monitor_end": state.get('monitor_end')
            }
        
        elif name == "deploy_canary":
            pct = int(args.get("percentage", 5))
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "deploy_phase4_canary.ps1",
                ["-ProjectId", GCP_PROJECT, "-CanaryPercentage", str(pct)]
            )
            return {
                "success": result["success"],
                "percentage": pct,
                "output": result.get("stdout", "")[:500] if result["success"] else result.get("stderr", "")[:500]
            }
        
        elif name == "run_probe":
            intensity = args.get("intensity", "normal")
            configs = {
                "gentle": (3, 2000),
                "normal": (10, 1000),
                "aggressive": (25, 500)
            }
            req, delay = configs.get(intensity, configs["normal"])
            
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "rate_limit_probe.ps1",
                ["-RequestsPerSide", str(req), "-DelayMsBetweenRequests", str(delay)]
            )
            
            # Parse success rates from output
            success_info = "Ïã§Ìñâ ÏôÑÎ£å"
            if result["success"]:
                stdout = result["stdout"]
                canary_match = re.search(r'Canary:.*?(\d+)/(\d+).*?Success', stdout)
                legacy_match = re.search(r'Legacy:.*?(\d+)/(\d+).*?Success', stdout)
                
                if canary_match and legacy_match:
                    success_info = f"Canary: {canary_match.group(1)}/{canary_match.group(2)}, Legacy: {legacy_match.group(1)}/{legacy_match.group(2)}"
            
            return {
                "success": result["success"],
                "intensity": intensity,
                "result": success_info
            }
        
        elif name == "get_logs":
            hours = int(args.get("hours", 1))
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "filter_logs_by_time.ps1",
                ["-Last", f"{hours}h", "-ShowSummary"]
            )
            return {
                "success": result["success"],
                "logs": result.get("stdout", "")[:1000] if result["success"] else "Î°úÍ∑∏ Ï°∞Ìöå Ïã§Ìå®"
            }
        
        elif name == "generate_report":
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "generate_daily_report.ps1",
                ["-Hours", "24"]
            )
            return {
                "success": result["success"],
                "message": "Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± ÏôÑÎ£å" if result["success"] else "Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± Ïã§Ìå®"
            }
        
        elif name == "rollback":
            result = await self.executor.run_powershell(
                SCRIPTS_DIR / "rollback_phase4_canary.ps1",
                ["-ProjectId", GCP_PROJECT, "-AutoApprove"]
            )
            return {
                "success": result["success"],
                "message": "Î°§Î∞± ÏôÑÎ£å" if result["success"] else "Î°§Î∞± Ïã§Ìå®"
            }
        
        return {"success": False, "error": "Unknown function"}


# Initialize
executor = CommandExecutor()
gitco = GitcoAI(executor)


@app.post("/slack/events")
async def slack_events(request: Request):
    """Handle Slack Events API"""
    body = await request.json()
    
    # URL verification
    if body.get("type") == "url_verification":
        return JSONResponse({"challenge": body.get("challenge")})
    
    # Handle messages
    if body.get("type") == "event_callback":
        event = body.get("event", {})
        
        # Ignore bot messages
        if event.get("bot_id"):
            return JSONResponse({"ok": True})
        
        if event.get("type") == "message" and not event.get("subtype"):
            text = event.get("text", "")
            channel = event.get("channel")
            
            # Remove bot mention if present
            text = re.sub(r'<@[A-Z0-9]+>', '', text).strip()
            
            if text:
                # Generate response
                response_text = await gitco.chat(text, channel)
                
                # Send to Slack
                if slack_client:
                    try:
                        slack_client.chat_postMessage(
                            channel=channel,
                            text=response_text,
                            mrkdwn=True
                        )
                    except SlackApiError as e:
                        print(f"Slack error: {e}")
    
    return JSONResponse({"ok": True})


@app.get("/health")
async def health():
    """Health check"""
    return {
        "status": "ok",
        "bot_active": bool(slack_client),
        "model": "gemini-2.0-flash-exp"
    }


if __name__ == "__main__":
    if not SLACK_BOT_TOKEN:
        print("‚ö†Ô∏è  SLACK_BOT_TOKEN environment variable not set!")
    
    print(f"ü§ñ Starting Gitco - Natural Conversation Bot")
    print(f"üìç Project: {GCP_PROJECT}")
    print(f"üåê Server: http://localhost:8080")
    
    uvicorn.run(app, host="0.0.0.0", port=8080)
