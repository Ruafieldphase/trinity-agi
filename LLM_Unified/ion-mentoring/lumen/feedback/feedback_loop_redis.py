"""
Feedback Loop Redis Monitoring

Redis ìºì‹œ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  í”¼ë“œë°± ë£¨í”„ì— í†µí•©

Lumen v1.7 Resonance Memory íŒ¨í„´:
- Track A: Cache Performance (hit rate, latency, memory)
- Track B: Cost Efficiency (cache cost, request cost)
- Track C: Adaptive Signals (TTL, size, optimization)

ê°ì‘ â†’ ì¦ë¹™ â†’ ì ì‘:
1. ê°ì‘: Redis ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (hit rate, memory, latency)
2. ì¦ë¹™: ì„±ëŠ¥ ë¶„ì„ ë° ìƒíƒœ ì €ì¥
3. ì ì‘: TTL/Size ìµœì í™” ì œì•ˆ
"""

from google.cloud import monitoring_v3
import datetime
from typing import Dict, Optional, List, Tuple
import logging
import json
import os
from dataclasses import dataclass, asdict
from enum import Enum

logger = logging.getLogger(__name__)


class CacheHealthStatus(Enum):
    """ìºì‹œ ê±´ê°• ìƒíƒœ"""
    OPTIMAL = "OPTIMAL"          # ìµœì  (hit rate > 80%)
    GOOD = "GOOD"                # ì–‘í˜¸ (hit rate 60-80%)
    DEGRADED = "DEGRADED"        # ì €í•˜ (hit rate 40-60%)
    POOR = "POOR"                # ë¶ˆëŸ‰ (hit rate < 40%)


class OptimizationAction(Enum):
    """ìµœì í™” ì•¡ì…˜"""
    NONE = "NONE"                           # ì¡°ì¹˜ ë¶ˆí•„ìš”
    INCREASE_TTL = "INCREASE_TTL"           # TTL ì¦ê°€ (ë” ì˜¤ë˜ ìºì‹±)
    DECREASE_TTL = "DECREASE_TTL"           # TTL ê°ì†Œ (ë” ìì£¼ ê°±ì‹ )
    INCREASE_CACHE_SIZE = "INCREASE_CACHE_SIZE"  # ìºì‹œ í¬ê¸° ì¦ê°€
    DECREASE_CACHE_SIZE = "DECREASE_CACHE_SIZE"  # ìºì‹œ í¬ê¸° ê°ì†Œ
    CLEAR_CACHE = "CLEAR_CACHE"             # ìºì‹œ ì´ˆê¸°í™”


@dataclass
class CacheMetrics:
    """ìºì‹œ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    timestamp: str
    hit_rate: float              # ìºì‹œ íˆíŠ¸ìœ¨ (%)
    miss_rate: float             # ìºì‹œ ë¯¸ìŠ¤ìœ¨ (%)
    total_hits: int              # ì´ íˆíŠ¸ ìˆ˜
    total_misses: int            # ì´ ë¯¸ìŠ¤ ìˆ˜
    memory_usage_mb: float       # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
    memory_limit_mb: float       # ë©”ëª¨ë¦¬ ì œí•œ (MB)
    memory_usage_pct: float      # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (%)
    avg_latency_ms: float        # í‰ê·  ë ˆì´í„´ì‹œ (ms)
    eviction_count: int          # ì œê±°ëœ í‚¤ ìˆ˜
    current_ttl_seconds: int     # í˜„ì¬ TTL (ì´ˆ)
    
    def to_dict(self) -> dict:
        return asdict(self)


@dataclass
class CacheFeedback:
    """ìºì‹œ í”¼ë“œë°± ê²°ê³¼"""
    health_status: CacheHealthStatus
    optimization_action: OptimizationAction
    recommended_ttl_seconds: Optional[int]
    recommended_cache_size_mb: Optional[float]
    reasoning: str
    metrics: CacheMetrics
    
    def to_dict(self) -> dict:
        result = {
            "health_status": self.health_status.value,
            "optimization_action": self.optimization_action.value,
            "recommended_ttl_seconds": self.recommended_ttl_seconds,
            "recommended_cache_size_mb": self.recommended_cache_size_mb,
            "reasoning": self.reasoning,
            "metrics": self.metrics.to_dict()
        }
        return result


class FeedbackLoopRedis:
    """Redis ìºì‹œ í”¼ë“œë°± ë£¨í”„"""
    
    # TTL ë²”ìœ„ (ì´ˆ)
    MIN_TTL_SECONDS = 60        # 1ë¶„
    MAX_TTL_SECONDS = 3600      # 1ì‹œê°„
    DEFAULT_TTL_SECONDS = 300   # 5ë¶„
    
    # ìºì‹œ í¬ê¸° ë²”ìœ„ (MB)
    MIN_CACHE_SIZE_MB = 10.0
    MAX_CACHE_SIZE_MB = 1024.0
    DEFAULT_CACHE_SIZE_MB = 256.0
    
    # íˆíŠ¸ìœ¨ ì„ê³„ê°’
    OPTIMAL_HIT_RATE = 80.0
    GOOD_HIT_RATE = 60.0
    DEGRADED_HIT_RATE = 40.0
    
    def __init__(self, project_id: str, service_name: str = "ion-api-canary"):
        """
        Args:
            project_id: GCP í”„ë¡œì íŠ¸ ID
            service_name: Cloud Run ì„œë¹„ìŠ¤ ì´ë¦„
        """
        self.project_id = project_id
        self.service_name = service_name
        self.monitoring_client = monitoring_v3.MetricServiceClient()
        self.project_name = f"projects/{project_id}"
        
        # ìƒíƒœ íŒŒì¼ ê²½ë¡œ
        self.state_file = os.path.join(
            os.path.dirname(__file__), 
            "../../outputs/feedback_loop_state.json"
        )
    
    def collect_cache_metrics(self, hours: int = 1) -> CacheMetrics:
        """
        Redis ìºì‹œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        
        Args:
            hours: ì¸¡ì • ê¸°ê°„ (ì‹œê°„)
        
        Returns:
            CacheMetrics: ìºì‹œ ì„±ëŠ¥ ë©”íŠ¸ë¦­
        """
        try:
            now = datetime.datetime.utcnow()
            end_time = now
            start_time = now - datetime.timedelta(hours=hours)
            
            interval = monitoring_v3.TimeInterval({
                "start_time": {"seconds": int(start_time.timestamp())},
                "end_time": {"seconds": int(end_time.timestamp())},
            })
            
            # 1. ìºì‹œ íˆíŠ¸ìœ¨ ì¡°íšŒ
            hit_rate = self._get_cache_hit_rate(interval)
            
            # 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ
            memory_usage_mb, memory_limit_mb = self._get_memory_usage(interval)
            
            # 3. ë ˆì´í„´ì‹œ ì¡°íšŒ
            avg_latency_ms = self._get_cache_latency(interval)
            
            # 4. ì œê±° íšŸìˆ˜ ì¡°íšŒ
            eviction_count = self._get_eviction_count(interval)
            
            # 5. í˜„ì¬ TTL ì¡°íšŒ (ìƒíƒœ íŒŒì¼ ë˜ëŠ” ê¸°ë³¸ê°’)
            current_ttl = self._get_current_ttl()
            
            # 6. Hit/Miss ê³„ì‚° (ì„ì‹œ: ì‹¤ì œë¡œëŠ” custom metric í•„ìš”)
            total_requests = 10000  # ì„ì‹œê°’
            total_hits = int(total_requests * hit_rate / 100)
            total_misses = total_requests - total_hits
            
            miss_rate = 100.0 - hit_rate
            memory_usage_pct = (memory_usage_mb / memory_limit_mb * 100) if memory_limit_mb > 0 else 0.0
            
            metrics = CacheMetrics(
                timestamp=now.isoformat(),
                hit_rate=hit_rate,
                miss_rate=miss_rate,
                total_hits=total_hits,
                total_misses=total_misses,
                memory_usage_mb=memory_usage_mb,
                memory_limit_mb=memory_limit_mb,
                memory_usage_pct=memory_usage_pct,
                avg_latency_ms=avg_latency_ms,
                eviction_count=eviction_count,
                current_ttl_seconds=current_ttl
            )
            
            logger.info(f"Collected cache metrics: hit_rate={hit_rate:.2f}%, memory={memory_usage_mb:.2f}MB")
            return metrics
        
        except Exception as e:
            logger.error(f"Failed to collect cache metrics: {e}")
            # Fallback: ë”ë¯¸ ë©”íŠ¸ë¦­
            return self._get_dummy_metrics()
    
    def _get_cache_hit_rate(self, interval: monitoring_v3.TimeInterval) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ì¡°íšŒ"""
        try:
            metric_type = "custom.googleapis.com/cache_hit_rate"
            
            request = monitoring_v3.ListTimeSeriesRequest({
                "name": self.project_name,
                "filter": f'metric.type="{metric_type}"',
                "interval": interval,
            })
            
            results = self.monitoring_client.list_time_series(request=request)
            
            for result in results:
                if result.points:
                    return result.points[-1].value.double_value
            
            # ê¸°ë³¸ê°’: Phase 14 ì¸¡ì • 60%
            return 60.0
        
        except Exception as e:
            logger.warning(f"Cache hit rate metric not found: {e}")
            return 60.0
    
    def _get_memory_usage(self, interval: monitoring_v3.TimeInterval) -> Tuple[float, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            # Cloud Run ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­
            metric_type = "run.googleapis.com/container/memory/utilizations"
            
            request = monitoring_v3.ListTimeSeriesRequest({
                "name": self.project_name,
                "filter": f'metric.type="{metric_type}" AND resource.labels.service_name="{self.service_name}"',
                "interval": interval,
            })
            
            results = self.monitoring_client.list_time_series(request=request)
            
            memory_usage_pct = 0.0
            for result in results:
                if result.points:
                    memory_usage_pct = result.points[-1].value.double_value * 100
                    break
            
            # Redis ê¸°ë³¸ ë©”ëª¨ë¦¬ ì œí•œ: 256MB
            memory_limit_mb = self.DEFAULT_CACHE_SIZE_MB
            memory_usage_mb = memory_limit_mb * (memory_usage_pct / 100)
            
            return memory_usage_mb, memory_limit_mb
        
        except Exception as e:
            logger.warning(f"Memory usage metric not found: {e}")
            return 150.0, 256.0  # 150MB / 256MB (58%)
    
    def _get_cache_latency(self, interval: monitoring_v3.TimeInterval) -> float:
        """ìºì‹œ ë ˆì´í„´ì‹œ ì¡°íšŒ"""
        try:
            # Custom metric: custom/cache_latency_ms
            metric_type = "custom.googleapis.com/cache_latency_ms"
            
            request = monitoring_v3.ListTimeSeriesRequest({
                "name": self.project_name,
                "filter": f'metric.type="{metric_type}"',
                "interval": interval,
            })
            
            results = self.monitoring_client.list_time_series(request=request)
            
            for result in results:
                if result.points:
                    return result.points[-1].value.double_value
            
            # ê¸°ë³¸ê°’: Redis í‰ê·  ë ˆì´í„´ì‹œ ~1ms
            return 1.0
        
        except Exception as e:
            logger.warning(f"Cache latency metric not found: {e}")
            return 1.0
    
    def _get_eviction_count(self, interval: monitoring_v3.TimeInterval) -> int:
        """ì œê±°ëœ í‚¤ ìˆ˜ ì¡°íšŒ"""
        try:
            # Custom metric: custom/cache_evictions
            metric_type = "custom.googleapis.com/cache_evictions"
            
            request = monitoring_v3.ListTimeSeriesRequest({
                "name": self.project_name,
                "filter": f'metric.type="{metric_type}"',
                "interval": interval,
            })
            
            results = self.monitoring_client.list_time_series(request=request)
            
            for result in results:
                if result.points:
                    return int(result.points[-1].value.int64_value)
            
            return 0
        
        except Exception as e:
            logger.warning(f"Cache eviction metric not found: {e}")
            return 0
    
    def _get_current_ttl(self) -> int:
        """í˜„ì¬ TTL ì¡°íšŒ (ìƒíƒœ íŒŒì¼)"""
        try:
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                    return state.get("current_ttl_seconds", self.DEFAULT_TTL_SECONDS)
        except Exception as e:
            logger.warning(f"Failed to read state file: {e}")
        
        return self.DEFAULT_TTL_SECONDS
    
    def _get_dummy_metrics(self) -> CacheMetrics:
        """ë”ë¯¸ ë©”íŠ¸ë¦­ (fallback)"""
        return CacheMetrics(
            timestamp=datetime.datetime.utcnow().isoformat(),
            hit_rate=60.0,
            miss_rate=40.0,
            total_hits=6000,
            total_misses=4000,
            memory_usage_mb=150.0,
            memory_limit_mb=256.0,
            memory_usage_pct=58.6,
            avg_latency_ms=1.0,
            eviction_count=0,
            current_ttl_seconds=self.DEFAULT_TTL_SECONDS
        )
    
    def analyze_cache_feedback(self, metrics: CacheMetrics) -> CacheFeedback:
        """
        ìºì‹œ ë©”íŠ¸ë¦­ ë¶„ì„ ë° í”¼ë“œë°± ìƒì„±
        
        Lumen v1.7 Resonance Memory:
        - Track A: Cache Performance (hit rate)
        - Track B: Memory Efficiency (usage vs limit)
        - Track C: Adaptive Signal (TTL optimization)
        
        Args:
            metrics: ìˆ˜ì§‘ëœ ìºì‹œ ë©”íŠ¸ë¦­
        
        Returns:
            CacheFeedback: ìµœì í™” ì œì•ˆ
        """
        # 1. Health Status ê²°ì •
        if metrics.hit_rate >= self.OPTIMAL_HIT_RATE:
            health_status = CacheHealthStatus.OPTIMAL
        elif metrics.hit_rate >= self.GOOD_HIT_RATE:
            health_status = CacheHealthStatus.GOOD
        elif metrics.hit_rate >= self.DEGRADED_HIT_RATE:
            health_status = CacheHealthStatus.DEGRADED
        else:
            health_status = CacheHealthStatus.POOR
        
        # 2. Optimization Action ê²°ì •
        action, recommended_ttl, recommended_size, reasoning = self._decide_optimization(
            metrics, health_status
        )
        
        feedback = CacheFeedback(
            health_status=health_status,
            optimization_action=action,
            recommended_ttl_seconds=recommended_ttl,
            recommended_cache_size_mb=recommended_size,
            reasoning=reasoning,
            metrics=metrics
        )
        
        logger.info(f"Cache feedback: {health_status.value}, action: {action.value}")
        return feedback
    
    def _decide_optimization(
        self, 
        metrics: CacheMetrics, 
        health_status: CacheHealthStatus
    ) -> Tuple[OptimizationAction, Optional[int], Optional[float], str]:
        """
        ìµœì í™” ì•¡ì…˜ ê²°ì • ë¡œì§
        
        Returns:
            (action, recommended_ttl, recommended_size, reasoning)
        """
        current_ttl = metrics.current_ttl_seconds
        memory_usage_pct = metrics.memory_usage_pct
        eviction_count = metrics.eviction_count
        hit_rate = metrics.hit_rate
        
        # Case 1: OPTIMAL - ì¡°ì¹˜ ë¶ˆí•„ìš”
        if health_status == CacheHealthStatus.OPTIMAL:
            if memory_usage_pct > 90:
                # ë©”ëª¨ë¦¬ ë¶€ì¡± â†’ í¬ê¸° ì¦ê°€
                new_size = min(metrics.memory_limit_mb * 1.5, self.MAX_CACHE_SIZE_MB)
                return (
                    OptimizationAction.INCREASE_CACHE_SIZE,
                    None,
                    new_size,
                    f"ë†’ì€ íˆíŠ¸ìœ¨({hit_rate:.1f}%)ì´ì§€ë§Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ({memory_usage_pct:.1f}%) ë†’ìŒ. "
                    f"ìºì‹œ í¬ê¸° ì¦ê°€ ê¶Œì¥."
                )
            else:
                return (
                    OptimizationAction.NONE,
                    None,
                    None,
                    f"ìºì‹œê°€ ìµœì  ìƒíƒœì…ë‹ˆë‹¤. íˆíŠ¸ìœ¨: {hit_rate:.1f}%, ë©”ëª¨ë¦¬: {memory_usage_pct:.1f}%"
                )
        
        # Case 2: GOOD - TTL ë¯¸ì„¸ ì¡°ì •
        if health_status == CacheHealthStatus.GOOD:
            if eviction_count > 100:
                # ë§ì€ ì œê±° â†’ TTL ê°ì†Œ ë˜ëŠ” í¬ê¸° ì¦ê°€
                new_ttl = max(current_ttl - 60, self.MIN_TTL_SECONDS)
                return (
                    OptimizationAction.DECREASE_TTL,
                    new_ttl,
                    None,
                    f"ë§ì€ í‚¤ ì œê±°({eviction_count}ê°œ) ë°œìƒ. TTLì„ {current_ttl}ì´ˆ â†’ {new_ttl}ì´ˆë¡œ ê°ì†Œí•˜ì—¬ "
                    f"ë©”ëª¨ë¦¬ ì••ë°• ì™„í™”."
                )
            elif hit_rate < 70 and current_ttl < self.MAX_TTL_SECONDS:
                # íˆíŠ¸ìœ¨ ê°œì„  ì—¬ì§€ â†’ TTL ì¦ê°€
                new_ttl = min(current_ttl + 120, self.MAX_TTL_SECONDS)
                return (
                    OptimizationAction.INCREASE_TTL,
                    new_ttl,
                    None,
                    f"íˆíŠ¸ìœ¨({hit_rate:.1f}%) ê°œì„  ê°€ëŠ¥. TTLì„ {current_ttl}ì´ˆ â†’ {new_ttl}ì´ˆë¡œ ì¦ê°€í•˜ì—¬ "
                    f"ìºì‹± íš¨ê³¼ í–¥ìƒ."
                )
            else:
                return (
                    OptimizationAction.NONE,
                    None,
                    None,
                    f"ìºì‹œê°€ ì–‘í˜¸í•©ë‹ˆë‹¤. íˆíŠ¸ìœ¨: {hit_rate:.1f}%, TTL: {current_ttl}ì´ˆ"
                )
        
        # Case 3: DEGRADED - ì ê·¹ì  ì¡°ì •
        if health_status == CacheHealthStatus.DEGRADED:
            if memory_usage_pct > 80:
                # ë©”ëª¨ë¦¬ ë¶€ì¡± â†’ í¬ê¸° ì¦ê°€
                new_size = min(metrics.memory_limit_mb * 1.5, self.MAX_CACHE_SIZE_MB)
                return (
                    OptimizationAction.INCREASE_CACHE_SIZE,
                    None,
                    new_size,
                    f"íˆíŠ¸ìœ¨ ì €í•˜({hit_rate:.1f}%)ì™€ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ({memory_usage_pct:.1f}%). "
                    f"ìºì‹œ í¬ê¸° {metrics.memory_limit_mb:.0f}MB â†’ {new_size:.0f}MBë¡œ ì¦ê°€ ê¶Œì¥."
                )
            elif current_ttl < 600:
                # TTLì´ ì§§ìŒ â†’ ì¦ê°€
                new_ttl = min(current_ttl * 2, self.MAX_TTL_SECONDS)
                return (
                    OptimizationAction.INCREASE_TTL,
                    new_ttl,
                    None,
                    f"íˆíŠ¸ìœ¨ ì €í•˜({hit_rate:.1f}%). TTLì´ ì§§ìŒ({current_ttl}ì´ˆ). "
                    f"{new_ttl}ì´ˆë¡œ ì¦ê°€í•˜ì—¬ ìºì‹± íš¨ê³¼ í–¥ìƒ."
                )
            else:
                return (
                    OptimizationAction.CLEAR_CACHE,
                    None,
                    None,
                    f"íˆíŠ¸ìœ¨ ì €í•˜({hit_rate:.1f}%). ìºì‹œ íŒ¨í„´ ë³€ê²½ ê°€ëŠ¥ì„±. "
                    f"ìºì‹œ ì´ˆê¸°í™” í›„ ì¬êµ¬ì¶• ê¶Œì¥."
                )
        
        # Case 4: POOR - ê¸´ê¸‰ ì¡°ì¹˜
        if health_status == CacheHealthStatus.POOR:
            return (
                OptimizationAction.CLEAR_CACHE,
                self.DEFAULT_TTL_SECONDS,
                None,
                f"ìºì‹œ íˆíŠ¸ìœ¨ ì‹¬ê°({hit_rate:.1f}%). ìºì‹œ ì´ˆê¸°í™” ë° TTL {self.DEFAULT_TTL_SECONDS}ì´ˆë¡œ ì¬ì„¤ì • í•„ìš”."
            )
        
        # Default
        return (
            OptimizationAction.NONE,
            None,
            None,
            "ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ"
        )
    
    def save_feedback_state(self, feedback: CacheFeedback):
        """í”¼ë“œë°± ìƒíƒœ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(self.state_file), exist_ok=True)
            
            state = {
                "timestamp": feedback.metrics.timestamp,
                "health_status": feedback.health_status.value,
                "optimization_action": feedback.optimization_action.value,
                "current_ttl_seconds": feedback.metrics.current_ttl_seconds,
                "recommended_ttl_seconds": feedback.recommended_ttl_seconds,
                "recommended_cache_size_mb": feedback.recommended_cache_size_mb,
                "hit_rate": feedback.metrics.hit_rate,
                "memory_usage_mb": feedback.metrics.memory_usage_mb,
                "reasoning": feedback.reasoning
            }
            
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Saved feedback state to {self.state_file}")
        
        except Exception as e:
            logger.error(f"Failed to save feedback state: {e}")
    
    def generate_feedback_report(self, feedback: CacheFeedback) -> str:
        """
        Markdown í”¼ë“œë°± ë¦¬í¬íŠ¸ ìƒì„±
        
        Returns:
            Markdown í˜•ì‹ ë¦¬í¬íŠ¸
        """
        m = feedback.metrics
        
        report = f"""# Redis Cache Feedback Report

**ìƒì„± ì‹œê°**: {m.timestamp}

---

## ğŸ“Š Cache Health Status

**ìƒíƒœ**: {feedback.health_status.value}

| ë©”íŠ¸ë¦­ | ê°’ | ìƒíƒœ |
|--------|-----|------|
| **íˆíŠ¸ìœ¨** | {m.hit_rate:.2f}% | {'ğŸŸ¢ OPTIMAL' if m.hit_rate >= 80 else 'ğŸŸ¡ GOOD' if m.hit_rate >= 60 else 'ğŸŸ  DEGRADED' if m.hit_rate >= 40 else 'ğŸ”´ POOR'} |
| **ë¯¸ìŠ¤ìœ¨** | {m.miss_rate:.2f}% | - |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ** | {m.memory_usage_pct:.2f}% ({m.memory_usage_mb:.2f}MB / {m.memory_limit_mb:.2f}MB) | {'ğŸŸ¢' if m.memory_usage_pct < 70 else 'ğŸŸ¡' if m.memory_usage_pct < 85 else 'ğŸ”´'} |
| **í‰ê·  ë ˆì´í„´ì‹œ** | {m.avg_latency_ms:.2f}ms | {'ğŸŸ¢' if m.avg_latency_ms < 5 else 'ğŸŸ¡' if m.avg_latency_ms < 10 else 'ğŸ”´'} |
| **ì œê±°ëœ í‚¤** | {m.eviction_count} | {'ğŸŸ¢' if m.eviction_count < 10 else 'ğŸŸ¡' if m.eviction_count < 100 else 'ğŸ”´'} |
| **í˜„ì¬ TTL** | {m.current_ttl_seconds}ì´ˆ | - |

---

## ğŸ¯ Optimization Action

**ê¶Œì¥ ì•¡ì…˜**: {feedback.optimization_action.value}

**ìƒì„¸ ë¶„ì„**:
{feedback.reasoning}

**ê¶Œì¥ ì„¤ì •**:
"""
        
        if feedback.recommended_ttl_seconds:
            report += f"- **TTL**: {m.current_ttl_seconds}ì´ˆ â†’ **{feedback.recommended_ttl_seconds}ì´ˆ**\n"
        
        if feedback.recommended_cache_size_mb:
            report += f"- **ìºì‹œ í¬ê¸°**: {m.memory_limit_mb:.0f}MB â†’ **{feedback.recommended_cache_size_mb:.0f}MB**\n"
        
        if feedback.optimization_action == OptimizationAction.NONE:
            report += "- í˜„ì¬ ì„¤ì • ìœ ì§€\n"
        
        report += f"""

---

## ğŸ“ˆ Performance Metrics

### Hit/Miss Statistics
- **ì´ íˆíŠ¸**: {m.total_hits:,}
- **ì´ ë¯¸ìŠ¤**: {m.total_misses:,}
- **íˆíŠ¸ìœ¨**: {m.hit_rate:.2f}%

### Memory Usage
- **ì‚¬ìš©ëŸ‰**: {m.memory_usage_mb:.2f}MB
- **ì œí•œ**: {m.memory_limit_mb:.2f}MB
- **ì‚¬ìš©ë¥ **: {m.memory_usage_pct:.2f}%

### Latency
- **í‰ê· **: {m.avg_latency_ms:.2f}ms

---

## ğŸ”„ Lumen v1.7 Resonance Memory

### Track A: Cache Performance
- Hit Rate: {m.hit_rate:.2f}%
- Latency: {m.avg_latency_ms:.2f}ms

### Track B: Memory Efficiency
- Usage: {m.memory_usage_pct:.2f}%
- Evictions: {m.eviction_count}

### Track C: Adaptive Signal
- Current TTL: {m.current_ttl_seconds}s
- Recommended: {feedback.recommended_ttl_seconds or m.current_ttl_seconds}s

---

**ìƒì„±**: Lumen Feedback Loop Redis v1.0
"""
        
        return report
    
    def run_feedback_loop(self) -> CacheFeedback:
        """
        í”¼ë“œë°± ë£¨í”„ ì‹¤í–‰: ìˆ˜ì§‘ â†’ ë¶„ì„ â†’ ì €ì¥ â†’ ë¦¬í¬íŠ¸
        
        Returns:
            CacheFeedback: í”¼ë“œë°± ê²°ê³¼
        """
        logger.info("Starting Redis cache feedback loop...")
        
        # 1. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ê°ì‘)
        metrics = self.collect_cache_metrics(hours=1)
        
        # 2. í”¼ë“œë°± ë¶„ì„ (ì¦ë¹™)
        feedback = self.analyze_cache_feedback(metrics)
        
        # 3. ìƒíƒœ ì €ì¥ (ì¦ë¹™)
        self.save_feedback_state(feedback)
        
        # 4. ë¦¬í¬íŠ¸ ìƒì„± (ì¦ë¹™)
        report = self.generate_feedback_report(feedback)
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        report_file = os.path.join(
            os.path.dirname(self.state_file),
            "feedback_loop_report.md"
        )
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"Saved feedback report to {report_file}")
        except Exception as e:
            logger.error(f"Failed to save report: {e}")
        
        logger.info(f"Feedback loop completed: {feedback.health_status.value}, {feedback.optimization_action.value}")
        return feedback


if __name__ == "__main__":
    import sys
    
    # Logging ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # í”„ë¡œì íŠ¸ ID
    project_id = os.environ.get("GCP_PROJECT_ID", "naeda-genesis")
    service_name = os.environ.get("SERVICE_NAME", "ion-api-canary")
    
    # í”¼ë“œë°± ë£¨í”„ ì‹¤í–‰
    feedback_loop = FeedbackLoopRedis(project_id, service_name)
    feedback = feedback_loop.run_feedback_loop()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print(f"Health Status: {feedback.health_status.value}")
    print(f"Optimization Action: {feedback.optimization_action.value}")
    print(f"Hit Rate: {feedback.metrics.hit_rate:.2f}%")
    print(f"Memory Usage: {feedback.metrics.memory_usage_pct:.2f}%")
    print("\nReasoning:")
    print(feedback.reasoning)
    print("=" * 60)
    
    # Exit code
    if feedback.health_status == CacheHealthStatus.OPTIMAL:
        sys.exit(0)
    elif feedback.health_status == CacheHealthStatus.GOOD:
        sys.exit(0)
    elif feedback.health_status == CacheHealthStatus.DEGRADED:
        sys.exit(1)
    else:  # POOR
        sys.exit(2)
