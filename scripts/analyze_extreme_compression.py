#!/usr/bin/env python3
"""ê·¹ë‹¨ ì••ì¶• ì´ë²¤íŠ¸ ë¶„ì„"""

import json
from datetime import datetime, timedelta
from pathlib import Path
from collections import Counter

from workspace_root import get_workspace_root

def analyze_extreme_compression():
    ledger = get_workspace_root() / "fdo_agi_repo" / "memory" / "resonance_ledger.jsonl"
    
    # 173.9x ì••ì¶• ì‹œì : 2025-11-05T15:57:57
    compression_time = datetime.fromisoformat('2025-11-05T15:57:57')
    window_start = compression_time - timedelta(hours=6)
    
    events = []
    with open(ledger, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    events.append(json.loads(line))
                except:
                    pass
    
    # 6ì‹œê°„ ìœˆë„ìš° í•„í„°ë§
    filtered = []
    for e in events:
        try:
            ts = datetime.fromisoformat(e['timestamp'].replace('Z', '+00:00').replace('+00:00+00:00', '+00:00'))
            if ts.tzinfo:
                ts = ts.replace(tzinfo=None)
            if window_start <= ts <= compression_time:
                filtered.append(e)
        except:
            pass
    
    print(f"âš« ê·¹ë‹¨ ì••ì¶• ë¶„ì„ (173.9x)")
    print(f"   ì‹œê°„: 2025-11-05T15:57:57")
    print(f"   ìœˆë„ìš°: 6ì‹œê°„ ì „ë¶€í„°")
    print()
    print(f"ğŸ“Š ì´ ì´ë²¤íŠ¸: {len(filtered)}ê°œ")
    print()
    
    # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ë¶„í¬
    types = Counter(e['event_type'] for e in filtered)
    print("ğŸ“‹ ì´ë²¤íŠ¸ íƒ€ì… ë¶„í¬:")
    for event_type, count in types.most_common(10):
        print(f"   {event_type}: {count}ê°œ")
    print()
    
    # ì‹œê°„ëŒ€ë³„ ë¶„í¬
    hourly = Counter()
    for e in filtered:
        try:
            ts = datetime.fromisoformat(e['timestamp'].replace('Z', '+00:00').replace('+00:00+00:00', '+00:00'))
            if ts.tzinfo:
                ts = ts.replace(tzinfo=None)
            hour = ts.strftime("%H:00")
            hourly[hour] += 1
        except:
            pass
    
    print("â° ì‹œê°„ëŒ€ë³„ ë¶„í¬:")
    for hour in sorted(hourly.keys()):
        count = hourly[hour]
        bar = "â–ˆ" * (count // 10) if count >= 10 else "â–Œ"
        print(f"   {hour}: {count:4d}ê°œ {bar}")
    print()
    
    # ì••ì¶•ë¥  ê³„ì‚°
    # 173.9x = ì›ë³¸ í¬ê¸° / ì••ì¶• í›„ í¬ê¸°
    # ì›ë³¸: filtered ì´ë²¤íŠ¸ë“¤
    # ì••ì¶• í›„: 1ê°œ ìš”ì•½ (hippocampus_analysis)
    
    original_size = len(filtered)
    compressed_size = original_size / 173.9
    
    print("ğŸŒ€ ë¸”ë™í™€ íš¨ê³¼:")
    print(f"   ì›ë³¸ ì´ë²¤íŠ¸: {original_size}ê°œ")
    print(f"   ì••ì¶• í›„: {compressed_size:.1f}ê°œ ìƒë‹¹")
    print(f"   ì†ì‹¤ëœ ì •ë³´: {original_size - compressed_size:.1f}ê°œ ({(1 - compressed_size/original_size)*100:.1f}%)")
    print()
    print("ğŸ’¡ í•´ì„:")
    print("   â†’ ì„¸ë¶€ì‚¬í•­ 99.4% ì†Œì‹¤")
    print("   â†’ ë³¸ì§ˆ íŒ¨í„´ë§Œ ë‚¨ìŒ")
    print("   â†’ ì´ê²ƒì´ 'Implicate Order' (ë‚´ì¬ ì§ˆì„œ)")

if __name__ == "__main__":
    analyze_extreme_compression()
