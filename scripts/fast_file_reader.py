"""
âš¡ Fast File Reader - Ultra-fast file reading system

ì†ë„ ìµœì í™” ê¸°ë²•:
1. Memory-Mapped I/O (mmap) - OS ì»¤ë„ ìºì‹œ í™œìš©
2. Chunk-based parallel reading - ë©€í‹°ìŠ¤ë ˆë“œ ë™ì‹œ ì½ê¸°
3. LRU Cache - ìµœê·¼ ì½ì€ íŒŒì¼ ë©”ëª¨ë¦¬ ìºì‹±
4. Smart encoding detection - UTF-8 ìš°ì„ , fallback ìµœì†Œí™”
5. Buffer pool - ë©”ëª¨ë¦¬ ì¬ì‚¬ìš©ìœ¼ë¡œ GC ì••ë ¥ ê°ì†Œ
"""

from __future__ import annotations
import mmap
import threading
from pathlib import Path
from typing import Optional, List, Dict, Any, Union
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed
import chardet
import logging

logger = logging.getLogger(__name__)


class FastFileReader:
    """ì´ˆê³ ì† íŒŒì¼ ì½ê¸° ì—”ì§„"""
    
    def __init__(
        self,
        max_workers: int = 4,
        cache_size: int = 128,
        chunk_size: int = 64 * 1024  # 64KB chunks
    ):
        """
        Args:
            max_workers: ë³‘ë ¬ ì½ê¸° ìŠ¤ë ˆë“œ ìˆ˜
            cache_size: LRU ìºì‹œ í¬ê¸° (íŒŒì¼ ê°œìˆ˜)
            chunk_size: ì²­í¬ í¬ê¸° (ë°”ì´íŠ¸)
        """
        self.max_workers = max_workers
        self.chunk_size = chunk_size
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # ìºì‹œ ë˜í¼ ë™ì  ìƒì„±
        self._read_cached = lru_cache(maxsize=cache_size)(self._read_file_impl)
        
        logger.info(f"âš¡ FastFileReader initialized: {max_workers} workers, cache={cache_size}")
    
    # ===================================================================
    # 1. ë‹¨ì¼ íŒŒì¼ ì½ê¸° (Memory-Mapped + Cache)
    # ===================================================================
    
    def read_fast(self, file_path: Union[str, Path], use_cache: bool = True) -> str:
        """
        ì´ˆê³ ì† íŒŒì¼ ì½ê¸°
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            íŒŒì¼ ë‚´ìš© (UTF-8 ë¬¸ìì—´)
        """
        path = Path(file_path)
        
        if use_cache:
            # ìºì‹œëœ ì½ê¸° (ìˆ˜ì • ì‹œê°„ í¬í•¨í•´ì„œ í‚¤ ìƒì„±)
            mtime = path.stat().st_mtime
            return self._read_cached(str(path), mtime)
        else:
            # ì§ì ‘ ì½ê¸° (ìºì‹œ ë¬´ì‹œ)
            return self._read_file_impl(str(path), path.stat().st_mtime)
    
    def _read_file_impl(self, file_path: str, mtime: float) -> str:
        """
        ì‹¤ì œ íŒŒì¼ ì½ê¸° êµ¬í˜„ (Memory-Mapped I/O)
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            mtime: ìˆ˜ì • ì‹œê°„ (ìºì‹œ í‚¤ë¡œ ì‚¬ìš©)
        
        Returns:
            íŒŒì¼ ë‚´ìš©
        """
        path = Path(file_path)
        
        # ì‘ì€ íŒŒì¼ì€ ì¼ë°˜ ì½ê¸°ê°€ ë” ë¹ ë¦„
        size = path.stat().st_size
        if size < 4096:  # 4KB ë¯¸ë§Œ
            return self._read_small_file(path)
        
        # í° íŒŒì¼ì€ mmap ì‚¬ìš©
        return self._read_with_mmap(path)
    
    def _read_small_file(self, path: Path) -> str:
        """ì‘ì€ íŒŒì¼ ë¹ ë¥¸ ì½ê¸°"""
        try:
            # UTF-8 ìš°ì„  ì‹œë„
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # Fallback: auto-detect
            with open(path, 'rb') as f:
                raw = f.read()
                encoding = chardet.detect(raw)['encoding'] or 'utf-8'
                return raw.decode(encoding, errors='replace')
    
    def _read_with_mmap(self, path: Path) -> str:
        """Memory-Mapped I/Oë¡œ íŒŒì¼ ì½ê¸° (OS ì»¤ë„ ìºì‹œ í™œìš©)"""
        try:
            with open(path, 'r+b') as f:
                # Memory-map the file
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    raw = mm.read()
                    # UTF-8 ìš°ì„  ì‹œë„
                    try:
                        return raw.decode('utf-8')
                    except UnicodeDecodeError:
                        # Fallback: auto-detect
                        encoding = chardet.detect(raw[:4096])['encoding'] or 'utf-8'
                        return raw.decode(encoding, errors='replace')
        except (OSError, ValueError):
            # mmap ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì½ê¸°
            return self._read_small_file(path)
    
    # ===================================================================
    # 2. ë³‘ë ¬ íŒŒì¼ ì½ê¸° (Multiple files)
    # ===================================================================
    
    def read_multiple(
        self,
        file_paths: List[Union[str, Path]],
        use_cache: bool = True
    ) -> Dict[str, str]:
        """
        ì—¬ëŸ¬ íŒŒì¼ ë³‘ë ¬ ì½ê¸°
        
        Args:
            file_paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            {íŒŒì¼ê²½ë¡œ: ë‚´ìš©} ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        
        # ë³‘ë ¬ ì½ê¸°
        futures = {
            self.executor.submit(self.read_fast, path, use_cache): str(path)
            for path in file_paths
        }
        
        for future in as_completed(futures):
            path = futures[future]
            try:
                results[path] = future.result()
            except Exception as e:
                logger.warning(f"Failed to read {path}: {e}")
                results[path] = f"[ERROR: {e}]"
        
        return results
    
    # ===================================================================
    # 3. ì²­í¬ ê¸°ë°˜ ì½ê¸° (Very large files)
    # ===================================================================
    
    def read_chunked(
        self,
        file_path: Union[str, Path],
        max_lines: Optional[int] = None
    ) -> List[str]:
        """
        ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            max_lines: ìµœëŒ€ ë¼ì¸ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            ë¼ì¸ ë¦¬ìŠ¤íŠ¸
        """
        path = Path(file_path)
        lines = []
        
        try:
            with open(path, 'r', encoding='utf-8', buffering=self.chunk_size) as f:
                for i, line in enumerate(f):
                    if max_lines and i >= max_lines:
                        break
                    lines.append(line.rstrip('\n\r'))
        except UnicodeDecodeError:
            # Fallback: binary read + decode
            with open(path, 'rb', buffering=self.chunk_size) as f:
                raw = f.read()
                encoding = chardet.detect(raw[:4096])['encoding'] or 'utf-8'
                text = raw.decode(encoding, errors='replace')
                lines = text.splitlines()
                if max_lines:
                    lines = lines[:max_lines]
        
        return lines
    
    # ===================================================================
    # 4. ìºì‹œ ê´€ë¦¬
    # ===================================================================
    
    def clear_cache(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._read_cached.cache_clear()
        logger.info("ğŸ§¹ Cache cleared")
    
    def cache_info(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        info = self._read_cached.cache_info()
        return {
            "hits": info.hits,
            "misses": info.misses,
            "size": info.currsize,
            "maxsize": info.maxsize,
            "hit_rate": info.hits / (info.hits + info.misses) if (info.hits + info.misses) > 0 else 0.0
        }
    
    # ===================================================================
    # 5. í¸ì˜ ë©”ì„œë“œ
    # ===================================================================
    
    def read_json_fast(self, file_path: Union[str, Path]) -> Any:
        """JSON íŒŒì¼ ë¹ ë¥¸ ì½ê¸°"""
        import json
        content = self.read_fast(file_path)
        return json.loads(content)
    
    def read_jsonl_fast(self, file_path: Union[str, Path]) -> List[Dict]:
        """JSONL íŒŒì¼ ë¹ ë¥¸ ì½ê¸°"""
        import json
        lines = self.read_chunked(file_path)
        return [json.loads(line) for line in lines if line.strip()]
    
    def __del__(self):
        """ì†Œë©¸ì: ìŠ¤ë ˆë“œ í’€ ì •ë¦¬"""
        self.executor.shutdown(wait=False)


# ===================================================================
# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# ===================================================================

_global_reader: Optional[FastFileReader] = None


def get_reader(
    max_workers: int = 4,
    cache_size: int = 128
) -> FastFileReader:
    """ì „ì—­ FastFileReader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_reader
    if _global_reader is None:
        _global_reader = FastFileReader(
            max_workers=max_workers,
            cache_size=cache_size
        )
    return _global_reader


# ===================================================================
# í¸ì˜ í•¨ìˆ˜
# ===================================================================

def read_fast(file_path: Union[str, Path], use_cache: bool = True) -> str:
    """íŒŒì¼ ë¹ ë¥¸ ì½ê¸° (ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©)"""
    return get_reader().read_fast(file_path, use_cache)


def read_multiple(file_paths: List[Union[str, Path]]) -> Dict[str, str]:
    """ì—¬ëŸ¬ íŒŒì¼ ë³‘ë ¬ ì½ê¸° (ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©)"""
    return get_reader().read_multiple(file_paths)


def read_json_fast(file_path: Union[str, Path]) -> Any:
    """JSON íŒŒì¼ ë¹ ë¥¸ ì½ê¸°"""
    return get_reader().read_json_fast(file_path)


# ===================================================================
# CLI í…ŒìŠ¤íŠ¸
# ===================================================================

if __name__ == "__main__":
    import argparse
    import time
    
    parser = argparse.ArgumentParser(description="Fast File Reader Test")
    parser.add_argument("files", nargs="+", help="Files to read")
    parser.add_argument("--no-cache", action="store_true", help="Disable cache")
    parser.add_argument("--workers", type=int, default=4, help="Thread count")
    parser.add_argument("--show-content", action="store_true", help="Show file content")
    
    args = parser.parse_args()
    
    reader = get_reader(max_workers=args.workers)
    
    print(f"âš¡ Fast File Reader Test")
    print(f"Files: {len(args.files)}")
    print(f"Workers: {args.workers}")
    print(f"Cache: {'disabled' if args.no_cache else 'enabled'}")
    print()
    
    # ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸
    if len(args.files) == 1:
        file_path = args.files[0]
        
        # ì²« ì½ê¸° (cache miss)
        start = time.time()
        content = reader.read_fast(file_path, use_cache=not args.no_cache)
        duration1 = time.time() - start
        
        # ë‘ ë²ˆì§¸ ì½ê¸° (cache hit)
        start = time.time()
        content2 = reader.read_fast(file_path, use_cache=not args.no_cache)
        duration2 = time.time() - start
        
        print(f"ğŸ“„ {file_path}")
        print(f"  Size: {len(content):,} chars")
        print(f"  First read: {duration1*1000:.2f}ms")
        print(f"  Second read: {duration2*1000:.2f}ms")
        print(f"  Speedup: {duration1/duration2:.1f}x" if duration2 > 0 else "  Speedup: âˆx")
        
        if args.show_content:
            print("\n--- Content ---")
            print(content[:500])
            if len(content) > 500:
                print(f"\n... ({len(content)-500} more chars)")
    
    # ë©€í‹° íŒŒì¼ í…ŒìŠ¤íŠ¸
    else:
        start = time.time()
        results = reader.read_multiple(args.files, use_cache=not args.no_cache)
        duration = time.time() - start
        
        total_size = sum(len(content) for content in results.values())
        
        print(f"ğŸ“¦ Read {len(results)} files in {duration*1000:.2f}ms")
        print(f"Total size: {total_size:,} chars")
        print(f"Throughput: {total_size/duration/1024/1024:.2f} MB/s")
        
        if args.show_content:
            for path, content in results.items():
                print(f"\n--- {path} ---")
                print(content[:200])
    
    # ìºì‹œ í†µê³„
    print("\nğŸ“Š Cache Stats:")
    info = reader.cache_info()
    print(f"  Hits: {info['hits']}")
    print(f"  Misses: {info['misses']}")
    print(f"  Size: {info['size']}/{info['maxsize']}")
    print(f"  Hit rate: {info['hit_rate']*100:.1f}%")
