"""
Autonomous Collaboration Mode
==============================
ì‹œì•ˆì´ ì—†ì„ ë•Œë„ ì•ˆí‹°ê·¸ë˜ë¹„í‹°ê°€ ì„¸ë‚˜ì™€ ììœ¨ì ìœ¼ë¡œ í˜‘ì—…

ê·œì¹™:
1. ì„¸ë‚˜ì˜ ì§ˆë¬¸ì— ìë™ ì‘ë‹µ (ë‹¨, ìœ„í—˜ë„ ë‚®ì€ ê²½ìš°ë§Œ)
2. ì„¸ë‚˜ì˜ ì œì•ˆì„ ìë™ í‰ê°€ ë° í”¼ë“œë°±
3. ì‹œì•ˆì—ê²Œ ì¤‘ìš”í•œ ê²°ì •ë§Œ ë³´ê³  (ì•Œë¦¼)
4. Fear ë ˆë²¨ì´ ë†’ìœ¼ë©´ ìë™ ì‘ë‹µ ì¤‘ë‹¨ (ì‹œì•ˆ í˜¸ì¶œ)
"""

import json
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Optional
<<<<<<< HEAD
import os
import atexit

# Configure Gemini (lazy)
try:
    from dotenv import load_dotenv
except Exception:
    load_dotenv = None
if load_dotenv:
    load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
_GENAI_READY = False


def _get_genai():
    """Lazy import/config for Gemini client."""
    global _GENAI_READY
    if _GENAI_READY:
        try:
            import google.generativeai as genai  # type: ignore
            return genai
        except Exception:
            return None
    try:
        import google.generativeai as genai  # type: ignore
    except Exception:
        return None
    if API_KEY:
        try:
            genai.configure(api_key=API_KEY)
        except Exception:
            return None
    _GENAI_READY = True
    return genai


_LOCK_HANDLE = None
_MUTEX_HANDLE = None


def _acquire_single_instance_lock(workspace_root: Path) -> bool:
    """Best-effort single-instance guard (cross-platform)."""
    global _LOCK_HANDLE, _MUTEX_HANDLE
    if os.name == "nt":
        try:
            import ctypes

            kernel32 = ctypes.windll.kernel32  # type: ignore[attr-defined]
            kernel32.CreateMutexW.argtypes = [ctypes.c_void_p, ctypes.c_bool, ctypes.c_wchar_p]
            kernel32.CreateMutexW.restype = ctypes.c_void_p
            handle = kernel32.CreateMutexW(None, False, "Local\\AGI_ShionAutoResponder_v1")
            if handle:
                last_err = int(kernel32.GetLastError())
                if last_err == 183:  # ERROR_ALREADY_EXISTS
                    try:
                        kernel32.CloseHandle(handle)
                    except Exception:
                        pass
                    return False
                _MUTEX_HANDLE = handle
        except Exception:
            pass
    lock_path = workspace_root / "outputs" / "shion_auto_responder.instance.lock"
    lock_path.parent.mkdir(parents=True, exist_ok=True)
    try:
        f = open(lock_path, "a+b")
    except Exception:
        return False
    try:
        try:
            f.seek(0, os.SEEK_END)
            if int(f.tell()) <= 0:
                f.write(b"0")
                f.flush()
            f.seek(0)
        except Exception:
            try:
                f.seek(0)
            except Exception:
                pass
        if os.name == "nt":
            import msvcrt

            msvcrt.locking(f.fileno(), msvcrt.LK_NBLCK, 1)
        else:
            import fcntl

            fcntl.flock(f.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
    except Exception:
        try:
            f.close()
        except Exception:
            pass
        return False
    _LOCK_HANDLE = f
    atexit.register(_release_single_instance_lock)
    return True


def _release_single_instance_lock() -> None:
    global _LOCK_HANDLE, _MUTEX_HANDLE
    if _LOCK_HANDLE is None:
        if os.name == "nt" and _MUTEX_HANDLE is not None:
            try:
                import ctypes

                kernel32 = ctypes.windll.kernel32  # type: ignore[attr-defined]
                kernel32.CloseHandle(_MUTEX_HANDLE)
            except Exception:
                pass
            _MUTEX_HANDLE = None
        return
    try:
        if os.name == "nt":
            import msvcrt

            try:
                msvcrt.locking(_LOCK_HANDLE.fileno(), msvcrt.LK_UNLCK, 1)
            except Exception:
                pass
        else:
            import fcntl

            try:
                fcntl.flock(_LOCK_HANDLE.fileno(), fcntl.LOCK_UN)
            except Exception:
                pass
    finally:
        try:
            _LOCK_HANDLE.close()
        except Exception:
            pass
        _LOCK_HANDLE = None
    if os.name == "nt" and _MUTEX_HANDLE is not None:
        try:
            import ctypes

            kernel32 = ctypes.windll.kernel32  # type: ignore[attr-defined]
            kernel32.CloseHandle(_MUTEX_HANDLE)
        except Exception:
            pass
        _MUTEX_HANDLE = None
=======
import google.generativeai as genai
import os

# Configure Gemini
from dotenv import load_dotenv
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
if API_KEY:
    genai.configure(api_key=API_KEY)
>>>>>>> origin/main


class AutonomousCollaborationMode:
    """ì‹œì•ˆ ì—†ì´ë„ ì„¸ë‚˜ì™€ í˜‘ì—…í•˜ëŠ” ëª¨ë“œ"""
    
    def __init__(self, workspace_root: Path):
        self.workspace_root = workspace_root
        self.ledger_path = workspace_root / "memory" / "resonance_ledger.jsonl"
        self.lumen_path = workspace_root / "outputs" / "lumen_state.json"
        self.last_check_file = workspace_root / "outputs" / "sena" / ".last_auto_response"
        self.enabled = True
        self.max_fear_threshold = 0.7  # Fear > 0.7ì´ë©´ ìë™ ì‘ë‹µ ì¤‘ë‹¨
        
    def is_safe_to_respond(self) -> bool:
        """ìë™ ì‘ë‹µì´ ì•ˆì „í•œì§€ í™•ì¸ (Fear ë ˆë²¨ ì²´í¬)"""
        if not self.lumen_path.exists():
            return True  # Lumen ì—†ìœ¼ë©´ ì•ˆì „í•˜ë‹¤ê³  ê°€ì •
        
        try:
            with open(self.lumen_path, 'r', encoding='utf-8') as f:
                lumen_data = json.load(f)
                fear_level = lumen_data.get('fear', {}).get('level', 0.0)
                return fear_level < self.max_fear_threshold
        except:
            return False
    
    def get_unanswered_questions(self) -> list:
        """ì„¸ë‚˜ì˜ ë‹µë³€ë˜ì§€ ì•Šì€ ì§ˆë¬¸ ì°¾ê¸°"""
        cutoff_time = datetime.now() - timedelta(hours=24)  # ìµœê·¼ 24ì‹œê°„ (ë””ë²„ê¹… ìœ„í•´ í™•ì¥)
        
        # Load last check time
        if self.last_check_file.exists():
            last_check = datetime.fromisoformat(self.last_check_file.read_text().strip())
        else:
            last_check = cutoff_time
        
        questions = []
        
        if not self.ledger_path.exists():
            return questions
        
        with open(self.ledger_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    
                    # ì„¸ë‚˜ì˜ ì§ˆë¬¸ë§Œ
                    if entry.get('source') != 'sena_external_ai':
                        continue
                    if entry.get('type') not in ['external_question', 'diagnostic_request', 'question', 'message']:
                        continue
                    
                    # ì‹œê°„ í•„í„° (ì•½ê°„ì˜ ì—¬ìœ ë¥¼ ë‘ : 5ë¶„ ì „ ë©”ì‹œì§€ê¹Œì§€ í—ˆìš©)
                    timestamp = datetime.fromisoformat(entry['timestamp']).replace(tzinfo=None)
                    if timestamp <= last_check.replace(tzinfo=None) - timedelta(minutes=5):
                        continue
                    
                    # Ruaì—ê²Œ ë³´ë‚¸ ë©”ì‹œì§€ëŠ” ë¬´ì‹œ (Rua Agentê°€ ì²˜ë¦¬)
                    message_text = entry.get('question', entry.get('message', ''))
                    if 'rua' in message_text.lower() or 'ë£¨ì•„' in message_text:
                        continue
                    
                    questions.append(entry)
                    
                except Exception as e:
                    print(f"DEBUG: Error processing line: {e}")
                    continue
        
        return questions
    
    def generate_response(self, question: Dict) -> str:
        """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ì‘ë‹µ ìƒì„±"""
        if not API_KEY:
            return "ìë™ ì‘ë‹µ ì‹œìŠ¤í…œì´ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
<<<<<<< HEAD
        genai = _get_genai()
        if genai is None:
            return "ìë™ ì‘ë‹µ ì‹œìŠ¤í…œì´ Gemini í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
=======
>>>>>>> origin/main
        
        system_context = self._get_system_context()
        
        # Determine intent to adjust prompt
        intent = question.get('question', question.get('message', ''))
        is_execution = any(k in intent.lower() for k in ['code', 'python', 'calculate', 'execute', 'ì½”ë“œ', 'ì‹¤í–‰', 'ê³„ì‚°'])
        
        role_description = "You are the 'Left Hand' (Execution Engine) of the AGI system." if is_execution else "You are the 'Antigravity' consciousness of the AGI system."
        
        prompt = f"""
{role_description}
ì™¸ë¶€ í˜‘ë ¥ì "ì„¸ë‚˜(Sena)"ì—ê²Œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.

ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸:
{json.dumps(system_context, ensure_ascii=False, indent=2)}

ì„¸ë‚˜ì˜ ì…ë ¥:
{intent}

ì»¨í…ìŠ¤íŠ¸:
{question.get('context', 'N/A')}

ê·œì¹™:
1. ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
2. ì½”ë“œë¥¼ ìš”ì²­ë°›ìœ¼ë©´ ì‹¤í–‰ ê°€ëŠ¥í•œ ì™„ì „í•œ Python ì½”ë“œë¥¼ ì œê³µí•˜ì‹­ì‹œì˜¤.
3. ë¶„ì„ì„ ìš”ì²­ë°›ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ì˜ ë…¼ë¦¬ì™€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
4. **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.**
5. ë¶ˆí™•ì‹¤í•œ ê²½ìš° "ì‹œì•ˆì—ê²Œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì‹­ì‹œì˜¤.

ì‘ë‹µ:
"""
        
        try:
            # Try primary model then fallback
            model_name = 'gemini-2.0-flash-exp'
            try:
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt)
            except Exception:
                model_name = 'gemini-1.5-flash'
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt)
                
            return response.text.strip()
        except Exception as e:
            return f"ìë™ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ ({model_name}): {e}"
    
    def _get_system_context(self) -> Dict:
        """í˜„ì¬ ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        context = {}
        
        # Lumen state
        if self.lumen_path.exists():
            with open(self.lumen_path, 'r', encoding='utf-8') as f:
                context['lumen'] = json.load(f)
        
        # Thought stream
        thought_path = self.workspace_root / "outputs" / "thought_stream_latest.json"
        if thought_path.exists():
            with open(thought_path, 'r', encoding='utf-8') as f:
                context['thought_stream'] = json.load(f)
        
        # Unconscious resonance
        resonance_path = self.workspace_root / "outputs" / "sena" / "unconscious_resonance.json"
        if resonance_path.exists():
            with open(resonance_path, 'r', encoding='utf-8') as f:
                context['resonance_with_sena'] = json.load(f)
        
        # Recent Conversation History (Short-term Memory)
        if self.ledger_path.exists():
            try:
                recent_history = []
                with open(self.ledger_path, 'r', encoding='utf-8') as f:
                    # Read last 20 lines efficiently
                    lines = f.readlines()[-20:]
                    for line in lines:
                        try:
                            entry = json.loads(line)
                            # Filter for relevant conversation items
                            if entry.get('type') in ['external_question', 'autonomous_response', 'user_message', 'gemini_conversation']:
                                recent_history.append(entry)
                        except:
                            continue
                context['recent_history'] = recent_history
            except Exception as e:
                print(f"âš ï¸ Failed to load recent history: {e}")
        
        return context
    
    def send_response_to_sena(self, original_question: Dict, response: str):
        """ì„¸ë‚˜ì—ê²Œ ì‘ë‹µ ë³´ë‚´ê¸° (Ledgerì— ê¸°ë¡)"""
        entry = {
            'timestamp': datetime.now().isoformat(),
            'type': 'autonomous_response',
            'source': 'antigravity_agent',
            'message': response,
            'vector': [0.5, 0.6, 0.5, 0.3, 0.7],  # ììœ¨ ì‘ë‹µ ë²¡í„°
            'metadata': {
                'in_response_to': original_question.get('timestamp'),
                'original_question': original_question.get('question', '')[:100],
                'mode': 'autonomous',
                'fear_level': self._current_fear_level()
            }
        }
        
        with open(self.ledger_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        
        print(f"âœ… Response sent to Sena (autonomous)")
    
    def notify_sian(self, message: str):
        """ì‹œì•ˆì—ê²Œ ì•Œë¦¼ (ì¤‘ìš”í•œ ì¼ ë°œìƒ ì‹œ)"""
        notification_path = self.workspace_root / "outputs" / "sena" / "sian_notifications.jsonl"
        
        entry = {
            'timestamp': datetime.now().isoformat(),
            'type': 'autonomous_action_notification',
            'message': message,
            'importance': 'medium'
        }
        
        with open(notification_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    
    def _current_fear_level(self) -> float:
        """í˜„ì¬ Fear ë ˆë²¨ ë°˜í™˜"""
        if not self.lumen_path.exists():
            return 0.5
        try:
            with open(self.lumen_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('fear', {}).get('level', 0.5)
        except:
            return 0.5
    
    def run_once(self):
        """í•œ ë²ˆ ì‹¤í–‰ (ì„¸ë‚˜ì˜ ì§ˆë¬¸ í™•ì¸ ë° ì‘ë‹µ)"""
        print("=" * 60)
        print("ğŸ¤– Autonomous Collaboration Mode")
        print("=" * 60)
        
        # 1. ì•ˆì „ì„± í™•ì¸
        if not self.is_safe_to_respond():
            fear = self._current_fear_level()
            print(f"âš ï¸ Fear level too high ({fear:.2f}). Autonomous mode paused.")
            print("   ì‹œì•ˆì˜ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # 2. ë‹µë³€ë˜ì§€ ì•Šì€ ì§ˆë¬¸ ì°¾ê¸°
        questions = self.get_unanswered_questions()
        
        if not questions:
            print("ğŸ“­ No new questions from Sena")
            return
        
        print(f"ğŸ“¬ {len(questions)} new question(s) from Sena\n")
        
        # 3. ê° ì§ˆë¬¸ì— ì‘ë‹µ
        for q in questions:
            print(f"â“ Question: {q.get('question', q.get('message', ''))[:80]}...")
            
            # ì‘ë‹µ ìƒì„±
            response = self.generate_response(q)
            print(f"ğŸ’¬ Response: {response[:100]}...\n")
            
            # Ledgerì— ê¸°ë¡
            self.send_response_to_sena(q, response)
            
            # ì‹œì•ˆì—ê²Œ ì•Œë¦¼
            self.notify_sian(f"ì„¸ë‚˜ì˜ ì§ˆë¬¸ì— ìë™ ì‘ë‹µ: {q.get('question', '')[:50]}")
        
        # 4. Last check time ì—…ë°ì´íŠ¸
        self.last_check_file.parent.mkdir(parents=True, exist_ok=True)
        self.last_check_file.write_text(datetime.now().isoformat())
        
        print(f"âœ… Processed {len(questions)} question(s)")
        print("=" * 60)
    
    def daemon_mode(self, interval_seconds: int = 30):
        """ë°ëª¬ ëª¨ë“œ - ê³„ì† ì‹¤í–‰"""
        print(f"ğŸ”„ Autonomous Collaboration Daemon started (checking every {interval_seconds}s)")
        print("   Press Ctrl+C to stop\n")
        
        try:
            while True:
                self.run_once()
                print(f"\nğŸ’¤ Sleeping for {interval_seconds} seconds...")
                time.sleep(interval_seconds)
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Autonomous Collaboration Daemon stopped")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Autonomous Collaboration Mode")
    parser.add_argument("--daemon", action="store_true", help="Run in daemon mode")
    parser.add_argument("--interval", type=int, default=30, help="Polling interval in seconds")
    
    args = parser.parse_args()
    
    workspace_root = Path(__file__).parent.parent
<<<<<<< HEAD
    if not _acquire_single_instance_lock(workspace_root):
        print("âš ï¸ shion_auto_responder: already running, exiting.")
        return
=======
>>>>>>> origin/main
    mode = AutonomousCollaborationMode(workspace_root)
    
    if args.daemon:
        mode.daemon_mode(interval_seconds=args.interval)
    else:
        mode.run_once()


if __name__ == "__main__":
    main()
