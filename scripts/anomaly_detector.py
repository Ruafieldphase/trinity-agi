#!/usr/bin/env python3
"""
Anomaly Detector with Machine Learning

ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ML ê¸°ë°˜ìœ¼ë¡œ ì´ìƒ íŒ¨í„´ì„ ê°ì§€í•©ë‹ˆë‹¤.

Features:
- Isolation Forest ê¸°ë°˜ ì´ìƒ ê°ì§€
- Sliding window (1ì‹œê°„) ë¶„ì„
- Multi-level severity (Critical, Warning, Info)
- Alert ìƒì„± ë° ë¡œê¹…

Author: GitHub Copilot
Created: 2025-11-03
Phase: 7 (System Stabilization)
"""

import argparse
import json
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import psutil
from sklearn.ensemble import IsolationForest

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
WORKSPACE_ROOT = Path(__file__).resolve().parent.parent


class AnomalyDetector:
    """ì‹¤ì‹œê°„ Anomaly Detection ì‹œìŠ¤í…œ"""
    
    def __init__(self, baseline_path: Path, dry_run: bool = False):
        """
        Args:
            baseline_path: Baseline JSON íŒŒì¼ ê²½ë¡œ
            dry_run: Dry-run ëª¨ë“œ (Alert ìƒì„± ì•ˆ í•¨)
        """
        self.baseline_path = baseline_path
        self.dry_run = dry_run
        self.baseline = self._load_baseline()
        self.history = []  # Sliding window
        self.model = None
        self._init_model()
        
    def _load_baseline(self) -> Dict:
        """Baseline JSON ë¡œë“œ"""
        if not self.baseline_path.exists():
            raise FileNotFoundError(f"Baseline not found: {self.baseline_path}")
        
        with open(self.baseline_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _init_model(self):
        """Isolation Forest ëª¨ë¸ ì´ˆê¸°í™”"""
        # contamination: ì˜ˆìƒ ì´ìƒì¹˜ ë¹„ìœ¨ (ê¸°ë³¸ 5%)
        self.model = IsolationForest(
            n_estimators=100,
            contamination=0.05,
            random_state=42,
            n_jobs=-1
        )
        print("âœ… Isolation Forest initialized (contamination=0.05)")
    
    def collect_current_metrics(self) -> Dict:
        """í˜„ì¬ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # CPU & Memory
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # Monitoring metrics (ìµœì‹  íŒŒì¼)
            monitoring_path = WORKSPACE_ROOT / "outputs" / "monitoring_metrics_latest.json"
            if monitoring_path.exists():
                # UTF-8 BOM ëŒ€ì‘
                with open(monitoring_path, 'r', encoding='utf-8-sig') as f:
                    monitoring = json.load(f)
                    
                agi_metrics = monitoring.get("agi_metrics", {})
                lumen_metrics = monitoring.get("lumen_metrics", {})
                queue_metrics = monitoring.get("queue_metrics", {})
                
                success_rate = agi_metrics.get("success_rate", 0)
                avg_latency_ms = lumen_metrics.get("avg_latency_ms", 0)
                queue_size = queue_metrics.get("pending", 0)
            else:
                success_rate = 0
                avg_latency_ms = 0
                queue_size = 0
            
            return {
                "timestamp": datetime.now().isoformat(),
                "cpu_percent": cpu_percent,
                "memory_percent": memory_percent,
                "success_rate": success_rate,
                "avg_latency_ms": avg_latency_ms,
                "queue_size": queue_size,
            }
        except Exception as e:
            print(f"âš ï¸  Failed to collect metrics: {e}", file=sys.stderr)
            return {}
    
    def check_threshold_anomaly(self, metrics: Dict) -> List[Dict]:
        """Threshold ê¸°ë°˜ ì´ìƒ ê°ì§€ (ê°„ë‹¨í•œ ë£° ê¸°ë°˜)"""
        anomalies = []
        
        for key in ["cpu_percent", "memory_percent", "success_rate", "avg_latency_ms", "queue_size"]:
            if key not in metrics or key not in self.baseline:
                continue
            
            value = metrics[key]
            baseline_stats = self.baseline[key]
            
            lower = baseline_stats["lower_threshold"]
            upper = baseline_stats["upper_threshold"]
            
            # Success rateëŠ” ë‚®ìœ¼ë©´ ì´ìƒ
            if key == "success_rate":
                if value < lower:
                    severity = "Critical" if value < lower - 10 else "Warning"
                    anomalies.append({
                        "metric": key,
                        "value": value,
                        "baseline_range": f"{lower:.2f}~{upper:.2f}",
                        "severity": severity,
                        "message": f"Success rate too low: {value:.2f}% (expected >{lower:.2f}%)"
                    })
            # ë‚˜ë¨¸ì§€ëŠ” ë†’ìœ¼ë©´ ì´ìƒ
            else:
                if value > upper:
                    # Critical: threshold + 2Ïƒ ì´ˆê³¼
                    mean = baseline_stats["mean"]
                    std = baseline_stats["std"]
                    critical_threshold = mean + 5 * std
                    
                    severity = "Critical" if value > critical_threshold else "Warning"
                    anomalies.append({
                        "metric": key,
                        "value": value,
                        "baseline_range": f"{lower:.2f}~{upper:.2f}",
                        "severity": severity,
                        "message": f"{key} too high: {value:.2f} (expected <{upper:.2f})"
                    })
        
        return anomalies
    
    def check_ml_anomaly(self, metrics: Dict) -> Optional[Dict]:
        """ML ê¸°ë°˜ ì´ìƒ ê°ì§€ (Isolation Forest)"""
        # Sliding windowì— ì¶”ê°€
        self.history.append(metrics)
        
        # 1ì‹œê°„ (60ê°œ) ì´ìƒ ìœ ì§€
        cutoff = datetime.now() - timedelta(hours=1)
        self.history = [
            m for m in self.history
            if datetime.fromisoformat(m["timestamp"]) > cutoff
        ]
        
        # ìµœì†Œ 10ê°œ ë°ì´í„° í•„ìš”
        if len(self.history) < 10:
            return None
        
        # Feature matrix ìƒì„±
        features = []
        for m in self.history:
            features.append([
                m.get("cpu_percent", 0),
                m.get("memory_percent", 0),
                m.get("success_rate", 0),
                m.get("avg_latency_ms", 0),
                m.get("queue_size", 0),
            ])
        
        X = np.array(features)
        
        # ëª¨ë¸ í•™ìŠµ (ë§¤ë²ˆ ì¬í•™ìŠµ)
        try:
            self.model.fit(X)
            predictions = self.model.predict(X)
            
            # ë§ˆì§€ë§‰ ë°ì´í„°í¬ì¸íŠ¸ê°€ ì´ìƒì¸ì§€ í™•ì¸
            if predictions[-1] == -1:
                # Anomaly score ê³„ì‚°
                scores = self.model.score_samples(X)
                current_score = scores[-1]
                mean_score = np.mean(scores[:-1])
                
                return {
                    "metric": "ml_composite",
                    "anomaly_score": float(current_score),
                    "mean_score": float(mean_score),
                    "severity": "Info",
                    "message": f"ML detected anomaly (score: {current_score:.3f}, mean: {mean_score:.3f})"
                }
        except Exception as e:
            print(f"âš ï¸  ML anomaly check failed: {e}", file=sys.stderr)
        
        return None
    
    def create_alert(self, anomalies: List[Dict], metrics: Dict):
        """Alert ìƒì„± ë° ì €ì¥"""
        if not anomalies:
            return
        
        alert = {
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics,
            "anomalies": anomalies,
            "total_anomalies": len(anomalies),
            "max_severity": max(a["severity"] for a in anomalies),
        }
        
        # Alert ë¡œê·¸ ì €ì¥
        alert_log_path = WORKSPACE_ROOT / "outputs" / "anomaly_alerts.jsonl"
        alert_log_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(alert_log_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(alert, ensure_ascii=False) + '\n')
        
        # ìµœì‹  Alert ì €ì¥
        latest_path = WORKSPACE_ROOT / "outputs" / "anomaly_alert_latest.json"
        with open(latest_path, 'w', encoding='utf-8') as f:
            json.dump(alert, f, indent=2, ensure_ascii=False)
        
        # Console ì¶œë ¥
        severity_color = {
            "Critical": "\033[91m",  # Red
            "Warning": "\033[93m",   # Yellow
            "Info": "\033[94m",      # Blue
        }
        reset = "\033[0m"
        
        max_severity = alert["max_severity"]
        color = severity_color.get(max_severity, "")
        
        print(f"\n{color}ğŸš¨ [{max_severity}] Anomaly Detected!{reset}")
        for anomaly in anomalies:
            print(f"   â€¢ {anomaly['message']}")
        print(f"   ğŸ“ Saved to: {latest_path}\n")
    
    def run_once(self):
        """1íšŒ ê²€ì‚¬ ì‹¤í–‰"""
        print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] Checking for anomalies...")
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics = self.collect_current_metrics()
        if not metrics:
            print("   âš ï¸  No metrics collected")
            return
        
        # Threshold ê¸°ë°˜ ê²€ì‚¬
        threshold_anomalies = self.check_threshold_anomaly(metrics)
        
        # ML ê¸°ë°˜ ê²€ì‚¬
        ml_anomaly = self.check_ml_anomaly(metrics)
        
        # í†µí•©
        all_anomalies = threshold_anomalies[:]
        if ml_anomaly:
            all_anomalies.append(ml_anomaly)
        
        # Alert ìƒì„±
        if all_anomalies:
            if self.dry_run:
                print(f"   [DRY-RUN] Would create alert for {len(all_anomalies)} anomalies")
                for a in all_anomalies:
                    print(f"      â€¢ [{a['severity']}] {a['message']}")
            else:
                self.create_alert(all_anomalies, metrics)
        else:
            print("   âœ… No anomalies detected")
    
    def run_loop(self, interval: int):
        """ë°˜ë³µ ê²€ì‚¬ ì‹¤í–‰"""
        print(f"ğŸš€ Starting anomaly detection loop (interval: {interval}s)")
        print(f"   Baseline: {self.baseline_path}")
        print(f"   Dry-run: {self.dry_run}")
        print(f"\nPress Ctrl+C to stop\n")
        
        try:
            while True:
                self.run_once()
                time.sleep(interval)
        except KeyboardInterrupt:
            print("\n\nâœ… Anomaly detection stopped by user")


def main():
    parser = argparse.ArgumentParser(description="Anomaly Detector with ML")
    parser.add_argument("--baseline", type=str, required=True, help="Baseline JSON path")
    parser.add_argument("--interval", type=int, default=60, help="Check interval (seconds, default: 60)")
    parser.add_argument("--dry-run", action="store_true", help="Dry-run mode (no alerts)")
    parser.add_argument("--once", action="store_true", help="Run once and exit")
    
    args = parser.parse_args()
    
    baseline_path = Path(args.baseline)
    if not baseline_path.is_absolute():
        baseline_path = WORKSPACE_ROOT / baseline_path
    
    # Detector ì´ˆê¸°í™”
    try:
        detector = AnomalyDetector(baseline_path=baseline_path, dry_run=args.dry_run)
    except Exception as e:
        print(f"âŒ Failed to initialize detector: {e}", file=sys.stderr)
        return 1
    
    # ì‹¤í–‰
    if args.once:
        detector.run_once()
    else:
        detector.run_loop(interval=args.interval)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
