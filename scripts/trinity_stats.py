#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Trinity Statistics Generator (Core-Elro-Core)
ÏÇºÏúÑÏùºÏ≤¥ ÌÜµÌï© ÌÜµÍ≥Ñ ÏÉùÏÑ±

Ï†ï(Ê≠£) - Core: Í∞êÏùëÏùò ÎåÄÌôî
Î∞ò(Âèç) - Elro: Í∞êÏùëÏùò Íµ¨Ï°∞ (Ï†ïÎ≥¥Ïù¥Î°† Î≥ÄÌôò)
Ìï©(Âêà) - Core: ÏÑ§Í≥Ñ ÌÜµÌï©
"""
import json
import sys
from pathlib import Path
from datetime import datetime
from collections import Counter
import re
import argparse
from workspace_root import get_workspace_root

def parse_datetime(dt_str):
    """ISO 8601 datetime ÌååÏã±"""
    if not dt_str or dt_str == "null":
        return None
    try:
        # Remove timezone suffix for parsing
        dt_str = dt_str.replace('+00:00', 'Z')
        if dt_str.endswith('Z'):
            dt_str = dt_str[:-1]
        return datetime.fromisoformat(dt_str)
    except:
        return None

def load_jsonl(file_path):
    """JSONL ÌååÏùº Î°úÎìú"""
    records = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        records.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue
    except FileNotFoundError:
        print(f"Warning: {file_path} not found", file=sys.stderr)
    return records

def load_origin_core(extra_dir: Path):
    """Optional: Load additional Core conversation sources from original folder.
    Supports JSON array files like shared_conversations.json or conversations.json.
    Returns list of minimal records with conversation_id/title/create_time.
    """
    records = []
    if not extra_dir or not extra_dir.exists():
        return records
    candidates = [
        extra_dir / "shared_conversations.json",
        extra_dir / "conversations.json",
    ]
    for p in candidates:
        try:
            if p.exists():
                with open(p, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for i, item in enumerate(data):
                            title = (
                                item.get('title')
                                or item.get('conversation_title')
                                or item.get('name')
                                or f"core_origin_{p.stem}_{i}"
                            )
                            cid = (
                                item.get('conversation_id')
                                or item.get('id')
                                or f"core_origin_{p.stem}_{i}"
                            )
                            ct = item.get('create_time') or item.get('created_at') or None
                            records.append(
                                {
                                    'conversation_id': cid,
                                    'conversation_title': title,
                                    'create_time': ct,
                                }
                            )
        except Exception:
            # tolerate malformed/unknown formats
            continue
    return records

def extract_keywords(text, top_n=20):
    """ÌïúÍ∏Ä ÌÇ§ÏõåÎìú Ï∂îÏ∂ú (Í∞ÑÎã® Î≤ÑÏ†Ñ)"""
    if not text:
        return []
    # ÌïúÍ∏Ä 2Í∏ÄÏûê Ïù¥ÏÉÅ Îã®Ïñ¥ Ï∂îÏ∂ú
    words = re.findall(r'[Í∞Ä-Ìû£]{2,}', text)
    # Î∂àÏö©Ïñ¥ ÌïÑÌÑ∞ (Í∞ÑÎã® Î≤ÑÏ†Ñ)
    stopwords = {'ÏûàÎäî', 'ÏûàÏäµÎãàÎã§', 'Ìï©ÎãàÎã§', 'ÏûÖÎãàÎã§', 'Í≤ÉÏûÖÎãàÎã§', 'Îê©ÎãàÎã§', 'Í∞ôÏùÄ', 'ÎåÄÌïú', 'ÏúÑÌïú', 'ÌÜµÌï¥', 'Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ï†ÄÍ≤É', 'Ïñ¥Îñ§', 'Ïñ¥ÎñªÍ≤å', 'Î¨¥Ïóá', 'ÎàÑÍµ¨', 'Ïñ∏Ï†ú', 'Ïñ¥Îîî', 'Ïôú'}
    words = [w for w in words if w not in stopwords and len(w) >= 2]
    return Counter(words).most_common(top_n)

def analyze_phase(records, phase_name):
    """Îã®Ïùº phase Î∂ÑÏÑù"""
    if not records:
        return {
            "phase": phase_name,
            "total_messages": 0,
            "unique_conversations": 0,
            "avg_turns": 0,
            "max_turns": 0,
            "time_span_days": 0,
            "keywords": []
        }
    
    # ÎåÄÌôîÎ≥Ñ Í∑∏Î£πÌïë
    conv_groups = {}
    for r in records:
        conv_id = r.get('conversation_id', 'unknown')
        if conv_id not in conv_groups:
            conv_groups[conv_id] = []
        conv_groups[conv_id].append(r)
    
    # ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
    total_messages = len(records)
    unique_conversations = len(conv_groups)
    turn_counts = [len(msgs) for msgs in conv_groups.values()]
    avg_turns = sum(turn_counts) / len(turn_counts) if turn_counts else 0
    max_turns = max(turn_counts) if turn_counts else 0
    
    # ÏãúÍ∞Ñ Î≤îÏúÑ
    timestamps = []
    for r in records:
        ct = r.get('create_time')
        dt = parse_datetime(ct)
        if dt:
            timestamps.append(dt)
    
    time_span_days = 0
    if len(timestamps) >= 2:
        timestamps.sort()
        time_span_days = (timestamps[-1] - timestamps[0]).days
    
    # ÌÇ§ÏõåÎìú Ï∂îÏ∂ú (Ï†úÎ™© Í∏∞Î∞ò)
    all_titles = " ".join([r.get('conversation_title', '') for r in records if r.get('conversation_title')])
    keywords = extract_keywords(all_titles, top_n=15)
    
    return {
        "phase": phase_name,
        "total_messages": total_messages,
        "unique_conversations": unique_conversations,
        "avg_turns": round(avg_turns, 1),
        "max_turns": max_turns,
        "time_span_days": time_span_days,
        "keywords": [{"keyword": k, "count": c} for k, c in keywords]
    }

def main():
    ap = argparse.ArgumentParser(description="Generate Trinity (Core/Elro/Core) statistics")
    ap.add_argument(
        "--extra-Core-dir",
        dest="extra_core_dir",
        default=str(get_workspace_root() / "ai_binoche_conversation_origin" / "Core"),
        help="Optional folder to ingest additional Core sources (JSON arrays)",
    )
    args = ap.parse_args()

    workspace = get_workspace_root()
    output_dir = workspace / "outputs" / "trinity"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load data
    print("Loading Trinity datasets...", file=sys.stderr)
    Core_records = load_jsonl(workspace / "outputs" / "Core" / "core_conversations_flat.jsonl")
    elro_records = load_jsonl(workspace / "outputs" / "elro" / "elro_conversations_flat.jsonl")
    core_records = load_jsonl(workspace / "outputs" / "Core" / "core_conversations_flat.jsonl")

    # Optionally augment Core with original sources (non-destructive merge)
    extra_dir = Path(args.extra_core_dir) if args.extra_core_dir else None
    extra_core = load_origin_core(extra_dir) if extra_dir else []
    if extra_core:
        core_records.extend(extra_core)
    
    print(f"  Core (Ê≠£): {len(Core_records)} messages", file=sys.stderr)
    print(f"  Elro (Âèç): {len(elro_records)} messages", file=sys.stderr)
    print(f"  Core (Âêà): {len(core_records)} messages", file=sys.stderr)
    
    # Analyze each phase
    Core_stats = analyze_phase(Core_records, "Core (Ê≠£ - Thesis)")
    elro_stats = analyze_phase(elro_records, "Elro (Âèç - Antithesis)")
    core_stats = analyze_phase(core_records, "Core (Âêà - Synthesis)")
    
    # Combined stats
    trinity_stats = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "analyst": "Binoche_Observer üåø",
            "philosophy": "Core-Elro-Core Dialectical Trinity"
        },
        "summary": {
            "total_messages": Core_stats["total_messages"] + elro_stats["total_messages"] + core_stats["total_messages"],
            "total_conversations": Core_stats["unique_conversations"] + elro_stats["unique_conversations"] + core_stats["unique_conversations"],
            "time_span_years": round(max(Core_stats["time_span_days"], elro_stats["time_span_days"], core_stats["time_span_days"]) / 365.25, 1)
        },
        "phases": {
            "Core": Core_stats,
            "elro": elro_stats,
            "Core": core_stats
        }
    }
    
    # Save JSON
    output_json = output_dir / "trinity_statistics.json"
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(trinity_stats, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ Trinity statistics saved: {output_json}", file=sys.stderr)
    print(json.dumps(trinity_stats, ensure_ascii=False, indent=2))

if __name__ == '__main__':
    main()
