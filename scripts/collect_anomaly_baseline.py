#!/usr/bin/env python3
"""
Anomaly Detection Baseline Collector

ì§€ë‚œ Nì¼ê°„ì˜ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ì—¬ Normal behavior baselineì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
ì´ baselineì€ ì´í›„ Anomaly Detectionì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.

Author: GitHub Copilot
Created: 2025-11-03
Phase: 7 (System Stabilization)
"""

import argparse
import json
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
WORKSPACE_ROOT = Path(__file__).resolve().parent.parent


def load_monitoring_metrics(days: int = 7) -> pd.DataFrame:
    """
    ì§€ë‚œ Nì¼ê°„ì˜ monitoring_metrics.json íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        days: ìˆ˜ì§‘í•  ë‚ ì§œ ë²”ìœ„ (ì¼)
        
    Returns:
        DataFrame with columns: timestamp, cpu_percent, memory_percent, 
                                success_rate, avg_latency_ms, queue_size
    """
    metrics_list = []
    
    # outputs/monitoring_metrics_*.json íŒ¨í„´ íŒŒì¼ ìˆ˜ì§‘
    outputs_dir = WORKSPACE_ROOT / "outputs"
    
    # ìµœì‹  íŒŒì¼ ìš°ì„ 
    for json_file in sorted(outputs_dir.glob("monitoring_metrics_*.json"), reverse=True):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # timestamp í™•ì¸
            timestamp_str = data.get("timestamp")
            if not timestamp_str:
                continue
                
            timestamp = datetime.fromisoformat(timestamp_str)
            cutoff = datetime.now() - timedelta(days=days)
            
            if timestamp < cutoff:
                continue
                
            # ë©”íŠ¸ë¦­ ì¶”ì¶œ
            metrics = {
                "timestamp": timestamp,
                "cpu_percent": data.get("system_metrics", {}).get("cpu_percent", 0),
                "memory_percent": data.get("system_metrics", {}).get("memory_percent", 0),
                "success_rate": data.get("agi_metrics", {}).get("success_rate", 0),
                "avg_latency_ms": data.get("lumen_metrics", {}).get("avg_latency_ms", 0),
                "queue_size": data.get("queue_metrics", {}).get("pending", 0),
            }
            metrics_list.append(metrics)
            
        except Exception as e:
            print(f"âš ï¸  Skipping {json_file.name}: {e}", file=sys.stderr)
            continue
    
    if not metrics_list:
        print(f"âš ï¸  No metrics found in the last {days} days", file=sys.stderr)
        return pd.DataFrame()
    
    df = pd.DataFrame(metrics_list)
    df = df.sort_values("timestamp")
    df = df.reset_index(drop=True)
    
    return df


def calculate_baseline_stats(df: pd.DataFrame) -> Dict:
    """
    Baseline í†µê³„ ê³„ì‚° (í‰ê· , í‘œì¤€í¸ì°¨, threshold)
    
    Args:
        df: ë©”íŠ¸ë¦­ DataFrame
        
    Returns:
        Baseline í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    if df.empty:
        return {}
    
    stats = {}
    
    for col in ["cpu_percent", "memory_percent", "success_rate", "avg_latency_ms", "queue_size"]:
        if col not in df.columns:
            continue
            
        values = df[col].dropna()
        if len(values) == 0:
            continue
        
        mean = float(values.mean())
        std = float(values.std())
        
        # Threshold = mean Â± 3Ïƒ (99.7% coverage)
        if col == "success_rate":
            # Success rateëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ â†’ lower thresholdë§Œ ì„¤ì •
            lower_threshold = max(0, mean - 3 * std)
            upper_threshold = 100.0
        else:
            # CPU, Memory, Latency, QueueëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ â†’ upper thresholdë§Œ ì¤‘ìš”
            lower_threshold = 0.0
            upper_threshold = min(100.0, mean + 3 * std)
        
        stats[col] = {
            "mean": mean,
            "std": std,
            "min": float(values.min()),
            "max": float(values.max()),
            "median": float(values.median()),
            "q25": float(values.quantile(0.25)),
            "q75": float(values.quantile(0.75)),
            "lower_threshold": lower_threshold,
            "upper_threshold": upper_threshold,
        }
    
    return stats


def save_baseline(baseline: Dict, output_path: Path):
    """Baselineì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(baseline, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Baseline saved to: {output_path}")


def print_baseline_summary(baseline: Dict):
    """Baseline ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š Anomaly Detection Baseline Summary")
    print("="*60)
    
    for metric, stats in baseline.items():
        print(f"\nğŸ”¹ {metric}")
        print(f"   Mean:       {stats['mean']:.2f}")
        print(f"   Std Dev:    {stats['std']:.2f}")
        print(f"   Min/Max:    {stats['min']:.2f} / {stats['max']:.2f}")
        print(f"   Thresholds: {stats['lower_threshold']:.2f} ~ {stats['upper_threshold']:.2f}")


def main():
    parser = argparse.ArgumentParser(description="Collect Anomaly Detection Baseline")
    parser.add_argument("--days", type=int, default=7, help="Number of days to collect (default: 7)")
    parser.add_argument("--output", type=str, default="outputs/anomaly_baseline.json", 
                        help="Output JSON path (default: outputs/anomaly_baseline.json)")
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    print(f"ğŸ” Collecting baseline from last {args.days} days...")
    
    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    df = load_monitoring_metrics(days=args.days)
    
    if df.empty:
        print("âŒ No metrics collected. Cannot create baseline.", file=sys.stderr)
        return 1
    
    print(f"âœ… Collected {len(df)} data points")
    
    if args.verbose:
        print(f"\nğŸ“ˆ Raw data preview:")
        print(df.head(10))
    
    # Baseline í†µê³„ ê³„ì‚°
    baseline = calculate_baseline_stats(df)
    
    if not baseline:
        print("âŒ Failed to calculate baseline statistics.", file=sys.stderr)
        return 1
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    baseline["_metadata"] = {
        "created_at": datetime.now().isoformat(),
        "days_collected": args.days,
        "data_points": len(df),
        "start_date": df["timestamp"].min().isoformat(),
        "end_date": df["timestamp"].max().isoformat(),
    }
    
    # ì €ì¥
    output_path = WORKSPACE_ROOT / args.output
    save_baseline(baseline, output_path)
    
    # ìš”ì•½ ì¶œë ¥
    print_baseline_summary(baseline)
    
    print("\n" + "="*60)
    print("âœ… Baseline collection complete!")
    print("="*60)
    print(f"\nNext step:")
    print(f"  .\\scripts\\start_anomaly_monitor.ps1")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
