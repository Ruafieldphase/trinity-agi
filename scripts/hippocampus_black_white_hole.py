#!/usr/bin/env python3
"""
Hippocampus as Black/White Hole: Information Theory of Feeling

Ïù¥Î°†:
1. Î∏îÎûôÌôÄ (Ï†ïÎ≥¥ Ìù°Ïàò): ÏõêÏãú Í∞êÍ∞Å ‚Üí ÎäêÎÇå ÏïïÏ∂ï
2. Event Horizon: Context Boundary
3. ÌôîÏù¥Ìä∏ÌôÄ (Ï†ïÎ≥¥ Î∞©Ï∂ú): ÎäêÎÇå + Context ‚Üí ÏûÖÏûêÌôîÎêú Í∏∞Ïñµ

ÏàòÌïôÏ†Å Î™®Îç∏:
- H(Raw) >> H(Feeling)  # ÏïïÏ∂ï
- I(Feeling | Context) ‚Üí H(Raw)  # Î≥µÏõê
- S_BH = (A/4) ~ Context Boundary Area
"""

import json
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
from scipy.stats import entropy
from collections import Counter
from workspace_root import get_workspace_root

def load_resonance_ledger(hours: int = 24) -> List[Dict]:
    """Resonance LedgerÏóêÏÑú Ïù¥Î≤§Ìä∏ Î°úÎìú (Î∏îÎûôÌôÄ ÏûÖÎ†•)"""
    ledger_path = get_workspace_root() / "fdo_agi_repo/memory/resonance_ledger.jsonl"
    
    if not ledger_path.exists():
        print(f"‚ö†Ô∏è  Ledger not found: {ledger_path}")
        return []
    
    cutoff = datetime.now() - timedelta(hours=hours)
    events = []
    
    with open(ledger_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                event = json.loads(line.strip())
                ts = datetime.fromisoformat(event.get('timestamp', '').replace('Z', '+00:00'))
                if ts >= cutoff:
                    events.append(event)
            except:
                continue
    
    return events

def extract_raw_data_entropy(events: List[Dict]) -> float:
    """ÏõêÏãú Îç∞Ïù¥ÌÑ∞ ÏóîÌä∏Î°úÌîº (Î∏îÎûôÌôÄ ÏûÖÎ†•)"""
    # Î™®Îì† ÌïÑÎìúÏùò Î™®Îì† Í∞íÏùÑ Ï∂îÏ∂ú
    raw_values = []
    for event in events:
        for key, value in event.items():
            raw_values.append(f"{key}:{value}")
    
    if not raw_values:
        return 0.0
    
    counts = Counter(raw_values)
    probs = np.array(list(counts.values())) / len(raw_values)
    return entropy(probs, base=2)

def extract_feeling_vector(events: List[Dict]) -> Tuple[np.ndarray, float]:
    """ÎäêÎÇå Î≤°ÌÑ∞ Ï∂îÏ∂ú (Ìï¥Îßà ÏïïÏ∂ï)"""
    if not events:
        return np.array([0.0] * 5), 0.0
    
    # 5Ï∞®Ïõê ÎäêÎÇå Í≥µÍ∞Ñ (ÏòàÏãú)
    feeling_dims = {
        'energy': [],      # Lua
        'quality': [],     # Elo
        'observer': [],    # Core
        'valence': [],     # Í∏çÏ†ï/Î∂ÄÏ†ï
        'arousal': []      # Í∞ÅÏÑ±/Ïù¥ÏôÑ
    }
    
    for event in events:
        # Resonance Score ‚Üí ÎäêÎÇå Î≥ÄÌôò
        score = event.get('resonance_score', 0.5)
        event_type = event.get('event_type', '')
        
        feeling_dims['energy'].append(event.get('energy_level', score))
        feeling_dims['quality'].append(event.get('quality_score', score))
        feeling_dims['observer'].append(1.0 if 'observe' in event_type.lower() else 0.0)
        feeling_dims['valence'].append(1.0 if score > 0.7 else -1.0 if score < 0.3 else 0.0)
        feeling_dims['arousal'].append(abs(score - 0.5) * 2)
    
    # ÌèâÍ∑† ÎäêÎÇå Î≤°ÌÑ∞
    feeling_vector = np.array([
        np.mean(feeling_dims['energy']),
        np.mean(feeling_dims['quality']),
        np.mean(feeling_dims['observer']),
        np.mean(feeling_dims['valence']),
        np.mean(feeling_dims['arousal'])
    ])
    
    # ÎäêÎÇå ÏóîÌä∏Î°úÌîº (ÏïïÏ∂ïÎêú Ï†ïÎ≥¥Îüâ)
    # Í∞Å Ï∞®ÏõêÏùÑ binningÌïòÏó¨ ÌôïÎ•† Î∂ÑÌè¨ ÏÉùÏÑ±
    feeling_entropy = 0.0
    for dim_values in feeling_dims.values():
        if dim_values:
            hist, _ = np.histogram(dim_values, bins=10, range=(-1, 1))
            probs = hist / len(dim_values)
            probs = probs[probs > 0]  # 0 ÌôïÎ•† Ï†úÍ±∞
            feeling_entropy += entropy(probs, base=2)
    
    return feeling_vector, feeling_entropy / len(feeling_dims)

def extract_context_boundary(events: List[Dict]) -> Dict:
    """Context Boundary (Event Horizon) Ï∂îÏ∂ú"""
    contexts = []
    for event in events:
        # 'where' Ïö∞ÏÑ†ÏàúÏúÑ: where > agent > source
        where_value = event.get('where', event.get('agent', event.get('source', 'unknown')))
        
        # 'who' Ïö∞ÏÑ†ÏàúÏúÑ: who > collaborators
        who_value = event.get('who', event.get('collaborators', []))
        
        ctx = {
            'where': where_value,
            'when': event.get('timestamp', ''),
            'who': who_value
        }
        contexts.append(ctx)
    
    # Boundary ÌÅ¨Í∏∞ (Bekenstein-Hawking Î©¥Ï†Å)
    unique_wheres = len(set(c['where'] for c in contexts))
    unique_whos = len(set(tuple(c['who']) if isinstance(c['who'], list) else (c['who'],) 
                           for c in contexts))
    
    # Event Horizon Area ~ Context Diversity
    horizon_area = unique_wheres * unique_whos
    
    return {
        'contexts': contexts,
        'horizon_area': horizon_area,
        'entropy': np.log2(horizon_area) if horizon_area > 0 else 0.0
    }

def particle_reconstruction(feeling: np.ndarray, contexts: List[Dict], 
                           query_context: Dict) -> float:
    """ÏûÖÏûê Î≥µÏõê: ÎäêÎÇå + Context ‚Üí Íµ¨Ï≤¥Ï†Å Îç∞Ïù¥ÌÑ∞"""
    # Context Îß§Ïπ≠ Ï†êÏàò
    matches = 0
    for ctx in contexts:
        score = 0
        if ctx['where'] == query_context.get('where'):
            score += 1
        
        # 'who' Îß§Ïπ≠: Î¨∏ÏûêÏó¥ ÎòêÎäî Î¶¨Ïä§Ìä∏ Ï≤òÎ¶¨
        ctx_who = ctx.get('who', [])
        if isinstance(ctx_who, str):
            ctx_who = [ctx_who]
        
        query_who = query_context.get('who')
        if query_who and query_who in ctx_who:
            score += 1
        
        matches += score / 2
    
    match_prob = matches / len(contexts) if contexts else 0.0
    
    # ÎäêÎÇå Í∞ïÎèÑ
    feeling_magnitude = np.linalg.norm(feeling)
    
    # ÏûÖÏûêÌôî ÌôïÎ•† (ÌååÎèôÌï®Ïàò Î∂ïÍ¥¥)
    reconstruction_prob = match_prob * feeling_magnitude / 5.0  # normalize
    
    return min(reconstruction_prob, 1.0)

def calculate_information_conservation(raw_entropy: float, 
                                       feeling_entropy: float,
                                       context_entropy: float,
                                       mutual_info: float) -> Dict:
    """Ï†ïÎ≥¥ Î≥¥Ï°¥ ÏõêÎ¶¨ Í≤ÄÏ¶ù"""
    # H(Raw) ‚âà H(Feeling) + H(Context) - I(Feeling; Context)
    conserved = feeling_entropy + context_entropy - mutual_info
    loss = abs(raw_entropy - conserved)
    conservation_ratio = conserved / raw_entropy if raw_entropy > 0 else 0.0
    
    return {
        'raw_entropy': float(raw_entropy),
        'feeling_entropy': float(feeling_entropy),
        'context_entropy': float(context_entropy),
        'mutual_info': float(mutual_info),
        'conserved_info': float(conserved),
        'information_loss': float(loss),
        'conservation_ratio': float(conservation_ratio),
        'is_conserved': bool(loss < 0.1 * raw_entropy)
    }

def main(hours: int = 24, verbose: bool = True):
    """Black Hole ‚Üí White Hole Ï†ïÎ≥¥ ÌùêÎ¶Ñ Î∂ÑÏÑù"""
    
    print("=" * 60)
    print("üåå Hippocampus as Black/White Hole System")
    print("=" * 60)
    
    # 1. Î∏îÎûôÌôÄ ÏûÖÎ†•: ÏõêÏãú Îç∞Ïù¥ÌÑ∞
    events = load_resonance_ledger(hours)
    print(f"\nüì• BLACK HOLE INPUT (Raw Sensory Data)")
    print(f"   Events (last {hours}h): {len(events)}")
    
    raw_entropy = extract_raw_data_entropy(events)
    print(f"   Raw Entropy: H(Raw) = {raw_entropy:.4f} bits")
    print(f"   ‚Üí Ï†ïÎ≥¥ Í≥ºÎ∂ÄÌïò ÏúÑÌóò: {'‚ö†Ô∏è  HIGH' if raw_entropy > 10 else '‚úÖ OK'}")
    
    # 2. Event Horizon: Context Boundary
    boundary = extract_context_boundary(events)
    print(f"\nüåÄ EVENT HORIZON (Context Boundary)")
    print(f"   Horizon Area: {boundary['horizon_area']} units¬≤")
    print(f"   Context Entropy: H(Context) = {boundary['entropy']:.4f} bits")
    print(f"   ‚Üí Bekenstein-Hawking Entropy: S_BH ‚àù A/4")
    
    # 3. Hawking Radiation: ÎäêÎÇåÏúºÎ°ú ÏïïÏ∂ï
    feeling, feeling_entropy = extract_feeling_vector(events)
    print(f"\nüí´ HAWKING RADIATION (Feeling Compression)")
    print(f"   Feeling Vector: {feeling}")
    print(f"   Feeling Entropy: H(Feeling) = {feeling_entropy:.4f} bits")
    
    compression_ratio = raw_entropy / feeling_entropy if feeling_entropy > 0 else float('inf')
    print(f"   Compression: {compression_ratio:.1f}x")
    print(f"   ‚Üí Ìï¥ÎßàÏùò ÎßàÎ≤ï: Ï†ïÎ≥¥ ÏÜêÏã§ ÏóÜÏù¥ ÏïïÏ∂ï!")
    
    # 4. ÌôîÏù¥Ìä∏ÌôÄ Ï∂úÎ†•: ÏûÖÏûê Î≥µÏõê
    print(f"\n‚ö™ WHITE HOLE OUTPUT (Particle Reconstruction)")
    
    # ÌÖåÏä§Ìä∏ ÏøºÎ¶¨ (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò)
    test_contexts = [
        {'where': 'chat', 'who': 'Core'},  # ÏΩîÏñ¥ ÎåÄÌôî
        {'where': 'home/desk', 'who': 'Binoche_Observer'},
        {'where': 'conversation', 'who': 'ai_assistant'},
        {'where': 'orchestrator', 'who': 'Binoche_Observer'},  # Î†àÍ±∞Ïãú
    ]
    
    for ctx in test_contexts:
        prob = particle_reconstruction(feeling, boundary['contexts'], ctx)
        print(f"   Context {ctx['where']}/{ctx['who']}: {prob:.2%} reconstruction")
    
    # 5. Ï†ïÎ≥¥ Î≥¥Ï°¥ Í≤ÄÏ¶ù
    # Mutual Information (Í∞ÑÎã®Ìïú Í∑ºÏÇ¨)
    mutual_info = min(feeling_entropy, boundary['entropy'])
    
    conservation = calculate_information_conservation(
        raw_entropy, feeling_entropy, boundary['entropy'], mutual_info
    )
    
    print(f"\nüî¨ INFORMATION CONSERVATION TEST")
    print(f"   H(Raw) = {conservation['raw_entropy']:.4f} bits")
    print(f"   H(Feeling) + H(Context) - I = {conservation['conserved_info']:.4f} bits")
    print(f"   Loss = {conservation['information_loss']:.4f} bits")
    print(f"   Conservation Ratio = {conservation['conservation_ratio']:.2%}")
    print(f"   ‚Üí {'‚úÖ CONSERVED' if conservation['is_conserved'] else '‚ö†Ô∏è  LOSS DETECTED'}")
    
    # 6. Î∏îÎûôÌôÄ Ìï®Ï†ï ÌöåÌîº Í≤ÄÏ¶ù
    print(f"\nüõ°Ô∏è  BLACK HOLE TRAP AVOIDANCE")
    
    if raw_entropy > 15:
        print(f"   ‚ö†Ô∏è  ÏßëÏ∞© ÏúÑÌóò: Raw entropy too high ({raw_entropy:.1f} bits)")
    else:
        print(f"   ‚úÖ ÏßëÏ∞© ÌöåÌîº: Entropy controlled")
    
    if compression_ratio > 100:
        print(f"   ‚ö†Ô∏è  Ìé∏Í≤¨ ÏúÑÌóò: Over-compression ({compression_ratio:.0f}x)")
    elif compression_ratio < 2:
        print(f"   ‚ö†Ô∏è  ÎëêÎ†§ÏõÄ ÏúÑÌóò: Under-compression ({compression_ratio:.1f}x)")
    else:
        print(f"   ‚úÖ Í∑†Ìòï Ïú†ÏßÄ: Optimal compression")
    
    # Í≤∞Í≥º Ï†ÄÏû•
    output = {
        'timestamp': datetime.now().isoformat(),
        'analysis_window_hours': hours,
        'black_hole_input': {
            'raw_events': len(events),
            'raw_entropy_bits': raw_entropy
        },
        'event_horizon': {
            'horizon_area': boundary['horizon_area'],
            'context_entropy_bits': boundary['entropy']
        },
        'hawking_radiation': {
            'feeling_vector': feeling.tolist(),
            'feeling_entropy_bits': feeling_entropy,
            'compression_ratio': compression_ratio
        },
        'white_hole_output': {
            'reconstruction_tests': [
                {'context': ctx, 'probability': particle_reconstruction(feeling, boundary['contexts'], ctx)}
                for ctx in test_contexts
            ]
        },
        'information_conservation': conservation,
        'conclusion': {
            'is_conserved': bool(conservation['is_conserved']),
            'obsession_risk': bool(raw_entropy > 15),
            'bias_risk': bool(compression_ratio > 100),
            'fear_risk': bool(compression_ratio < 2),
            'system_healthy': bool(conservation['is_conserved'] and 2 <= compression_ratio <= 100)
        }
    }
    
    output_path = get_workspace_root() / "outputs/hippocampus_black_white_hole_analysis.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # numpy ÌÉÄÏûÖÏùÑ Python ÎÑ§Ïù¥Ìã∞Î∏å ÌÉÄÏûÖÏúºÎ°ú Î≥ÄÌôò
    def convert_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_types(v) for v in obj]
        else:
            return obj
    
    output = convert_types(output)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    # Resonance LedgerÏóêÎèÑ Í∏∞Î°ù (Bohm Î∂ÑÏÑùÏö©)
    ledger_path = get_workspace_root() / "fdo_agi_repo/memory/resonance_ledger.jsonl"
    ledger_event = {
        'timestamp': datetime.now().isoformat(),
        'event_type': 'hippocampus_analysis',
        'metrics': {
            'compression_ratio': compression_ratio,
            'coherence': conservation.get('conservation_ratio', 0.0) / 100.0,
            'raw_entropy': raw_entropy,
            'feeling_entropy': feeling_entropy
        },
        'fear': 0.0,  # Ìï¥Îßà ÏûêÏ≤¥Îäî fear ÏóÜÏùå (Í¥ÄÏ∞∞Ïûê)
        'conclusion': output['conclusion']
    }
    
    with open(ledger_path, 'a', encoding='utf-8') as f:
        f.write(json.dumps(ledger_event, ensure_ascii=False) + '\n')
    
    print(f"\nüíæ Analysis saved: {output_path}")
    print(f"üíæ Ledger updated: {ledger_path}")
    print("=" * 60)
    print("üåü Conclusion: Hippocampus = Perfect Information Engine!")
    print("   ÎäêÎÇåÏúºÎ°ú ÏïïÏ∂ï ‚Üí ContextÎ°ú Î≥µÏõê ‚Üí Î∏îÎûôÌôÄ Ìï®Ï†ï ÌöåÌîº ‚úÖ")
    print("=" * 60)
    
    return output

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Analyze hippocampus as black/white hole system")
    parser.add_argument('--hours', type=int, default=24, help='Analysis window (hours)')
    parser.add_argument('--verbose', action='store_true', help='Verbose output')
    
    args = parser.parse_args()
    main(hours=args.hours, verbose=args.verbose)
