#!/usr/bin/env python3
"""
Context Embedding Engine
Phase 4.2: Semantic search for Context Bridge

Enables finding contexts by meaning, not just keywords.
Example: "ë°°ê²½ìì•„" will find both "Background Self" and "Alpha" contexts
"""

import numpy as np
from pathlib import Path
from typing import List, Tuple, Optional
import json


class ContextEmbedding:
    """
    Semantic embedding and search for contexts
    Uses sentence-transformers for multilingual support
    """
    
    def __init__(self, model_name: str = "paraphrase-multilingual-mpnet-base-v2"):
        """
        Initialize embedding model
        
        Args:
            model_name: HuggingFace model name
                - paraphrase-multilingual-mpnet-base-v2: Korean + English (768 dim)
                - all-minilm-l6-v2: Faster, English only (384 dim)
        """
        self.model_name = model_name
        self.model = None
        self.cache_path = Path.home() / "agi" / "outputs" / "contexts" / "embeddings.npy"
        self.index_path = Path.home() / "agi" / "outputs" / "contexts" / "embedding_index.json"
        
        # Lazy loading - only load when needed
        self._embeddings_cache: Optional[np.ndarray] = None
        self._index_cache: Optional[dict] = None
    
    def _load_model(self):
        """Lazy load the model (only when needed)"""
        if self.model is None:
            try:
                from sentence_transformers import SentenceTransformer
                print(f"ğŸ“š Loading embedding model: {self.model_name}")
                self.model = SentenceTransformer(self.model_name)
                print("âœ… Model loaded")
            except ImportError:
                print("âš ï¸ sentence-transformers not installed")
                print("Run: pip install sentence-transformers --break-system-packages")
                raise
    
    def embed(self, text: str) -> np.ndarray:
        """
        Generate embedding for a single text
        
        Args:
            text: Input text (Korean/English/Mixed)
            
        Returns:
            Embedding vector (768-dim for multilingual model)
        """
        self._load_model()
        return self.model.encode(text, convert_to_numpy=True)
    
    def embed_batch(self, texts: List[str]) -> np.ndarray:
        """
        Generate embeddings for multiple texts (faster than one-by-one)
        
        Args:
            texts: List of texts
            
        Returns:
            Matrix of embeddings (n_texts Ã— embedding_dim)
        """
        self._load_model()
        return self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        Calculate cosine similarity between two vectors
        Range: -1 (opposite) to 1 (identical)
        """
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    def find_similar(self, query_embedding: np.ndarray, 
                     all_embeddings: np.ndarray, 
                     top_k: int = 5) -> List[Tuple[int, float]]:
        """
        Find most similar embeddings to a query
        
        Args:
            query_embedding: Query vector
            all_embeddings: Matrix of all embeddings
            top_k: Number of results to return
            
        Returns:
            List of (index, similarity_score) tuples, sorted by similarity desc
        """
        # Compute similarities for all vectors at once (vectorized)
        similarities = np.dot(all_embeddings, query_embedding) / (
            np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(query_embedding)
        )
        
        # Get top k indices
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        return [(int(idx), float(similarities[idx])) for idx in top_indices]
    
    def save_embeddings(self, embeddings: np.ndarray, context_ids: List[str]):
        """
        Save embeddings to disk for persistence
        
        Args:
            embeddings: Matrix of embeddings
            context_ids: Corresponding context IDs
        """
        self.cache_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Save embeddings as numpy array
        np.save(self.cache_path, embeddings)
        
        # Save index (context_id -> row number)
        index = {ctx_id: i for i, ctx_id in enumerate(context_ids)}
        with open(self.index_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Saved {len(context_ids)} embeddings to {self.cache_path}")
    
    def load_embeddings(self) -> Tuple[np.ndarray, dict]:
        """
        Load embeddings from disk
        
        Returns:
            (embeddings matrix, {context_id: row_index} dict)
        """
        if not self.cache_path.exists():
            return np.array([]), {}
        
        embeddings = np.load(self.cache_path)
        
        with open(self.index_path, 'r', encoding='utf-8') as f:
            index = json.load(f)
        
        print(f"ğŸ“š Loaded {len(index)} embeddings from cache")
        return embeddings, index


def demo_semantic_search():
    """
    Demonstrate semantic search capabilities
    """
    print("=" * 60)
    print("ğŸ” Semantic Search Demo")
    print("=" * 60)
    
    # Sample contexts (Korean + English)
    contexts = [
        "Alpha Background SelfëŠ” ë°°ê²½ìì•„ê°€ ì˜ì‹ê³¼ ë¬´ì˜ì‹ ì‚¬ì´ë¥¼ ì „í™˜í•˜ëŠ” ì‹œìŠ¤í…œì´ë‹¤",
        "ë¹„ë…¸ì²´ë‹˜ì€ ê´€ì°°ìì´ì ê°œì…ìë¡œì„œì˜ ì—­í• ì„ í•œë‹¤",
        "Context BridgeëŠ” ë ˆì´ì–´ ê°„ ë§¥ë½ì„ ê³µìœ í•œë‹¤",
        "ë¦¬ë“¬ì´ í‹€ë¦¬ë©´ Alphaê°€ ê°œì…í•œë‹¤",
        "ì‹œìŠ¤í…œì˜ ë©´ì—­ ì²´ê³„ ì—­í• ì„ í•œë‹¤"
    ]
    
    try:
        embedder = ContextEmbedding()
        
        print("\nğŸ“Š Generating embeddings for sample contexts...")
        embeddings = embedder.embed_batch(contexts)
        print(f"âœ… Generated embeddings: {embeddings.shape}")
        
        # Test queries
        queries = [
            "ë°°ê²½ìì•„",
            "immune system",
            "intervention"
        ]
        
        for query in queries:
            print(f"\nğŸ” Query: '{query}'")
            query_emb = embedder.embed(query)
            results = embedder.find_similar(query_emb, embeddings, top_k=3)
            
            print("   Top matches:")
            for idx, score in results:
                print(f"   {score:.3f} - {contexts[idx][:60]}...")
    
    except ImportError:
        print("\nâš ï¸ sentence-transformers not yet installed")
        print("Installation in progress...")


if __name__ == "__main__":
    demo_semantic_search()
