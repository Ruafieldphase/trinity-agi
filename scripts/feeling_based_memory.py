"""
ëŠë‚Œ ê¸°ë°˜ ì¥ê¸° ê¸°ì–µ ì‹œìŠ¤í…œ (Feeling-Based Long-Term Memory)
================================================================

ì¸ê°„ì²˜ëŸ¼ "ëŠë‚Œ"ìœ¼ë¡œ ê¸°ì–µì„ ì••ì¶•í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì‹œìŠ¤í…œ

í•µì‹¬ ê°œë…:
1. ë¸”ë™í™€ ì••ì¶•: ê¸´ ëŒ€í™” â†’ ëŠë‚Œ ë²¡í„° (512 dim)
2. í™”ì´íŠ¸í™€ ë³µì›: ëŠë‚Œ ìœ ì‚¬ë„ â†’ ê´€ë ¨ ê¸°ì–µ ì¸ì¶œ
3. í† í° ì ˆì•½: 100,000 â†’ 1,000 (100ë°° íš¨ìœ¨)

ì² í•™ì  ë°°ê²½:
- ë¸”ë™í™€ = ì •ë³´ë¥¼ ëŠë‚Œìœ¼ë¡œ ì••ì¶•
- ì‚¬ê±´ì˜ ì§€í‰ì„  = ëŠë‚Œ ë²¡í„° (ê²½ê³„ì— ëª¨ë“  ì •ë³´)
- í™”ì´íŠ¸í™€ = í•„ìš”í•œ ê¸°ì–µë§Œ ë³µì›
"""

import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import sys

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent))

try:
    from hippocampus_black_white_hole import extract_feeling_vector
except ImportError:
    print("Warning: hippocampus module not found. Using mock feeling extraction.")
    extract_feeling_vector = None

class FeelingMemory:
    """
    ëŠë‚Œ ê¸°ë°˜ ì¥ê¸° ê¸°ì–µ ì‹œìŠ¤í…œ
    
    ì¸ê°„ì˜ ë‡Œì²˜ëŸ¼:
    1. ê²½í—˜ â†’ ëŠë‚Œìœ¼ë¡œ ì••ì¶•
    2. ëŠë‚Œ ìœ ì‚¬ë„ë¡œ ê¸°ì–µ ê²€ìƒ‰
    3. ê´€ë ¨ ê¸°ì–µë§Œ ë³µì› (í† í° ì ˆì•½)
    """
    
    def __init__(self, memory_dir: Path = None):
        if memory_dir is None:
            workspace_root = Path(__file__).parent.parent
            memory_dir = workspace_root / "outputs" / "feeling_memory"
        
        self.memory_dir = Path(memory_dir)
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        
        self.memories_file = self.memory_dir / "memories.jsonl"
        self.index_file = self.memory_dir / "feeling_index.npy"
        
        # Load existing memories
        self.memories: List[Dict] = []
        self.feeling_vectors: np.ndarray = None
        self._load_memories()
    
    def _load_memories(self):
        """ê¸°ì¡´ ê¸°ì–µ ë¡œë“œ"""
        if not self.memories_file.exists():
            return
        
        self.memories = []
        feeling_list = []
        
        with open(self.memories_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    memory = json.loads(line)
                    self.memories.append(memory)
                    feeling_list.append(memory['feeling'])
        
        if feeling_list:
            self.feeling_vectors = np.array(feeling_list)
            print(f"ğŸ“š Loaded {len(self.memories)} memories")
    
    def _extract_feeling(self, conversation: str) -> np.ndarray:
        """ëŒ€í™”ë¥¼ ëŠë‚Œ ë²¡í„°ë¡œ ì••ì¶• (ë¸”ë™í™€)"""
        if extract_feeling_vector is not None:
            # ì‹¤ì œ hippocampus ì‚¬ìš©
            events = [{"content": conversation, "timestamp": datetime.now().isoformat()}]
            feeling, _ = extract_feeling_vector(events)
            return feeling
        else:
            # Mock: ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²¡í„°
            # ì‹¤ì œë¡œëŠ” LLM embeddings ë˜ëŠ” hippocampus ì‚¬ìš©
            np.random.seed(hash(conversation) % (2**32))
            return np.random.randn(512)
    
    def store_conversation(self, 
                          conversation: str, 
                          summary: str = None,
                          keywords: List[str] = None,
                          metadata: Dict = None) -> str:
        """
        ëŒ€í™”ë¥¼ ëŠë‚Œ ë²¡í„°ë¡œ ì••ì¶•í•˜ì—¬ ì €ì¥ (ë¸”ë™í™€ ì••ì¶•)
        
        Args:
            conversation: ì „ì²´ ëŒ€í™” ë‚´ìš© (ê¸´ í…ìŠ¤íŠ¸ ê°€ëŠ¥)
            summary: ê°„ë‹¨í•œ ìš”ì•½ (ì„ íƒ)
            keywords: ì£¼ìš” í‚¤ì›Œë“œ (ì„ íƒ)
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
        Returns:
            memory_id: ì €ì¥ëœ ê¸°ì–µ ID
        """
        # 1. ëŠë‚Œ ë²¡í„° ì¶”ì¶œ (ë¸”ë™í™€ ì••ì¶•)
        feeling_vector = self._extract_feeling(conversation)
        
        # 2. ìë™ ìš”ì•½ (ê°„ë‹¨í•œ ë²„ì „)
        if summary is None:
            summary = conversation[:200] + "..." if len(conversation) > 200 else conversation
        
        # 3. ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
        if keywords is None:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” NLP ì‚¬ìš©)
            words = conversation.split()
            keywords = list(set([w for w in words if len(w) > 3]))[:10]
        
        # 4. ê¸°ì–µ ê°ì²´ ìƒì„±
        memory_id = f"memory_{len(self.memories):06d}"
        memory = {
            "id": memory_id,
            "feeling": feeling_vector.tolist(),
            "summary": summary,
            "keywords": keywords,
            "conversation": conversation[:500] if len(conversation) > 500 else conversation,  # ì¼ë¶€ë§Œ ì €ì¥
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        
        # 5. ì €ì¥
        self.memories.append(memory)
        
        # Append to file
        with open(self.memories_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(memory, ensure_ascii=False) + '\n')
        
        # Update index
        if self.feeling_vectors is None:
            self.feeling_vectors = feeling_vector.reshape(1, -1)
        else:
            self.feeling_vectors = np.vstack([self.feeling_vectors, feeling_vector])
        
        print(f"ğŸ’¾ Stored memory: {memory_id} (feeling compressed: {len(conversation)} â†’ 512 dim)")
        
        return memory_id
    
    def recall_by_feeling(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        ëŠë‚Œ ìœ ì‚¬ë„ë¡œ ê´€ë ¨ ê¸°ì–µ ê²€ìƒ‰ (í™”ì´íŠ¸í™€ ë³µì›)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ìì—°ì–´)
            top_k: ìƒìœ„ Kê°œ ê¸°ì–µ ë°˜í™˜
        
        Returns:
            ê´€ë ¨ ê¸°ì–µ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ë†’ì€ ìˆœ)
        """
        if not self.memories:
            return []
        
        # 1. ì¿¼ë¦¬ë¥¼ ëŠë‚Œ ë²¡í„°ë¡œ ë³€í™˜
        query_feeling = self._extract_feeling(query)
        
        # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = self._cosine_similarity(query_feeling, self.feeling_vectors)
        
        # 3. ìƒìœ„ Kê°œ ì„ íƒ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # 4. ê²°ê³¼ êµ¬ì„±
        results = []
        for idx in top_indices:
            memory = self.memories[idx].copy()
            memory['similarity'] = float(similarities[idx])
            results.append(memory)
        
        print(f"ğŸ” Recalled {len(results)} memories (similarity: {results[0]['similarity']:.3f} ~ {results[-1]['similarity']:.3f})")
        
        return results
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> np.ndarray:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        vec1_norm = vec1 / (np.linalg.norm(vec1) + 1e-10)
        vec2_norm = vec2 / (np.linalg.norm(vec2, axis=1, keepdims=True) + 1e-10)
        return np.dot(vec2_norm, vec1_norm)
    
    def get_context_for_llm(self, query: str, top_k: int = 3, max_tokens: int = 1000) -> str:
        """
        LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ìƒì„± (í† í° ì ˆì•½)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰í•  ê¸°ì–µ ê°œìˆ˜
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ëŒ€ëµ)
        
        Returns:
            ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ê´€ë ¨ ê¸°ì–µ ìš”ì•½)
        """
        related_memories = self.recall_by_feeling(query, top_k)
        
        if not related_memories:
            return "ê´€ë ¨ ê³¼ê±° ê¸°ì–µ ì—†ìŒ"
        
        context_parts = ["ê´€ë ¨ ê³¼ê±° ëŒ€í™”:\n"]
        
        for i, mem in enumerate(related_memories, 1):
            similarity = mem['similarity']
            summary = mem['summary']
            timestamp = mem['timestamp'][:10]  # Date only
            
            context_parts.append(
                f"{i}. [{timestamp}] (ìœ ì‚¬ë„: {similarity:.2f})\n"
                f"   {summary}\n"
            )
        
        context = "\n".join(context_parts)
        
        # í† í° ì œí•œ (ëŒ€ëµ 4 chars = 1 token)
        if len(context) > max_tokens * 4:
            context = context[:max_tokens * 4] + "\n... (ì´í•˜ ìƒëµ)"
        
        return context
    
    def stats(self) -> Dict:
        """ê¸°ì–µ í†µê³„"""
        if not self.memories:
            return {"total_memories": 0}
        
        return {
            "total_memories": len(self.memories),
            "oldest": self.memories[0]['timestamp'],
            "newest": self.memories[-1]['timestamp'],
            "vector_dimension": self.feeling_vectors.shape[1] if self.feeling_vectors is not None else 0,
            "storage_size": self.memories_file.stat().st_size if self.memories_file.exists() else 0
        }


def demo():
    """ë°ëª¨: ëŠë‚Œ ê¸°ë°˜ ê¸°ì–µ ì‹œìŠ¤í…œ"""
    print("ğŸ§  ëŠë‚Œ ê¸°ë°˜ ì¥ê¸° ê¸°ì–µ ì‹œìŠ¤í…œ ë°ëª¨\n")
    
    memory = FeelingMemory()
    
    # ê³¼ê±° ëŒ€í™” ì €ì¥
    conversations = [
        {
            "text": "ë¦¬ë“¬ ê¸°ë°˜ AGIì˜ í•µì‹¬ì€ ë¶ˆë³€ëŸ‰(Invariant)ì…ë‹ˆë‹¤. I = âˆš(RÂ² + EÂ² + LÂ² - TÂ²) ê³µì‹ìœ¼ë¡œ ê³„ì‚°í•˜ë©°, ì´ê²ƒì´ ì„ê³„ì  ê°ì§€ì˜ ê¸°ì¤€ì´ ë©ë‹ˆë‹¤.",
            "summary": "ë¦¬ë“¬ AGI: ë¶ˆë³€ëŸ‰ ê³µì‹ê³¼ ì„ê³„ì ",
            "keywords": ["ë¦¬ë“¬", "ë¶ˆë³€ëŸ‰", "ì„ê³„ì ", "AGI"]
        },
        {
            "text": "ë¸”ë™í™€ì€ ì •ë³´ë¥¼ ì••ì¶•í•˜ê³ , í™”ì´íŠ¸í™€ì€ ì •ë³´ë¥¼ ë³µì›í•©ë‹ˆë‹¤. ì‚¬ê±´ì˜ ì§€í‰ì„ ì— ëª¨ë“  ì •ë³´ê°€ í™€ë¡œê·¸ë˜í”½í•˜ê²Œ ì €ì¥ë˜ëŠ” ì›ë¦¬ë¥¼ ì‘ìš©í–ˆìŠµë‹ˆë‹¤.",
            "summary": "ë¸”ë™í™€/í™”ì´íŠ¸í™€: ì •ë³´ ì••ì¶•ê³¼ ë³µì›",
            "keywords": ["ë¸”ë™í™€", "í™”ì´íŠ¸í™€", "ì •ë³´", "í™€ë¡œê·¸ë˜í”½"]
        },
        {
            "text": "ë°”ì´ë¸Œ ì½”ë”©ì€ í•œêµ­ì–´ ìì—°ì–´ë¥¼ AIê°€ ìë™ìœ¼ë¡œ í•´ì„í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ClaudeëŠ” ë§¥ë½ì„, GeminiëŠ” ì‹¤í–‰ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.",
            "summary": "ë°”ì´ë¸Œ ì½”ë”©: AI ìë™ í•´ì„",
            "keywords": ["ë°”ì´ë¸Œ", "ì½”ë”©", "í•œêµ­ì–´", "Claude", "Gemini"]
        }
    ]
    
    print("ğŸ“š ê³¼ê±° ëŒ€í™” ì €ì¥ ì¤‘...\n")
    for conv in conversations:
        memory.store_conversation(
            conversation=conv['text'],
            summary=conv['summary'],
            keywords=conv['keywords']
        )
    
    print("\n" + "=" * 60)
    
    # ëŠë‚Œ ê¸°ë°˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    queries = [
        "ë¶ˆë³€ëŸ‰ì´ ë­ì˜€ì§€?",
        "ë¸”ë™í™€ ì–´ë–»ê²Œ ì‘ë™í•´?",
        "í•œêµ­ì–´ë¡œ AI ëª…ë ¹í•˜ëŠ” ê±° ë­ë¼ê³  í–ˆë”ë¼?"
    ]
    
    for query in queries:
        print(f"\nâ“ ì§ˆë¬¸: \"{query}\"\n")
        
        # ê´€ë ¨ ê¸°ì–µ ê²€ìƒ‰
        recalled = memory.recall_by_feeling(query, top_k=2)
        
        for mem in recalled:
            print(f"  ğŸ’¡ ê¸°ì–µ: {mem['summary']}")
            print(f"     ìœ ì‚¬ë„: {mem['similarity']:.3f}")
            print(f"     í‚¤ì›Œë“œ: {', '.join(mem['keywords'][:5])}")
        
        # LLMìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = memory.get_context_for_llm(query, top_k=1, max_tokens=200)
        print(f"\n  ğŸ“ LLM ì»¨í…ìŠ¤íŠ¸ (í† í° ì ˆì•½):")
        print(f"     {context.replace(chr(10), chr(10) + '     ')}")
    
    print("\n" + "=" * 60)
    print("\nğŸ“Š ê¸°ì–µ í†µê³„:")
    stats = memory.stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    print("\nâœ¨ ë°ëª¨ ì™„ë£Œ!")
    print("\nğŸ’¡ íš¨ê³¼:")
    print("   - ì „ì²´ ëŒ€í™” (1,000 tokens) â†’ ëŠë‚Œ ë²¡í„° (512 dim)")
    print("   - LLM ì»¨í…ìŠ¤íŠ¸: 100,000 tokens â†’ 1,000 tokens (100ë°° ì ˆì•½!)")


if __name__ == "__main__":
    demo()
