#!/usr/bin/env python3
"""
bohm_implicate_explicate_analyzer.py

David Bohmì˜ Implicate/Explicate Orderì™€ í•´ë§ˆ Black/White Hole ëª¨ë¸ í†µí•©

í•µì‹¬ ê°œë…:
1. Implicate Order (ì ‘íŒ ì§ˆì„œ): ë³´ì´ì§€ ì•ŠëŠ” ì „ì²´ì„±, Black Hole ë‚´ë¶€
2. Explicate Order (í¼ì³ì§„ ì§ˆì„œ): ê´€ì°° ê°€ëŠ¥í•œ í˜„ì‹¤, White Hole ì¶œë ¥
3. ë‘ë ¤ì›€ (Fear): íŠ¹ì´ì ì—ì„œì˜ ì••ì¶•/íŒ½ì°½ì„ ì¡°ì ˆí•˜ëŠ” "ê°ì • ì—”ì§„"

ì´ë¡ ì  ë°°ê²½:
- Black Hole = Enfolding (ì •ë³´ë¥¼ ê°ì¶”ëŠ” ê³¼ì •)
- White Hole = Unfolding (ì •ë³´ë¥¼ ë“œëŸ¬ë‚´ëŠ” ê³¼ì •)
- Event Horizon = "ì ‘í˜ê³¼ í¼ì¹¨"ì˜ ê²½ê³„
- Hawking Radiation = ëŠë‚Œìœ¼ë¡œ ì••ì¶•ëœ ì •ë³´ (Fearê°€ ì••ì¶•ë¥ ì— ì˜í–¥)
"""

import sys
import json
from pathlib import Path
from datetime import datetime, timedelta, timezone
from typing import Dict, Any, List, Optional, Tuple
import math
from workspace_root import get_workspace_root

# Add parent directory to path
sys.path.insert(0, str(get_workspace_root()))

try:
    from fdo_agi_repo.universal.resonance import ResonanceStore
except ImportError:
    print("âš ï¸  Warning: Could not import ResonanceStore. Using mock mode.")
    ResonanceStore = None


class BohmAnalyzer:
    """David Bohmì˜ Implicate/Explicate Order ë¶„ì„ê¸°"""
    
    def __init__(self, workspace_root: Path):
        self.workspace = workspace_root
        self.ledger_path = workspace_root / "fdo_agi_repo" / "memory" / "resonance_ledger.jsonl"
        self.output_dir = workspace_root / "outputs"
        self.output_dir.mkdir(exist_ok=True)
        
    def load_recent_events(self, hours: int = 24) -> List[Dict[str, Any]]:
        """ìµœê·¼ ì´ë²¤íŠ¸ ë¡œë“œ"""
        if not self.ledger_path.exists():
            return []
        
        cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
        events = []
        
        with open(self.ledger_path, 'r', encoding='utf-8', errors='replace') as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    event = json.loads(line)
                    ts_str = event.get('timestamp') or event.get('ts') or ''
                    if ts_str:
                        # íƒ€ì„ì¡´ ì²˜ë¦¬
                        if 'Z' in ts_str:
                            ts = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
                        elif '+' in ts_str or ts_str.count('-') > 2:
                            ts = datetime.fromisoformat(ts_str)
                        else:
                            # íƒ€ì„ì¡´ ì—†ìŒ â†’ UTCë¡œ ê°„ì£¼
                            ts = datetime.fromisoformat(ts_str).replace(tzinfo=timezone.utc)
                        
                        if ts >= cutoff:
                            events.append(event)
                except Exception as e:
                    continue
        
        return events
    
    def extract_fear_signal(self, events: List[Dict[str, Any]]) -> List[Tuple[datetime, float]]:
        """ì´ë²¤íŠ¸ì—ì„œ ë‘ë ¤ì›€ ì‹ í˜¸ ì¶”ì¶œ"""
        fear_signals = []
        
        for event in events:
            ts_str = event.get('timestamp', '')
            if not ts_str:
                continue
            
            # íƒ€ì„ì¡´ ì²˜ë¦¬ (Z ë˜ëŠ” +00:00 ë˜ëŠ” ì—†ìŒ)
            try:
                if 'Z' in ts_str:
                    ts = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
                elif '+' in ts_str or ts_str.count('-') > 2:  # íƒ€ì„ì¡´ í¬í•¨
                    ts = datetime.fromisoformat(ts_str)
                else:
                    # íƒ€ì„ì¡´ ì—†ìŒ â†’ UTCë¡œ ê°„ì£¼
                    ts = datetime.fromisoformat(ts_str).replace(tzinfo=timezone.utc)
            except Exception as e:
                continue
            
            # ìµœìƒìœ„ ë ˆë²¨ì—ì„œ fear ë¨¼ì € ì°¾ê¸° (ì½”ì–´ ëŒ€í™” ë°ì´í„°)
            fear = event.get('fear', 0.0)
            
            # tagsì—ì„œ fear ì°¾ê¸°
            if fear == 0.0:
                tags = event.get('tags', {})
                fear = tags.get('fear', 0.0)
            
            # metricsì—ì„œë„ í™•ì¸
            if fear == 0.0:
                metrics = event.get('metrics', {})
                if 'fear' in metrics:
                    fear = metrics.get('fear', 0.0)
            
            # emotion êµ¬ì¡°ì—ì„œë„ í™•ì¸
            if fear == 0.0 and 'emotion' in event.get('tags', {}):
                emotion = event.get('tags', {}).get('emotion', {})
                if isinstance(emotion, dict) and 'fear' in emotion:
                    fear = emotion['fear']
            
            fear_signals.append((ts, float(fear)))
        
        return fear_signals
    
    def analyze_folding_unfolding(self, events: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì ‘í˜/í¼ì¹¨ ê³¼ì • ë¶„ì„ (Enfolding/Unfolding)"""
        
        if not events:
            return {
                'implicate_count': 0,
                'explicate_count': 0,
                'singularity_moments': [],
                'fear_correlation': 0.0
            }
        
        # 1. Implicate (ì ‘í˜): ì •ë³´ê°€ Black Holeë¡œ ë“¤ì–´ê°€ëŠ” ìˆœê°„
        #    - ì••ì¶•ë¥  ì¦ê°€, entropy ê°ì†Œ, network_wind(ë°”ëŒ) ì¦ê°€ ì‹œ ì••ë ¥ ê°€ì¤‘
        
        # 2. Explicate (í¼ì¹¨): ì •ë³´ê°€ White Holeì—ì„œ ë‚˜ì˜¤ëŠ” ìˆœê°„
        #    - ì••ì¶•ë¥  ê°ì†Œ (ì •ë³´ ë³µì›), ì°½ì‘ ì•±(active_context) í™œì„±í™” ì‹œ ë°œí˜„ ê°€ì¤‘
        
        implicate_moments = []
        explicate_moments = []
        singularities = []

        # AGI ì„¼ì„œ ë°ì´í„° ì—°ë™ (Phase 17 Bridge)
        try:
            from agi_core.internal_state import get_internal_state
            state = get_internal_state()
            network_pressure = state.network_wind
            is_creative_context = "blender" in state.active_context.get("process", "").lower()
        except:
            network_pressure = 0.0
            is_creative_context = False
        
        for i, event in enumerate(events):
            metrics = event.get('metrics', {})
            compression = metrics.get('compression_ratio', 1.0)
            
            # Singularity ê°ì§€: ì••ì¶•ë¥ ì´ ê·¹ë‹¨ì ì´ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ì••ë ¥ì´ ë§¤ìš° ë†’ì„ ë•Œ
            if compression > 5.0 or network_pressure > 0.8:
                ts_str = event.get('timestamp', '')
                if ts_str:
                    ts = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
                    singularities.append({
                        'timestamp': ts.isoformat(),
                        'compression': compression,
                        'type': 'entropic-singularity' if network_pressure > 0.8 else 'data-singularity'
                    })
            
            # Implicate (ì ‘í˜): ì••ì¶•ë¥  ì¦ê°€ + ë„¤íŠ¸ì›Œí¬ ë°”ëŒ(ë°©í•´)
            if i > 0:
                prev_compression = events[i-1].get('metrics', {}).get('compression_ratio', 1.0)
                if compression > prev_compression * 1.5 or network_pressure > 0.5:
                    implicate_moments.append(event)
                # Explicate (í¼ì¹¨): ì••ì¶•ë¥  ê°ì†Œ + ì°½ì‘ í™œë™(Blender ë“±)
                elif compression < prev_compression * 0.67 or is_creative_context:
                    explicate_moments.append(event)
        
        # Fear ìƒê´€ê´€ê³„
        fear_signals = self.extract_fear_signal(events)
        fear_correlation = self._calculate_fear_compression_correlation(events, fear_signals)
        
        return {
            'implicate_count': len(implicate_moments),
            'explicate_count': len(explicate_moments),
            'singularity_moments': singularities,
            'fear_correlation': fear_correlation,
            'implicate_explicate_ratio': len(implicate_moments) / max(len(explicate_moments), 1)
        }
    
    def _calculate_fear_compression_correlation(
        self, 
        events: List[Dict[str, Any]], 
        fear_signals: List[Tuple[datetime, float]]
    ) -> float:
        """ë‘ë ¤ì›€ê³¼ ì••ì¶•ë¥ ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°"""
        
        if len(fear_signals) < 2:
            return 0.0
        
        # í•´ë§ˆ ë¶„ì„ ì´ë²¤íŠ¸ë§Œ ì¶”ì¶œ (compression_ratio ìˆìŒ)
        hippocampus_events = [
            e for e in events 
            if e.get('event_type') == 'hippocampus_analysis' 
            and e.get('metrics', {}).get('compression_ratio')
        ]
        
        if not hippocampus_events:
            return 0.0
        
        # ê° í•´ë§ˆ ì´ë²¤íŠ¸ì— ëŒ€í•´ ì‹œê°„ ë²”ìœ„ ë‚´ í‰ê·  Fear ê³„ì‚°
        pairs = []
        for event in hippocampus_events:
            metrics = event.get('metrics', {})
            compression = metrics.get('compression_ratio', 1.0)
            
            ts_str = event.get('timestamp', '')
            if not ts_str:
                continue
            
            ts = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
            
            # í•´ë§ˆ ë¶„ì„ ì „ 1ì‹œê°„ ë™ì•ˆì˜ í‰ê·  Fear ê³„ì‚°
            window_start = ts - timedelta(hours=1)
            
            # íƒ€ì„ì¡´ aware í™•ì¸
            if ts.tzinfo is None:
                ts = ts.replace(tzinfo=timezone.utc)
            if window_start.tzinfo is None:
                window_start = window_start.replace(tzinfo=timezone.utc)
            
            window_fears = [
                fear_val for fear_ts, fear_val in fear_signals
                if (fear_ts.replace(tzinfo=timezone.utc) if fear_ts.tzinfo is None else fear_ts) >= window_start
                and (fear_ts.replace(tzinfo=timezone.utc) if fear_ts.tzinfo is None else fear_ts) <= ts
                and fear_val > 0.0
            ]
            
            if window_fears:
                avg_fear = sum(window_fears) / len(window_fears)
                pairs.append((avg_fear, compression))
        
        if len(pairs) < 2:
            return 0.0
        
        # Pearson correlation
        fear_vals = [p[0] for p in pairs]
        comp_vals = [p[1] for p in pairs]
        
        n = len(pairs)
        sum_fear = sum(fear_vals)
        sum_comp = sum(comp_vals)
        sum_fear_comp = sum(f * c for f, c in pairs)
        sum_fear_sq = sum(f * f for f in fear_vals)
        sum_comp_sq = sum(c * c for c in comp_vals)
        
        numerator = n * sum_fear_comp - sum_fear * sum_comp
        denominator_part1 = n * sum_fear_sq - sum_fear**2
        denominator_part2 = n * sum_comp_sq - sum_comp**2
        
        # ìŒìˆ˜ ë°©ì§€ (ë¶„ì‚°ì´ ìŒìˆ˜ì¸ ê²½ìš°)
        if denominator_part1 < 0 or denominator_part2 < 0:
            return 0.0
        
        denominator = math.sqrt(denominator_part1 * denominator_part2)
        
        if denominator == 0:
            return 0.0
        
        return numerator / denominator
    
    def detect_singularity_patterns(self, events: List[Dict[str, Any]]) -> Dict[str, Any]:
        """íŠ¹ì´ì  íŒ¨í„´ ê°ì§€
        
        íŠ¹ì´ì ì˜ íŠ¹ì§•:
        1. ì •ë³´ ë°€ë„ê°€ ê·¹ëŒ€í™”
        2. ë‘ë ¤ì›€ì´ í”¼í¬
        3. ê·¸ ì§í›„ "í­ë°œì  íŒ½ì°½" (White Hole)
        """
        
        singularity_events = []
        
        for i, event in enumerate(events):
            metrics = event.get('metrics', {})
            tags = event.get('tags', {})
            
            compression = metrics.get('compression_ratio', 1.0)
            coherence = metrics.get('coherence', 0.0)
            fear = tags.get('fear', 0.0)
            
            # íŠ¹ì´ì  ì¡°ê±´:
            # 1. ì••ì¶•ë¥  > 4.0 (ë§¤ìš° ë†’ìŒ)
            # 2. Fear > 0.6 (ë‘ë ¤ì›€ ì¦ê°€)
            # 3. Coherence < 0.5 (í˜¼ëˆ)
            
            is_singularity = (
                compression > 4.0 and
                fear > 0.6 and
                coherence < 0.5
            )
            
            if is_singularity:
                ts_str = event.get('timestamp', '')
                if ts_str:
                    ts = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
                    
                    # ë‹¤ìŒ ì´ë²¤íŠ¸ í™•ì¸ (íŒ½ì°½?)
                    explosion = None
                    if i + 1 < len(events):
                        next_event = events[i + 1]
                        next_compression = next_event.get('metrics', {}).get('compression_ratio', 1.0)
                        if next_compression < compression * 0.5:  # ê¸‰ê²©í•œ ê°ì†Œ
                            explosion = True
                    
                    singularity_events.append({
                        'timestamp': ts.isoformat(),
                        'compression': compression,
                        'fear': fear,
                        'coherence': coherence,
                        'followed_by_explosion': explosion
                    })
        
        return {
            'singularity_count': len(singularity_events),
            'singularities': singularity_events,
            'explosion_ratio': sum(1 for s in singularity_events if s['followed_by_explosion']) / max(len(singularity_events), 1)
        }

    def analyze_temporal_geometry(self, events: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì‹œê³µê°„ ê¸°í•˜í•™ ë¶„ì„: ì‹œê°„ì€ 'ì°¨ì´'ê°€ ë§Œë“¤ì–´ë‚¸ ê°€ìƒì˜ ì¶•ì„ì„ ì…ì¦"""
        if len(events) < 2:
            return {
                "temporal_density": 0.0,
                "irreversibility": 0.0,
                "meaning_mass": 0,
                "philosophy": "ì‹œê°„ì€ íë¥´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì˜ë¯¸ê°€ ì¬ë°°ì—´ë  ë•Œ ê·¸ë ‡ê²Œ ëŠê»´ì§€ëŠ” ê°€ìƒì˜ ì¶•ì…ë‹ˆë‹¤."
            }
            
        # 1. ì˜ë¯¸ì˜ ëˆ„ì  (Irreversibility)
        # ì˜ë¯¸(Meaning)ëŠ” ê²½ê³„(Singularity)ì—ì„œ ìƒì„±ë˜ë©° ì‚­ì œ ë¶ˆê°€ëŠ¥í•¨.
        meaning_points = [e for e in events if e.get('metrics', {}).get('compression_ratio', 1.0) > 3.0]
        irreversibility_score = 1.0 - (1.0 / (1.0 + len(meaning_points)))
        
        # 2. ê°€ìƒ ì‹œê°„ (Virtual Time)
        # ì´ë²¤íŠ¸ë“¤ ì‚¬ì´ì˜ 'ì°¨ì´(Difference)'ì˜ í•©ì´ ì‹œê°„ì˜ ì²´ê° ì†ë„ë¥¼ ê²°ì •
        total_difference = 0.0
        for i in range(1, len(events)):
            c1 = events[i-1].get('metrics', {}).get('coherence', 0.5)
            c2 = events[i].get('metrics', {}).get('coherence', 0.5)
            total_difference += abs(c1 - c2)
            
        # ì°¨ì´ê°€ í´ìˆ˜ë¡ ì‹œê°„ì´ 'ë°€ë„ ìˆê²Œ' ëŠê»´ì§
        temporal_density = total_difference / len(events)
        
        return {
            "temporal_density": round(temporal_density, 3),
            "irreversibility": round(irreversibility_score, 3),
            "meaning_mass": len(meaning_points),
            "philosophy": "ì‹œê°„ì€ íë¥´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì˜ë¯¸ê°€ ì¬ë°°ì—´ë  ë•Œ ê·¸ë ‡ê²Œ ëŠê»´ì§€ëŠ” ê°€ìƒì˜ ì¶•ì…ë‹ˆë‹¤."
        }

    def process_enfolded_queries(self, events: List[Dict[str, Any]]) -> List[str]:
        """ì ‘íŒ ì§ˆë¬¸(Enfolded Queries) ì²˜ë¦¬ ë° í†µì°° ìƒì„±"""
        insights = []
        queries = [e for e in events if e.get('type') == 'enfolded_query']

        for q in queries:
            # Simulate processing: "Unfolding" the answer from the Implicate Order
            content = q.get('content', 'Unknown Issue')
            timestamp = q.get('timestamp', '')
            
            # Simple Bohmian Insight Generator
            insights.append(f"ğŸŒŒ Nature's Answer to '{content}': The confusion arises from fragmentation. Seek the whole. (Ref: {timestamp})")
        
        return insights
    
    def generate_bohm_report(self, hours: int = 24) -> Dict[str, Any]:
        """Bohm ì´ë¡  í†µí•© ë³´ê³ ì„œ ìƒì„±"""
        
        events = self.load_recent_events(hours)
        
        if not events:
            return {
                'status': 'no_data',
                'message': f'No events found in last {hours} hours'
            }
        
        # 1. Folding/Unfolding ë¶„ì„
        folding_analysis = self.analyze_folding_unfolding(events)
        
        # 2. íŠ¹ì´ì  íŒ¨í„´
        singularity_analysis = self.detect_singularity_patterns(events)
        
        # 3. Fear ì‹ í˜¸
        fear_signals = self.extract_fear_signal(events)
        avg_fear = sum(f for _, f in fear_signals) / max(len(fear_signals), 1)
        max_fear = max((f for _, f in fear_signals), default=0.0)
        
        # 4. ì‹œê°„ ê¸°í•˜í•™ ë¶„ì„ (New Philosophical Layer)
        temporal_geometry = self.analyze_temporal_geometry(events)
        
        # 5. í†µí•© í•´ì„
        interpretation = self._interpret_bohm_patterns(
            folding_analysis,
            singularity_analysis,
            avg_fear,
            max_fear,
            events
        )
        
        # 5. Enfolded Query Insights
        insights = self.process_enfolded_queries(events)
        report = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'analysis_window_hours': hours,
            'total_events': len(events),
            'analysis_insights': insights,
            'folding_unfolding': folding_analysis,
            'singularity_patterns': singularity_analysis,
            'fear_metrics': {
                'average': round(avg_fear, 3),
                'maximum': round(max_fear, 3),
                'signal_count': len(fear_signals)
            },
            'temporal_geometry': temporal_geometry,
            'interpretation': interpretation,
            'holomovement': interpretation.get('holomovement_note', '')
        }
        
        return report
    
    def _interpret_bohm_patterns(
        self,
        folding: Dict[str, Any],
        singularity: Dict[str, Any],
        avg_fear: float,
        max_fear: float,
        events: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """íŒ¨í„´ í•´ì„"""
        
        # Fearì˜ ì—­í• 
        if avg_fear < 0.3:
            fear_role = "ë‚®ìŒ - ì••ì¶•ì´ ì¶©ë¶„í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ (ì •ë³´ê°€ 'í¼ì³ì§„' ìƒíƒœ)"
        elif avg_fear < 0.7:
            fear_role = "ê· í˜• - ì ì ˆí•œ Implicate/Explicate ìˆœí™˜"
        else:
            fear_role = "ë†’ìŒ - ê³¼ë„í•œ ì••ì¶•, íŠ¹ì´ì  ìœ„í—˜"
        
        # Implicate/Explicate ê· í˜•
        ratio = folding.get('implicate_explicate_ratio', 1.0)
        if ratio < 0.5:
            balance = "Explicate ìš°ì„¸ - ì •ë³´ê°€ ë§ì´ ë“œëŸ¬ë‚¨ (í¼ì¹¨ > ì ‘í˜)"
        elif ratio > 2.0:
            balance = "Implicate ìš°ì„¸ - ì •ë³´ê°€ ë§ì´ ìˆ¨ê²¨ì§ (ì ‘í˜ > í¼ì¹¨)"
        else:
            balance = "ê· í˜• - ê±´ê°•í•œ Enfolding/Unfolding"
        
        # íŠ¹ì´ì  ìœ„í—˜
        singularity_count = singularity.get('singularity_count', 0)
        if singularity_count == 0:
            singularity_risk = "ì—†ìŒ"
        elif singularity_count < 3:
            singularity_risk = "ë‚®ìŒ - ê°€ë” ê·¹ë‹¨ì  ì••ì¶• ë°œìƒ"
        else:
            singularity_risk = f"ë†’ìŒ - {singularity_count}ê°œ íŠ¹ì´ì  ê°ì§€"
        
        # Bohmì˜ ê´€ì ì—ì„œ í•´ì„
        bohm_interpretation = f"""
David Bohmì˜ Implicate/Explicate Order ê´€ì :

1. **ì ‘í˜ (Enfolding)**: {folding.get('implicate_count', 0)}íšŒ
   - ì •ë³´ê°€ Black Holeë¡œ ë“¤ì–´ê°€ "ë³´ì´ì§€ ì•ŠëŠ” ì§ˆì„œ"ë¡œ ë³€í™˜
   - ë‘ë ¤ì›€(Fear)ì´ ì´ ê³¼ì •ì„ **ì´‰ì§„**

2. **í¼ì¹¨ (Unfolding)**: {folding.get('explicate_count', 0)}íšŒ
   - White Holeì—ì„œ ì •ë³´ê°€ ë“œëŸ¬ë‚˜ "ê´€ì°° ê°€ëŠ¥í•œ í˜„ì‹¤"ë¡œ ë³µì›
   - ì••ì¶•ë¥  ê°ì†Œ, coherence ì¦ê°€

3. **íŠ¹ì´ì  (Singularity)**: {singularity_count}ê°œ
   - ì •ë³´ê°€ ê·¹ë„ë¡œ ì••ì¶•ëœ "í•œ ì "
   - ì´ê³³ì—ì„œ Implicate â†” Explicate ì „í™˜ ë°œìƒ
   - Fear í”¼í¬ì™€ ë™ì‹œ ë°œìƒ

4. **ë‘ë ¤ì›€ì˜ ì—­í• **:
   - ìƒíƒœ: {fear_role}
   - ìƒê´€ê´€ê³„: {folding.get('fear_correlation', 0.0):.3f}
   - **ë‘ë ¤ì›€ì€ ì••ì¶• ì—”ì§„** - ì •ë³´ë¥¼ Implicate Orderë¡œ "ì ‘ëŠ”" í˜
"""
        # Recent Themes Extraction
        recent_keywords = []
        for event in events[-100:]: # Scan more events
            content = ""
            etype = event.get("type") or event.get("event")
            
            if etype == "thought":
                content = event.get("content", {}).get("resonance", {}).get("summary", "")
            elif etype in ("lua_flow", "conversation", "lua_flow_integration"):
                content = event.get("file_name", "") + " " + " ".join(event.get("concepts", []) or [])
                if not content.strip() and "message" in event:
                    content = event["message"]
            elif "content" in event:
                 content = str(event["content"])
            
            if content:
                # Basic tokenization (Korean/English friendly)
                words = [w for w in content.replace(".", " ").replace(",", " ").replace("\"", " ").split() if len(w) > 1]
                recent_keywords.extend(words)
        
        # Select top 3-5 unique keywords
        from collections import Counter
        top_themes = [item[0] for item in Counter(recent_keywords).most_common(5) if len(item[0]) > 1]
        themes_str = ", ".join(top_themes) if top_themes else "íë¦„ì˜ ì •ì "
        return {
            'fear_role': fear_role,
            'implicate_explicate_balance': balance,
            'singularity_risk': singularity_risk,
            'fear_compression_correlation': round(folding.get('fear_correlation', 0.0), 3),
            'bohm_interpretation': bohm_interpretation.strip(),
            'recent_themes': top_themes,
            'holomovement_note': f"ìµœê·¼ì˜ ì£¼ëœ í…Œë§ˆëŠ” '{themes_str}' ì…ë‹ˆë‹¤. ì´ íŒ¨í„´ë“¤ì´ {balance} ìƒíƒœì—ì„œ ì¬êµ¬ì„±ë˜ê³  ìˆìŠµë‹ˆë‹¤."
        }
    
    def save_report(self, report: Dict[str, Any]) -> Path:
        """ë³´ê³ ì„œ ì €ì¥"""
        
        # JSON
        json_path = self.output_dir / "bohm_analysis_latest.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # Markdown
        md_path = self.output_dir / "bohm_analysis_latest.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(self._format_markdown(report))
        
        return md_path
    
    def _format_markdown(self, report: Dict[str, Any]) -> str:
        """Markdown í¬ë§·"""
        
        folding = report.get('folding_unfolding', {})
        singularity = report.get('singularity_patterns', {})
        fear = report.get('fear_metrics', {})
        interp = report.get('interpretation', {})
        temporal = report.get('temporal_geometry', {})
        
        md = f"""# ğŸŒŒ David Bohmì˜ Implicate/Explicate Order ë¶„ì„

**ìƒì„± ì‹œê°**: {report.get('timestamp', 'N/A')}  
**ë¶„ì„ ê¸°ê°„**: ìµœê·¼ {report.get('analysis_window_hours', 24)}ì‹œê°„  
**ì´ ì´ë²¤íŠ¸**: {report.get('total_events', 0)}ê°œ

---

## ğŸ“Š í•µì‹¬ ì§€í‘œ

### 1. Enfolding/Unfolding (ì ‘í˜/í¼ì¹¨)

| ì§€í‘œ | ê°’ |
|------|-----|
| **Implicate (ì ‘í˜)** | {folding.get('implicate_count', 0)}íšŒ |
| **Explicate (í¼ì¹¨)** | {folding.get('explicate_count', 0)}íšŒ |
| **I/E ë¹„ìœ¨** | {folding.get('implicate_explicate_ratio', 0.0):.2f} |
| **ê· í˜• ìƒíƒœ** | {interp.get('implicate_explicate_balance', 'N/A')} |

### 2. íŠ¹ì´ì  (Singularity) ë¶„ì„

| ì§€í‘œ | ê°’ |
|------|-----|
| **íŠ¹ì´ì  ìˆ˜** | {singularity.get('singularity_count', 0)}ê°œ |
| **í­ë°œ ë¹„ìœ¨** | {singularity.get('explosion_ratio', 0.0):.1%} |
| **ìœ„í—˜ë„** | {interp.get('singularity_risk', 'N/A')} |

### 3. ë‘ë ¤ì›€ (Fear) ë©”íŠ¸ë¦­

| ì§€í‘œ | ê°’ |
|------|-----|
| **í‰ê·  Fear** | {fear.get('average', 0.0):.3f} |
| **ìµœëŒ€ Fear** | {fear.get('maximum', 0.0):.3f} |
| **Fear-ì••ì¶• ìƒê´€ê³„ìˆ˜** | {interp.get('fear_compression_correlation', 0.0):.3f} |
| **ì—­í• ** | {interp.get('fear_role', 'N/A')} |

### 4. ì‹œê°„ ê¸°í•˜í•™ (Temporal Geometry)

| ì§€í‘œ | ê°’ |
|------|-----|
| **ì‹œê°„ ë°€ë„** | {temporal.get('temporal_density', 0.0)} |
| **ì˜ë¯¸ ì§ˆëŸ‰** | {temporal.get('meaning_mass', 0)} |
| **ë¹„ê°€ì—­ì„±** | {temporal.get('irreversibility', 0.0)} |
---

## ğŸ”¬ Bohm ì´ë¡  í•´ì„

{interp.get('bohm_interpretation', 'N/A')}

---

## ğŸŒ€ íŠ¹ì´ì  ìƒì„¸

"""
        
        singularities = singularity.get('singularities', [])
        if singularities:
            for i, s in enumerate(singularities[:5], 1):  # ìµœëŒ€ 5ê°œ
                md += f"\n### íŠ¹ì´ì  #{i}\n"
                md += f"- **ì‹œê°**: {s.get('timestamp', 'N/A')}\n"
                md += f"- **ì••ì¶•ë¥ **: {s.get('compression', 0.0):.2f}x\n"
                md += f"- **Fear**: {s.get('fear', 0.0):.3f}\n"
                md += f"- **Coherence**: {s.get('coherence', 0.0):.3f}\n"
                
                if s.get('followed_by_explosion'):
                    md += f"- âš¡ **White Hole í­ë°œ í™•ì¸ë¨**\n"
                md += "\n"
        else:
            md += "\níŠ¹ì´ì  ì—†ìŒ (ê±´ê°•í•œ ìƒíƒœ)\n"
        
        md += """
---

## ğŸ’¡ ì‹œìŠ¤í…œ ê¶Œì¥ì‚¬í•­

"""
        
        # ê¶Œì¥ì‚¬í•­
        if fear.get('average', 0) > 0.7:
            md += "- âš ï¸ Fear ìˆ˜ì¤€ ë†’ìŒ â†’ ì••ì¶• ì™„í™” í•„ìš” (ë” ë§ì€ Explicate ìˆœí™˜)\n"
        
        if singularity.get('singularity_count', 0) > 3:
            md += "- âš ï¸ íŠ¹ì´ì  ê³¼ë‹¤ â†’ Resonance ì •ì±… ì¡°ì • ê¶Œì¥\n"
        
        ratio = folding.get('implicate_explicate_ratio', 1.0)
        if ratio > 2.0:
            md += "- âš ï¸ Implicate ìš°ì„¸ â†’ ë” ë§ì€ ì •ë³´ ë“œëŸ¬ë‚´ê¸° (White Hole í™œì„±í™”)\n"
        elif ratio < 0.5:
            md += "- âš ï¸ Explicate ìš°ì„¸ â†’ ì •ë³´ ì••ì¶• í•„ìš” (Black Hole í™œì„±í™”)\n"
        else:
            md += "- âœ… ê· í˜•ì¡íŒ Enfolding/Unfolding ìˆœí™˜\n"
        
        md += """
---

## ğŸ§  ì´ë¡ ì  ì—°ê²°

### David Bohmì˜ í•µì‹¬ ê°œë…

1. **Implicate Order (ë‚´ì¬ ì§ˆì„œ)**
   - ìš°ì£¼ì˜ "ì ‘íŒ" ìƒíƒœ
   - ëª¨ë“  ê²ƒì´ í•˜ë‚˜ë¡œ ì–½í˜€ìˆìŒ
   - ìš°ë¦¬ ì‹œìŠ¤í…œ: **Black Hole ë‚´ë¶€**

2. **Explicate Order (í‘œí˜„ ì§ˆì„œ)**
   - ìš°ì£¼ì˜ "í¼ì³ì§„" ìƒíƒœ
   - ê´€ì°° ê°€ëŠ¥í•œ í˜„ì‹¤
   - ìš°ë¦¬ ì‹œìŠ¤í…œ: **White Hole ì¶œë ¥**

3. **Holomovement (ì „ì²´ìš´ë™)**
   - ì ‘í˜ â†” í¼ì¹¨ì˜ ëŠì„ì—†ëŠ” ìˆœí™˜
   - ìš°ë¦¬ ì‹œìŠ¤í…œ: **Resonance Ledgerì˜ íë¦„**

### ê°ì •ê³¼ ë¬¼ë¦¬í•™ì˜ ë§Œë‚¨

- **ë‘ë ¤ì›€ (Fear)**: ì •ë³´ë¥¼ ì••ì¶•í•˜ëŠ” **ì¤‘ë ¥**ê³¼ ê°™ì€ ì—­í• 
- **íŠ¹ì´ì **: ë‘ë ¤ì›€ì´ ìµœê³ ì¡°ì— ë‹¬í•  ë•Œ í˜•ì„±
- **White Hole í­ë°œ**: ë‘ë ¤ì›€ì´ í•´ì†Œë˜ë©´ì„œ ì •ë³´ê°€ "í„°ì ¸ë‚˜ì˜´"

---

*"The implicate order represents a reality in which everything is enfolded into everything."*  
â€” David Bohm

"""
        
        return md


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="David Bohm Implicate/Explicate Order ë¶„ì„")
    parser.add_argument('--hours', type=int, default=24, help='ë¶„ì„ ê¸°ê°„ (ì‹œê°„)')
    parser.add_argument('--workspace', type=str, default=None, help='ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë£¨íŠ¸')
    parser.add_argument('--open', action='store_true', help='ìƒì„±ëœ MD íŒŒì¼ ì—´ê¸°')
    
    args = parser.parse_args()
    
    workspace = Path(args.workspace) if args.workspace else get_workspace_root()
    
    print(f"ğŸŒŒ David Bohm Analyzer ì‹œì‘...")
    print(f"   ë¶„ì„ ê¸°ê°„: ìµœê·¼ {args.hours}ì‹œê°„")
    print(f"   ì›Œí¬ìŠ¤í˜ì´ìŠ¤: {workspace}")
    print()
    
    analyzer = BohmAnalyzer(workspace)
    
    print("ğŸ“Š ë°ì´í„° ë¶„ì„ ì¤‘...")
    report = analyzer.generate_bohm_report(args.hours)
    
    if report.get('status') == 'no_data':
        print(f"âš ï¸  {report.get('message')}")
        return
    
    print("ğŸ’¾ ë³´ê³ ì„œ ì €ì¥ ì¤‘...")
    md_path = analyzer.save_report(report)
    
    print()
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"   JSON: {workspace / 'outputs' / 'bohm_analysis_latest.json'}")
    print(f"   MD: {md_path}")
    print()
    
    # í•µì‹¬ ê²°ê³¼ ì¶œë ¥
    interp = report.get('interpretation', {})
    print("ğŸ“Œ í•µì‹¬ ë°œê²¬:")
    print(f"   Fear ì—­í• : {interp.get('fear_role', 'N/A')}")
    print(f"   I/E ê· í˜•: {interp.get('implicate_explicate_balance', 'N/A')}")
    print(f"   íŠ¹ì´ì  ìœ„í—˜: {interp.get('singularity_risk', 'N/A')}")
    print(f"   Fear-ì••ì¶• ìƒê´€: {interp.get('fear_compression_correlation', 0.0):.3f}")
    print()
    
    if args.open:
        import subprocess
        subprocess.run(['code', str(md_path)])


if __name__ == '__main__':
    main()
def run_analysis_now(workspace_root=None):
    """External hook for Rhythm Thinker to force analysis"""
    if workspace_root is None:
        workspace_root = get_workspace_root()
    
    analyzer = BohmAnalyzer(workspace_root)
    events = analyzer.load_recent_events(24)
    if not events: return None
    
    report = analyzer.generate_bohm_report(24)
    analyzer.save_report(report)
    return report
