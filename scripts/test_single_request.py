#!/usr/bin/env python3
"""
ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ - beomi llama-3 8B ëª¨ë¸
ëª©í‘œ: <1800ms ë‹¬ì„± í™•ì¸
"""

import asyncio
import time
import httpx


async def test_optimized_model():
    """ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ë‹¨ì¼ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
    
    client = httpx.AsyncClient(timeout=30.0)
    
    # Test 1: ìµœì í™” ì „ (max_tokens=200, temp=0.7)
    print("ğŸ§ª Test 1: ê¸°ì¡´ ì„¤ì • (max_tokens=200, temp=0.7)")
    start = time.time()
    try:
        response = await client.post(
            "http://localhost:8080/v1/chat/completions",
            json={
                "model": "yanolja_-_eeve-korean-instruct-10.8b-v1.0",
                "messages": [
                    {"role": "user", "content": "Explain AI in one sentence"}
                ],
                "max_tokens": 200,
                "temperature": 0.7
            }
        )
        latency_baseline = (time.time() - start) * 1000
        
        if response.status_code == 200:
            data = response.json()
            content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
            print(f"   âœ… Success: {latency_baseline:.0f}ms")
            print(f"   Response: {content[:100]}...")
        else:
            print(f"   âŒ HTTP {response.status_code}: {response.text}")
    except Exception as e:
        print(f"   âŒ Error: {e}")
        latency_baseline = 0
    
    # Test 2: ìµœì í™” í›„ (max_tokens=150, temp=0.5)
    print("\nğŸ§ª Test 2: ìµœì í™” ì„¤ì • (max_tokens=150, temp=0.5)")
    start = time.time()
    try:
        response = await client.post(
            "http://localhost:8080/v1/chat/completions",
            json={
                "model": "yanolja_-_eeve-korean-instruct-10.8b-v1.0",
                "messages": [
                    {"role": "user", "content": "Explain AI in one sentence"}
                ],
                "max_tokens": 150,
                "temperature": 0.5
            }
        )
        latency_optimized = (time.time() - start) * 1000
        
        if response.status_code == 200:
            data = response.json()
            content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
            print(f"   âœ… Success: {latency_optimized:.0f}ms")
            print(f"   Response: {content[:100]}...")
            
            if latency_optimized < 1800:
                print(f"   ğŸ¯ ëª©í‘œ ë‹¬ì„±! (<1800ms)")
            else:
                print(f"   âš ï¸  ëª©í‘œ ë¯¸ë‹¬ ({latency_optimized:.0f}ms > 1800ms)")
            
            # ê°œì„ ìœ¨ ê³„ì‚°
            if latency_baseline > 0:
                improvement = ((latency_baseline - latency_optimized) / latency_baseline) * 100
                print(f"\nğŸ“Š ì„±ëŠ¥ ê°œì„ : {improvement:.1f}%")
                print(f"   Before: {latency_baseline:.0f}ms â†’ After: {latency_optimized:.0f}ms")
        else:
            print(f"   âŒ HTTP {response.status_code}: {response.text}")
    except Exception as e:
        print(f"   âŒ Error: {e}")
    
    await client.aclose()


if __name__ == "__main__":
    asyncio.run(test_optimized_model())
