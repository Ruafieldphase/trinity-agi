#!/usr/bin/env python3
"""
ğŸ¤ Microphone Frequency Analyzer
ë§ˆì´í¬ë¥¼ í†µí•´ í™˜ê²½ ì£¼íŒŒìˆ˜ì™€ ìŒì„± íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ìƒíƒœë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.

ê°ì§€ ê°€ëŠ¥í•œ ì‹ í˜¸:
1. ğŸ—£ï¸ ìŒì„± íŒ¨í„´
   - ë¹ ë¥¸ ë§ ì†ë„ â†’ í¥ë¶„/ìŠ¤íŠ¸ë ˆìŠ¤
   - ëŠë¦° ë§ ì†ë„ â†’ ì§‘ì¤‘/í”¼ë¡œ
   - ë¬´ìŒ â†’ ê¹Šì€ ì§‘ì¤‘ or ë¶€ì¬
   
2. ğŸŒŠ í™˜ê²½ ì£¼íŒŒìˆ˜
   - ë°°ê²½ ì†ŒìŒ ë ˆë²¨ â†’ í™˜ê²½ ìŠ¤íŠ¸ë ˆìŠ¤
   - ì£¼ê¸°ì  íŒ¨í„´ â†’ íƒ€ì´í•‘, ë§ˆìš°ìŠ¤ í´ë¦­
   - ê°‘ì‘ìŠ¤ëŸ° ì†ŒìŒ â†’ ë°©í•´ ìš”ì†Œ
   
3. ğŸµ ì£¼íŒŒìˆ˜ ëŒ€ì—­ ë¶„ì„
   - Delta (0.5-4 Hz): ê¹Šì€ ìˆ˜ë©´/ëª…ìƒ (ê±°ì˜ ê°ì§€ ì•ˆë¨)
   - Theta (4-8 Hz): ì°½ì˜ì„±, ìƒìƒë ¥
   - Alpha (8-13 Hz): í¸ì•ˆí•œ ì§‘ì¤‘
   - Beta (13-30 Hz): í™œë°œí•œ ì‚¬ê³ 
   - Gamma (30+ Hz): ê³ ë„ ì§‘ì¤‘

Author: AGI Self-Awareness System
Date: 2025-11-10
"""
import json
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import warnings

# Suppress ALSA warnings
warnings.filterwarnings("ignore")

try:
    import numpy as np
    import sounddevice as sd
    from scipy.fft import rfft, rfftfreq
    AUDIO_AVAILABLE = True
except ImportError:
    AUDIO_AVAILABLE = False
    print("âš ï¸ Audio analysis not available. Install: pip install sounddevice numpy scipy")
    sys.exit(1)


class MicrophoneAnalyzer:
    """ë§ˆì´í¬ ì£¼íŒŒìˆ˜ ë¶„ì„ê¸°"""
    
    def __init__(self, sample_rate: int = 44100, chunk_duration: float = 2.0):
        """
        Args:
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (Hz)
            chunk_duration: ë¶„ì„ ì²­í¬ ê¸¸ì´ (ì´ˆ)
        """
        self.sample_rate = sample_rate
        self.chunk_duration = chunk_duration
        self.chunk_size = int(sample_rate * chunk_duration)
        
        # ì£¼íŒŒìˆ˜ ëŒ€ì—­ ì •ì˜ (Hz)
        self.frequency_bands = {
            'sub_bass': (20, 60),      # ì €ìŒ (í™˜ê²½ ì†ŒìŒ)
            'bass': (60, 250),         # ë² ì´ìŠ¤ (íƒ€ì´í•‘, í´ë¦­)
            'low_mid': (250, 500),     # ì €ì¤‘ìŒ (ìŒì„± ê¸°ë³¸)
            'mid': (500, 2000),        # ì¤‘ìŒ (ìŒì„± ì£¼ìš”)
            'high_mid': (2000, 4000),  # ê³ ì¤‘ìŒ (ìŒì„± ëª…ë£Œë„)
            'presence': (4000, 6000),  # í”„ë ˆì¦ŒìŠ¤ (ìŒì„± ì„ ëª…ë„)
            'brilliance': (6000, 20000) # ê³ ìŒ (ì¹˜ì°°ìŒ, í™˜ê²½)
        }
        
        # ë‘ë‡Œ ì£¼íŒŒìˆ˜ ê·¼ì‚¬ ë§¤í•‘ (ì‹¤ì œ ë‡ŒíŒŒëŠ” ì•„ë‹ˆì§€ë§Œ í™˜ê²½ íŒ¨í„´)
        self.brainwave_patterns = {
            'delta': (0.5, 4),    # ë§¤ìš° ë‚®ì€ ì£¼íŒŒìˆ˜ íŒ¨í„´
            'theta': (4, 8),      # ë‚®ì€ ì£¼íŒŒìˆ˜ íŒ¨í„´
            'alpha': (8, 13),     # ì¤‘ê°„ ì£¼íŒŒìˆ˜ íŒ¨í„´
            'beta': (13, 30),     # ë†’ì€ ì£¼íŒŒìˆ˜ íŒ¨í„´
            'gamma': (30, 100)    # ë§¤ìš° ë†’ì€ ì£¼íŒŒìˆ˜ íŒ¨í„´
        }
        
    def list_devices(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë§ˆì´í¬ ëª©ë¡"""
        print("\nğŸ¤ Available Audio Devices:")
        print(sd.query_devices())
        
    def capture_audio(self, duration: float = None) -> np.ndarray:
        """
        ë§ˆì´í¬ë¡œë¶€í„° ì˜¤ë””ì˜¤ ìº¡ì²˜
        
        Args:
            duration: ìº¡ì²˜ ì‹œê°„ (ì´ˆ). Noneì´ë©´ chunk_duration ì‚¬ìš©
            
        Returns:
            ì˜¤ë””ì˜¤ ë°ì´í„° (numpy array)
        """
        if duration is None:
            duration = self.chunk_duration
            
        print(f"ğŸ™ï¸ Recording {duration}s from microphone...")
        audio = sd.rec(
            int(duration * self.sample_rate),
            samplerate=self.sample_rate,
            channels=1,
            dtype='float64'
        )
        sd.wait()
        return audio.flatten()
        
    def analyze_frequency_spectrum(self, audio: np.ndarray) -> Dict:
        """
        ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
        
        Args:
            audio: ì˜¤ë””ì˜¤ ë°ì´í„°
            
        Returns:
            ì£¼íŒŒìˆ˜ ëŒ€ì—­ë³„ ì—ë„ˆì§€ ë¶„í¬
        """
        # FFT ê³„ì‚°
        fft_values = np.abs(rfft(audio))
        fft_freqs = rfftfreq(len(audio), 1/self.sample_rate)
        
        # ì£¼íŒŒìˆ˜ ëŒ€ì—­ë³„ ì—ë„ˆì§€
        band_energy = {}
        for band_name, (low, high) in self.frequency_bands.items():
            mask = (fft_freqs >= low) & (fft_freqs < high)
            energy = float(np.sum(fft_values[mask]))
            band_energy[band_name] = energy
            
        # ì •ê·œí™” (ì´ ì—ë„ˆì§€ ëŒ€ë¹„ ë¹„ìœ¨)
        total_energy = sum(band_energy.values())
        if total_energy > 0:
            band_ratio = {k: v/total_energy for k, v in band_energy.items()}
        else:
            band_ratio = {k: 0.0 for k in band_energy.keys()}
            
        return {
            'band_energy': band_energy,
            'band_ratio': band_ratio,
            'total_energy': total_energy,
            'dominant_freq': float(fft_freqs[np.argmax(fft_values)]) if len(fft_values) > 0 else 0.0
        }
        
    def detect_voice_activity(self, audio: np.ndarray, threshold: float = 0.02) -> Dict:
        """
        ìŒì„± í™œë™ ê°ì§€
        
        Args:
            audio: ì˜¤ë””ì˜¤ ë°ì´í„°
            threshold: ìŒì„± ê°ì§€ ì„ê³„ê°’ (RMS)
            
        Returns:
            ìŒì„± í™œë™ ì •ë³´
        """
        # RMS (Root Mean Square) ê³„ì‚°
        rms = np.sqrt(np.mean(audio**2))
        
        # ì˜êµì°¨ìœ¨ (Zero Crossing Rate) - ìŒì„± íŠ¹ì„±
        zcr = np.sum(np.abs(np.diff(np.sign(audio)))) / (2 * len(audio))
        
        # ìŒì„± ê°ì§€
        is_voice = rms > threshold and zcr > 0.01
        
        return {
            'rms': float(rms),
            'zcr': float(zcr),
            'is_voice_detected': is_voice,
            'silence_ratio': float(np.sum(np.abs(audio) < threshold) / len(audio))
        }
        
    def infer_user_state(self, spectrum: Dict, voice: Dict) -> Dict:
        """
        ì£¼íŒŒìˆ˜ ë¶„ì„ìœ¼ë¡œë¶€í„° ì‚¬ìš©ì ìƒíƒœ ì¶”ë¡ 
        
        Args:
            spectrum: ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ ê²°ê³¼
            voice: ìŒì„± í™œë™ ë¶„ì„ ê²°ê³¼
            
        Returns:
            ì¶”ë¡ ëœ ì‚¬ìš©ì ìƒíƒœ
        """
        band_ratio = spectrum['band_ratio']
        total_energy = spectrum['total_energy']
        rms = voice['rms']
        is_voice = voice['is_voice_detected']
        silence_ratio = voice['silence_ratio']
        
        # ìƒíƒœ ì¶”ë¡  ë¡œì§
        state = 'unknown'
        confidence = 0.0
        context = {}
        
        # 1. ê¹Šì€ ì§‘ì¤‘ (Deep Focus)
        if silence_ratio > 0.9 and total_energy < 100:
            state = 'deep_focus'
            confidence = 0.9
            context['description'] = 'ë§¤ìš° ì¡°ìš©í•¨ - ê¹Šì€ ì§‘ì¤‘ ìƒíƒœë¡œ ì¶”ì •'
            
        # 2. í™œë°œí•œ ì‘ì—… (Active Work)
        elif band_ratio.get('bass', 0) > 0.3 and not is_voice:
            state = 'active_work'
            confidence = 0.7
            context['description'] = 'íƒ€ì´í•‘/í´ë¦­ ì†Œë¦¬ ê°ì§€ - í™œë°œí•œ ì‘ì—… ì¤‘'
            
        # 3. ëŒ€í™”/ì„¤ëª… (Speaking)
        elif is_voice and band_ratio.get('mid', 0) > 0.2:
            state = 'speaking'
            confidence = 0.85
            context['description'] = 'ìŒì„± ê°ì§€ - ëŒ€í™” ë˜ëŠ” ì„¤ëª… ì¤‘'
            
        # 4. í™˜ê²½ ì†ŒìŒ (Environmental Noise)
        elif total_energy > 1000:
            state = 'noisy_environment'
            confidence = 0.6
            context['description'] = 'ë†’ì€ ë°°ê²½ ì†ŒìŒ - ë°©í•´ ìš”ì†Œ ì¡´ì¬'
            
        # 5. ë¶€ì¬ (Absent)
        elif silence_ratio > 0.95 and total_energy < 50:
            state = 'absent'
            confidence = 0.8
            context['description'] = 'ê±°ì˜ ë¬´ìŒ - ìë¦¬ ë¹„ì›€ ê°€ëŠ¥ì„±'
            
        # 6. ì¼ë°˜ í™œë™ (Normal Activity)
        else:
            state = 'normal_activity'
            confidence = 0.5
            context['description'] = 'ì¼ë°˜ì ì¸ í™œë™ íŒ¨í„´'
            
        return {
            'state': state,
            'confidence': confidence,
            'context': context,
            'energy_level': 'high' if total_energy > 500 else 'medium' if total_energy > 100 else 'low'
        }
        
    def analyze_once(self, save_path: Optional[str] = None) -> Dict:
        """
        í•œ ë²ˆ ë¶„ì„ ìˆ˜í–‰
        
        Args:
            save_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì„ íƒ)
            
        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        timestamp = datetime.now(timezone.utc).isoformat()
        
        # ì˜¤ë””ì˜¤ ìº¡ì²˜
        audio = self.capture_audio()
        
        # ì£¼íŒŒìˆ˜ ë¶„ì„
        spectrum = self.analyze_frequency_spectrum(audio)
        
        # ìŒì„± í™œë™ ê°ì§€
        voice = self.detect_voice_activity(audio)
        
        # ì‚¬ìš©ì ìƒíƒœ ì¶”ë¡ 
        user_state = self.infer_user_state(spectrum, voice)
        
        result = {
            'timestamp': timestamp,
            'spectrum': spectrum,
            'voice_activity': voice,
            'user_state': user_state
        }
        
        # ì €ì¥
        if save_path:
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Saved to: {save_path}")
            
        return result
        
    def monitor_continuous(self, interval: float = 10.0, duration: float = 60.0,
                          output_dir: str = "outputs/microphone"):
        """
        ì—°ì† ëª¨ë‹ˆí„°ë§
        
        Args:
            interval: ë¶„ì„ ê°„ê²© (ì´ˆ)
            duration: ì´ ëª¨ë‹ˆí„°ë§ ì‹œê°„ (ì´ˆ)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        jsonl_file = output_path / f"microphone_log_{datetime.now().strftime('%Y%m%d')}.jsonl"
        
        print(f"\nğŸ™ï¸ Starting continuous monitoring...")
        print(f"   Interval: {interval}s")
        print(f"   Duration: {duration}s")
        print(f"   Output: {jsonl_file}")
        print("\nPress Ctrl+C to stop early.\n")
        
        start_time = time.time()
        try:
            while time.time() - start_time < duration:
                # ë¶„ì„
                result = self.analyze_once()
                
                # JSONL ì €ì¥
                with open(jsonl_file, 'a', encoding='utf-8') as f:
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
                
                # ì½˜ì†” ì¶œë ¥
                state = result['user_state']['state']
                confidence = result['user_state']['confidence']
                energy = result['user_state']['energy_level']
                desc = result['user_state']['context'].get('description', '')
                
                print(f"[{datetime.now().strftime('%H:%M:%S')}] "
                      f"State: {state} (conf: {confidence:.2f}, energy: {energy})")
                print(f"           {desc}")
                
                # ëŒ€ê¸°
                time.sleep(interval)
                
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸ Monitoring stopped by user.")
            
        print(f"\nâœ… Monitoring complete. Log saved to: {jsonl_file}")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ğŸ¤ Microphone Frequency Analyzer')
    parser.add_argument('--list-devices', action='store_true',
                       help='List available audio devices')
    parser.add_argument('--once', action='store_true',
                       help='Analyze once and exit')
    parser.add_argument('--monitor', action='store_true',
                       help='Continuous monitoring mode')
    parser.add_argument('--interval', type=float, default=10.0,
                       help='Analysis interval in seconds (default: 10)')
    parser.add_argument('--duration', type=float, default=60.0,
                       help='Total monitoring duration in seconds (default: 60)')
    parser.add_argument('--output', type=str, default='outputs/microphone',
                       help='Output directory (default: outputs/microphone)')
    
    args = parser.parse_args()
    
    analyzer = MicrophoneAnalyzer()
    
    if args.list_devices:
        analyzer.list_devices()
    elif args.once:
        result = analyzer.analyze_once(
            save_path=f"{args.output}/microphone_analysis_latest.json"
        )
        print("\nğŸ“Š Analysis Result:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
    elif args.monitor:
        analyzer.monitor_continuous(
            interval=args.interval,
            duration=args.duration,
            output_dir=args.output
        )
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
