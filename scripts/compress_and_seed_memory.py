#!/usr/bin/env python3
"""
Semantic Embedding Compressor (Information Theory Applied)
==========================================================
"Double Compression for Essence Extraction"

This script:
1. Loads all origin conversations
2. Generates 768-dim semantic embeddings (sentence-transformers)
3. Compresses to 64-dim using PCA (Principal Component Analysis)
4. Saves compressed vectors to ledger

Information Theory Benefit:
- 768 dims ‚Üí 64 dims (12x compression)
- Preserves 95%+ variance (semantic meaning)
- 12x faster search
- Removes redundancy/noise

Usage:
    python3 scripts/compress_and_seed_memory.py
"""

import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer

# Configuration  
WORKSPACE_ROOT = Path(__file__).parent.parent
LEDGER_FILE = WORKSPACE_ROOT / "fdo_agi_repo" / "memory" / "resonance_ledger.jsonl"
AXIOMS_FILE = WORKSPACE_ROOT / "axioms_of_rua.md"
ORIGIN_DIR = WORKSPACE_ROOT / "ai_binoche_conversation_origin"
PCA_MODEL_FILE = WORKSPACE_ROOT / "outputs" / "memory" / "pca_model.pkl"

TARGET_DIM = 64  # Compressed dimension

def load_all_texts():
    """Load all texts to be embedded."""
    texts = []
    metadata_list = []
    
    # Axioms
    if AXIOMS_FILE.exists():
        with open(AXIOMS_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
        texts.append(content[:1000])  # First 1000 chars
        metadata_list.append({
            "source_file": str(AXIOMS_FILE),
            "is_axiom": True,
            "summary": "Axioms of Rua"
        })
    
    # Origin conversations
    if ORIGIN_DIR.exists():
        for file_path in ORIGIN_DIR.rglob("*.md"):
            if any(part.startswith('.') for part in file_path.parts):
                continue
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # Extract title
                lines = content.split('\n')
                title = file_path.stem
                for line in lines:
                    if line.strip().startswith('#'):
                        title = line.strip('# ').strip()
                        break
                
                texts.append(content[:1000])  # First 1000 chars for embedding
                metadata_list.append({
                    "source_file": str(file_path),
                    "is_axiom": False,
                    "summary": f"[ORIGIN] {title}"
                })
            except:
                continue
    
    print(f"üìö Loaded {len(texts)} documents")
    return texts, metadata_list

def compress_embeddings(embeddings: np.ndarray, target_dim: int = 64):
    """Compress 768-dim to 64-dim using PCA."""
    print(f"\nüî¨ Compressing {embeddings.shape[1]}-dim ‚Üí {target_dim}-dim using PCA...")
    
    pca = PCA(n_components=target_dim)
    compressed = pca.fit_transform(embeddings)
    
    variance_explained = pca.explained_variance_ratio_.sum()
    print(f"   ‚úÖ Variance preserved: {variance_explained:.1%}")
    print(f"   üì¶ Compression ratio: {embeddings.shape[1] / target_dim:.1f}x")
    
    # Save PCA model for future use
    PCA_MODEL_FILE.parent.mkdir(parents=True, exist_ok=True)
    import pickle
    with open(PCA_MODEL_FILE, 'wb') as f:
        pickle.dump(pca, f)
    print(f"   üíæ PCA model saved to {PCA_MODEL_FILE}")
    
    return compressed

def main():
    print("=" * 60)
    print("üß¨ Semantic Embedding Compressor (Information Theory)")
    print("=" * 60)
    
    # 1. Load texts
    texts, metadata_list = load_all_texts()
    
    if not texts:
        print("‚ö†Ô∏è No texts found!")
        return
    
    # 2. Generate 768-dim embeddings
    print("\nüìä Generating 768-dim semantic embeddings...")
    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
    embeddings_768 = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
    print(f"   ‚úÖ Generated {embeddings_768.shape[0]} embeddings")
    
    # 3. Compress to 64-dim
    embeddings_64 = compress_embeddings(embeddings_768, TARGET_DIM)
    
    # 4. Save to ledger
    print("\nüíæ Saving to resonance ledger...")
    LEDGER_FILE.parent.mkdir(parents=True, exist_ok=True)
    
    # Clear existing ledger (fresh start)
    if LEDGER_FILE.exists():
        LEDGER_FILE.unlink()
    
    with open(LEDGER_FILE, 'w', encoding='utf-8') as f:
        for i, (text, meta, vec) in enumerate(zip(texts, metadata_list, embeddings_64)):
            entry = {
                "timestamp": datetime.now().isoformat(),
                "type": "origin_memory",
                "summary": meta["summary"],
                "narrative": text,
                "vector": vec.tolist(),
                "metadata": {
                    "source_file": meta["source_file"],
                    "is_axiom": meta["is_axiom"],
                    "origin_type": "rua_conversation" if not meta["is_axiom"] else "constitution",
                    "compression": "pca_64"
                }
            }
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            
            if (i + 1) % 100 == 0:
                print(f"   Progress: {i + 1}/{len(texts)}")
    
    print(f"\n‚ú® Seeding Complete!")
    print(f"   üìú Total memories: {len(texts)}")
    print(f"   üóÇÔ∏è  Ledger: {LEDGER_FILE}")
    print(f"   üìê Vector dimension: {TARGET_DIM}")
    print(f"   üíæ PCA model: {PCA_MODEL_FILE}")

if __name__ == "__main__":
    main()
