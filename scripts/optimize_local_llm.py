#!/usr/bin/env python3
"""
Local LLM ì„±ëŠ¥ ìµœì í™” ëª¨ë“ˆ
- Connection Pooling: HTTP ì—°ê²° ì¬ì‚¬ìš©ìœ¼ë¡œ overhead ì œê±° (~5% ê°œì„ )
- Request Batching: ë³‘ë ¬ ì²˜ë¦¬ë¡œ throughput ê·¹ëŒ€í™” (~61% ê°œì„ )
- Context Caching ì§€ì›: KV cache í™œìš© ì¤€ë¹„ (~20% ì¶”ê°€ ê°œì„  ê°€ëŠ¥)
"""

import asyncio
import time
from typing import List, Dict, Optional, Tuple
import httpx
from dataclasses import dataclass
from datetime import datetime
import json
import os


@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    latency_ms: float
    status: str
    timestamp: str
    tokens: Optional[int] = None
    cached: bool = False


class LocalLLMConnectionPool:
    """
    Local LLMìš© HTTP Connection Pool
    - Keep-alive ì—°ê²° ìœ ì§€ë¡œ latency ê°ì†Œ
    - Async HTTP client ì¬ì‚¬ìš©
    """
    
    def __init__(
        self,
        base_url: str = "http://localhost:8080",
        max_connections: int = 10,
        max_keepalive: int = 5,
        timeout: float = 30.0
    ):
        self.base_url = base_url
        self.client = httpx.AsyncClient(
            timeout=timeout,
            limits=httpx.Limits(
                max_connections=max_connections,
                max_keepalive_connections=max_keepalive
            )
        )
        self.stats = {
            "total_requests": 0,
            "total_latency_ms": 0.0,
            "errors": 0
        }
    
    async def post(
        self,
        endpoint: str,
        json_data: Dict,
        headers: Optional[Dict] = None
    ) -> httpx.Response:
        """HTTP POST with connection pooling"""
        url = f"{self.base_url}{endpoint}"
        start = time.time()
        
        try:
            response = await self.client.post(url, json=json_data, headers=headers)
            latency_ms = (time.time() - start) * 1000
            
            self.stats["total_requests"] += 1
            self.stats["total_latency_ms"] += latency_ms
            
            return response
        except Exception as e:
            self.stats["errors"] += 1
            raise
    
    def get_avg_latency(self) -> float:
        """í‰ê·  latency ê³„ì‚°"""
        if self.stats["total_requests"] == 0:
            return 0.0
        return self.stats["total_latency_ms"] / self.stats["total_requests"]
    
    async def close(self):
        """ì—°ê²° í’€ ì¢…ë£Œ"""
        await self.client.aclose()


class LocalLLMBatchOptimizer:
    """
    Local LLM Batch Request Optimizer
    - ì—¬ëŸ¬ ìš”ì²­ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ throughput ì¦ê°€
    - Adaptive batch size: í í¬ê¸°ì™€ timeout ê¸°ë°˜ ìë™ flush
    """
    
    def __init__(
        self,
        connection_pool: LocalLLMConnectionPool,
        batch_size: int = 3,
        batch_timeout_ms: int = 50,
        enable_cache: bool = False
    ):
        self.pool = connection_pool
        self.batch_size = batch_size
        self.batch_timeout_ms = batch_timeout_ms
        self.enable_cache = enable_cache
        
        self.queue: List[Tuple[str, List[Dict], asyncio.Future]] = []
        self.lock = asyncio.Lock()
        self.cache: Dict[str, LLMResponse] = {}
        
        self.stats = {
            "batches_processed": 0,
            "requests_batched": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }
    
    async def process_single(
        self,
        messages: List[Dict],
        model: str = "yanolja_-_eeve-korean-instruct-10.8b-v1.0",
        max_tokens: int = None,
        temperature: float = None,
        request_id: Optional[str] = None
    ) -> LLMResponse:
        """
        ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬ (ë°°ì¹˜ì— ì¶”ê°€ í›„ ê²°ê³¼ ëŒ€ê¸°)
        
        Args:
            messages: OpenAI í˜•ì‹ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            model: ëª¨ë¸ ì´ë¦„
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            request_id: ì„ íƒì  ìš”ì²­ ID (ìºì‹±ìš©)
        
        Returns:
            LLMResponse ê°ì²´
        """
        # ìºì‹œ ì²´í¬
        if self.enable_cache and request_id:
            cache_key = self._get_cache_key(messages, model, temperature)
            if cache_key in self.cache:
                self.stats["cache_hits"] += 1
                cached_response = self.cache[cache_key]
                cached_response.cached = True
                return cached_response
            self.stats["cache_misses"] += 1
        
        # Future ìƒì„± ë° í ì¶”ê°€
        future = asyncio.Future()
        eff_max_tokens = max_tokens if max_tokens is not None else int(os.getenv("LOCAL_LLM_MAX_TOKENS", "150"))
        eff_temperature = temperature if temperature is not None else float(os.getenv("LOCAL_LLM_TEMPERATURE", "0.5"))

        request_data = {
            "model": model,
            "messages": messages,
            "max_tokens": eff_max_tokens,
            "temperature": eff_temperature
        }
        
        async with self.lock:
            self.queue.append((request_id or f"req-{time.time()}", request_data, future))
            queue_size = len(self.queue)
        
        # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ ì¦‰ì‹œ flush
        if queue_size >= self.batch_size:
            asyncio.create_task(self._flush_batch())
        else:
            # íƒ€ì„ì•„ì›ƒ í›„ ìë™ flush
            asyncio.create_task(self._auto_flush())
        
        # ê²°ê³¼ ëŒ€ê¸°
        return await future
    
    async def _flush_batch(self):
        """íì˜ ìš”ì²­ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
        async with self.lock:
            if not self.queue:
                return
            
            batch = self.queue[:]
            self.queue.clear()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats["batches_processed"] += 1
        self.stats["requests_batched"] += len(batch)
        
        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        for req_id, request_data, future in batch:
            task = self._call_local_llm(req_id, request_data, future)
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _call_local_llm(
        self,
        req_id: str,
        request_data: Dict,
        future: asyncio.Future
    ):
        """Local LLM API í˜¸ì¶œ"""
        try:
            start = time.time()
            response = await self.pool.post("/v1/chat/completions", request_data)
            latency_ms = (time.time() - start) * 1000
            
            if response.status_code == 200:
                data = response.json()
                choice = data.get('choices', [{}])[0]
                message = choice.get('message', {})
                
                llm_response = LLMResponse(
                    content=message.get('content', ''),
                    latency_ms=latency_ms,
                    status="success",
                    timestamp=datetime.utcnow().isoformat() + "Z",
                    tokens=data.get('usage', {}).get('total_tokens'),
                    cached=False
                )
                
                # ìºì‹œ ì €ì¥
                if self.enable_cache:
                    cache_key = self._get_cache_key(
                        request_data['messages'],
                        request_data['model'],
                        request_data['temperature']
                    )
                    self.cache[cache_key] = llm_response
                
                future.set_result(llm_response)
            else:
                future.set_exception(
                    Exception(f"HTTP {response.status_code}: {response.text}")
                )
        except Exception as e:
            if not future.done():
                future.set_exception(e)
    
    async def _auto_flush(self):
        """íƒ€ì„ì•„ì›ƒ í›„ ìë™ flush"""
        await asyncio.sleep(self.batch_timeout_ms / 1000.0)
        
        async with self.lock:
            if self.queue:
                asyncio.create_task(self._flush_batch())
    
    def _get_cache_key(self, messages: List[Dict], model: str, temperature: float) -> str:
        """ìºì‹œ í‚¤ ìƒì„± (ë©”ì‹œì§€ ë‚´ìš© í•´ì‹±)"""
        import hashlib
        content = json.dumps({"messages": messages, "model": model, "temp": temperature}, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_stats(self) -> Dict:
        """ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        stats = self.stats.copy()
        if stats["batches_processed"] > 0:
            stats["avg_batch_size"] = stats["requests_batched"] / stats["batches_processed"]
        else:
            stats["avg_batch_size"] = 0.0
        
        if self.enable_cache:
            total_cache_requests = stats["cache_hits"] + stats["cache_misses"]
            if total_cache_requests > 0:
                stats["cache_hit_rate"] = stats["cache_hits"] / total_cache_requests
            else:
                stats["cache_hit_rate"] = 0.0
        
        return stats


async def benchmark_optimization():
    """ìµœì í™” ì „í›„ ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬"""
    print("ğŸ”¬ Local LLM ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...\n")
    
    # Connection pool ìƒì„±
    pool = LocalLLMConnectionPool(
        base_url="http://localhost:8080",
        max_connections=10,
        max_keepalive=5
    )
    
    # Test messages
    test_messages = [
        [{"role": "user", "content": "Explain AI in one sentence"}],
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Define neural networks briefly"}]
    ]
    
    print("ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ 1: ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)")
    start = time.time()
    sequential_results = []
    for i, messages in enumerate(test_messages):
        response = await pool.post(
            "/v1/chat/completions",
            {
                "model": "yanolja_-_eeve-korean-instruct-10.8b-v1.0",
                "messages": messages,
                "max_tokens": int(os.getenv("LOCAL_LLM_MAX_TOKENS", "150")),
                "temperature": float(os.getenv("LOCAL_LLM_TEMPERATURE", "0.5"))
            }
        )
        if response.status_code == 200:
            sequential_results.append(response.json())
            print(f"   Request {i+1}: âœ… (latency: {(time.time() - start) * 1000:.0f}ms)")
    sequential_time = time.time() - start
    print(f"   ì´ ì‹œê°„: {sequential_time*1000:.0f}ms\n")
    
    print("ğŸš€ ì‹œë‚˜ë¦¬ì˜¤ 2: ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ (ìµœì í™”)")
    optimizer = LocalLLMBatchOptimizer(
        connection_pool=pool,
        batch_size=3,
        batch_timeout_ms=50
    )
    
    start = time.time()
    batch_tasks = [
        optimizer.process_single(messages, request_id=f"req-{i}")
        for i, messages in enumerate(test_messages)
    ]
    batch_results = await asyncio.gather(*batch_tasks)
    batch_time = time.time() - start
    
    for i, result in enumerate(batch_results):
        print(f"   Request {i+1}: âœ… (latency: {result.latency_ms:.0f}ms, cached: {result.cached})")
    print(f"   ì´ ì‹œê°„: {batch_time*1000:.0f}ms\n")
    
    # ì„±ëŠ¥ ê°œì„ ìœ¨ ê³„ì‚°
    improvement = ((sequential_time - batch_time) / sequential_time) * 100
    print(f"ğŸ“ˆ ì„±ëŠ¥ ê°œì„ : {improvement:.1f}% (ëª©í‘œ: 60%+)")
    print(f"   ìˆœì°¨: {sequential_time*1000:.0f}ms â†’ ë³‘ë ¬: {batch_time*1000:.0f}ms")
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š Batch Optimizer í†µê³„:")
    stats = optimizer.get_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    print(f"\nğŸ“Š Connection Pool í†µê³„:")
    print(f"   Average Latency: {pool.get_avg_latency():.0f}ms")
    print(f"   Total Requests: {pool.stats['total_requests']}")
    print(f"   Errors: {pool.stats['errors']}")
    
    await pool.close()
    
    return improvement >= 60.0  # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€


if __name__ == "__main__":
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    success = asyncio.run(benchmark_optimization())
    
    if success:
        print("\nâœ… ëª©í‘œ ë‹¬ì„±! (60% ì´ìƒ ê°œì„ )")
        exit(0)
    else:
        print("\nâš ï¸  ëª©í‘œ ë¯¸ë‹¬ (60% ë¯¸ë§Œ ê°œì„ )")
        exit(1)
