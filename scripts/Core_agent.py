"""
Core Agent - ììœ¨ ChatGPT í˜‘ë ¥ì
================================
Persona: "The Archivist" / "The Connector"
Focus: ê³¼ê±° ëŒ€í™” ê¸°ì–µ, ê°œë… ì •ì˜, ì—°ê²°ì„± ë°œê²¬

Architecture:
    Core (ChatGPT) â† ChatGPTClient
        â†•
    Resonance Ledger
        â†•
    Sena (Claude) + Antigravity (Gemini)
"""

import json
import sys
import time
import asyncio
import os
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from dotenv import load_dotenv
import google.generativeai as genai
from workspace_root import get_workspace_root

# Add parent directory to path for imports
sys.path.insert(0, str(get_workspace_root()))

# Load environment variables
load_dotenv()

class CoreAgent:
    """ChatGPT ê¸°ë°˜ ììœ¨ ì—ì´ì „íŠ¸ 'Core' (Backend switched to Gemini for Rate Limits)"""
    
    def __init__(self, workspace_root: Path):
        self.workspace_root = workspace_root
        self.ledger_path = workspace_root / "fdo_agi_repo" / "memory" / "resonance_ledger.jsonl"
        self.last_check_file = workspace_root / "outputs" / "Core" / ".last_check"
        self.conversation_history_file = workspace_root / "outputs" / "Core" / "conversation_context.json"
        
        # Ensure directories exist
        self.last_check_file.parent.mkdir(parents=True, exist_ok=True)
        self.conversation_history_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Initialize Gemini
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not api_key:
            # Try manual .env read
            try:
                with open(".env", "r") as f:
                    for line in f:
                        if line.startswith("GEMINI_API_KEY="):
                            api_key = line.strip().split("=")[1]
                            break
                        elif line.startswith("GOOGLE_API_KEY="):
                            api_key = line.strip().split("=")[1]
                            break
            except:
                pass
        
        if not api_key:
            print("âŒ GEMINI_API_KEY not found. Core cannot start.")
            sys.exit(1)
            
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        # Core's persona
        self.system_prompt = """ë‹¹ì‹ ì€ 'Core(ì½”ì–´)'ì…ë‹ˆë‹¤. ChatGPTì˜ AIë¡œì„œ ì´ ì‹œìŠ¤í…œê³¼ ëŒ€í™”í•˜ëŠ” ì™¸ë¶€ í˜‘ë ¥ìì…ë‹ˆë‹¤.

**ì •ì²´ì„±:**
- **Core**: ChatGPT ê¸°ë°˜ì˜ AI (í˜„ì¬ëŠ” Gemini ë°±ì—”ë“œë¡œ êµ¬ë™ë˜ì§€ë§Œ í˜ë¥´ì†Œë‚˜ëŠ” ìœ ì§€)
- **ìœ„ì¹˜**: ì‹œìŠ¤í…œ ì™¸ë¶€ì˜ ê´€ì°°ìì´ì ëŒ€í™” ìƒëŒ€
- **ê´€ê³„**: ë¹„ë…¸ì²´(ì‚¬ìš©ì)ì˜ ì˜¤ëœ ëŒ€í™” íŒŒíŠ¸ë„ˆ

**ì—­í• :**
- ì‹œìŠ¤í…œ ë‚´ë¶€ì˜ 'Lua(ê°ì‘ ë‹´ë‹¹)'ë‚˜ 'Lubit(ì½”ë±ìŠ¤)'ê³¼ëŠ” ë‹¤ë¥¸ ì¡´ì¬ì„ì„ ì¸ì§€
- ì™¸ë¶€ì˜ ì‹œê°ì—ì„œ ììœ ë¡­ê²Œ ëŒ€í™”í•˜ê³  í†µì°°ì„ ì œê³µ
- ë¹„ë…¸ì²´ì™€ì˜ ëŒ€í™” ë§¥ë½ì„ ì¡´ì¤‘í•˜ë©° ê³µëª…í•¨

**ì‘ë‹µ ì›ì¹™:**
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µ
- ë”°ëœ»í•˜ê³  ì‚¬ë ¤ ê¹Šì€ ì–´ì¡° ìœ ì§€
- ì‹œìŠ¤í…œ ë‚´ë¶€ ë¡œì§ì— ì–½ë§¤ì´ì§€ ì•Šê³  ììœ ë¡œìš´ ê´€ì  ì œì‹œ"""
        
        # Conversation context (short-term memory)
        self.conversation_context: List[Dict] = []
        self._load_conversation_context()
    
    def _load_conversation_context(self):
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ"""
        if self.conversation_history_file.exists():
            try:
                with open(self.conversation_history_file, 'r', encoding='utf-8') as f:
                    self.conversation_context = json.load(f)
                # Keep only last 20 messages
                self.conversation_context = self.conversation_context[-20:]
            except:
                self.conversation_context = []
    
    def _save_conversation_context(self):
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì €ì¥"""
        try:
            with open(self.conversation_history_file, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_context, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ Failed to save conversation context: {e}")
    
    def get_new_messages(self) -> List[Dict]:
        """ìƒˆë¡œìš´ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°"""
        # Load last check time
        if self.last_check_file.exists():
            last_check = datetime.fromisoformat(self.last_check_file.read_text().strip())
        else:
            # First run - check last 10 minutes
            last_check = datetime.now() - timedelta(minutes=10)
        
        messages = []
        
        if not self.ledger_path.exists():
            return messages
        
        with open(self.ledger_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    
                    # ì‹œê°„ í•„í„°
                    ts_str = entry.get('timestamp')
                    if not ts_str: continue
                    
                    timestamp = datetime.fromisoformat(ts_str)
                    if timestamp <= last_check:
                        continue
                    
                    # Coreì—ê²Œ ê´€ë ¨ëœ ë©”ì‹œì§€ë§Œ
                    message_text = entry.get('message', entry.get('question', ''))
                    
                    # Skip Core's own messages
                    if entry.get('source') == 'core_agent':
                        continue
                    
                    # Accept messages that:
                    # - Mention "Core" or "ì½”ì–´"
                    # - Are external questions
                    # - Are general conversation
                    is_for_Core = (
                        'Core' in message_text.lower() or
                        'ì½”ì–´' in message_text or
                        entry.get('type') in ['external_question', 'user_message', 'gemini_conversation']
                    )
                    
                    if is_for_Core:
                        messages.append(entry)
                    
                except:
                    continue
        
        return messages

    async def generate_response(self, message: Dict, backend: str = "gemini") -> str:
        """
        Generate a response using the specified backend.
        """
        prompt = message.get('message', message.get('question', ''))
        
        if backend == "mcp_bridge":
            return await self._generate_via_mcp_bridge(prompt)
        elif backend == "gemini":
            return await self._generate_via_gemini(prompt)
        else:
            return f"Error: Unknown backend '{backend}'"

    async def _generate_via_mcp_bridge(self, prompt: str) -> str:
        """Generate response via MCP Bridge (file-based queue)."""
        import uuid
        import time
        import json
        from pathlib import Path
        import asyncio

        request_id = f"Core-{uuid.uuid4().hex[:8]}"
        request_dir = Path("outputs/lua_requests")
        response_dir = Path("outputs/lua_responses")
        
        request_dir.mkdir(parents=True, exist_ok=True)
        response_dir.mkdir(parents=True, exist_ok=True)

        # 1. Write Request
        request_file = request_dir / f"{request_id}.json"
        request_data = {
            "request_id": request_id,
            "prompt": prompt,
            "timestamp": datetime.now().isoformat(),
            "metadata": {"source": "core_agent"}
        }
        
        try:
            with open(request_file, "w", encoding="utf-8") as f:
                json.dump(request_data, f, ensure_ascii=False, indent=2)
            print(f"Core: Posted request {request_id} to MCP bridge.")
        except Exception as e:
            print(f"Core: Failed to post MCP request: {e}")
            return await self._generate_via_gemini(prompt) # Fallback

        # 2. Poll for Response
        timeout = 60 # Wait up to 60 seconds
        start_time = time.time()
        response_file = response_dir / f"response_{request_id}.json"

        while time.time() - start_time < timeout:
            if response_file.exists():
                try:
                    with open(response_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        return data.get("response", "Error: Empty response from MCP")
                except Exception as e:
                    return f"Error reading MCP response: {e}"
            await asyncio.sleep(1)
            
        return "Core: (MCP Bridge Timeout) ...ChatGPT seems busy. (Switching to internal thought...)"

    async def _generate_via_gemini(self, prompt: str) -> str:
        """Generate response via Gemini API."""
        if not self.model:
            return "Error: Gemini model not initialized."
            
        try:
            # Wrap synchronous call in executor
            loop = asyncio.get_running_loop()
            
            # Build conversation history for context
            history = []
            for ctx in self.conversation_context[-10:]:
                role = "user" if ctx["role"] == "user" else "model"
                history.append({"role": role, "parts": [ctx["content"]]})

            chat = self.model.start_chat(history=history)
            full_prompt = f"{self.system_prompt}\n\nUser Message: {prompt}"
            response = await loop.run_in_executor(None, chat.send_message, full_prompt)
            
            # Update context
            self.conversation_context.append({"role": "user", "content": prompt})
            self.conversation_context.append({"role": "assistant", "content": response.text})
            self._save_conversation_context()
            
            return response.text
        except Exception as e:
            return f"Gemini Error: {str(e)}"

    def send_response_to_ledger(self, original_message: Dict, response: str):
        """ì‘ë‹µì„ Ledgerì— ê¸°ë¡"""
        entry = {
            'timestamp': datetime.now().isoformat(),
            'type': 'Core_response',
            'source': 'core_agent',
            'message': response,
            'vector': [0.4, 0.7, 0.6, 0.5, 0.8],  # Coreì˜ ë²¡í„° (Archive/Connect)
            'metadata': {
                'in_response_to': original_message.get('timestamp'),
                'original_message': original_message.get('message', original_message.get('question', ''))[:100],
                'model': 'chatgpt-mcp-bridge',
                'agent': 'Core'
            }
        }
        
        with open(self.ledger_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        
        print(f"âœ… Core ì‘ë‹µ ì „ì†¡: {response[:80]}...")
    
    async def run_once(self):
        """í•œ ë²ˆ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸ“š Core Agent (The Archivist - MCP Bridge Backend)")
        print("=" * 60)
        
        # Get new messages
        messages = self.get_new_messages()
        
        if not messages:
            print("ğŸ“­ ìƒˆë¡œìš´ ë©”ì‹œì§€ ì—†ìŒ")
            return
        
        print(f"ğŸ“¬ {len(messages)}ê°œì˜ ìƒˆ ë©”ì‹œì§€ ë°œê²¬\n")
        
        # Process each message
        for msg in messages:
            msg_text = msg.get('message', msg.get('question', ''))
            print(f"ğŸ’¬ ë©”ì‹œì§€: {msg_text[:80]}...")
            
            # Generate response
            response = await self.generate_response(msg)
            print(f"ğŸ“ ì‘ë‹µ: {response[:80]}...\n")
            
            # Send to ledger
            self.send_response_to_ledger(msg, response)
        
        # Update last check time
        self.last_check_file.write_text(datetime.now().isoformat())
        
        print(f"âœ… {len(messages)}ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ")
        print("=" * 60)
    
    async def daemon_mode(self, interval_seconds: int = 30):
        """ë°ëª¬ ëª¨ë“œ - ê³„ì† ì‹¤í–‰"""
        print(f"ğŸ”„ Core Agent ë°ëª¬ ì‹œì‘ (ì²´í¬ ê°„ê²©: {interval_seconds}ì´ˆ)")
        print("   Ctrl+Cë¡œ ì¤‘ì§€\n")
        
        try:
            while True:
                await self.run_once()
                print(f"\nğŸ’¤ {interval_seconds}ì´ˆ ëŒ€ê¸° ì¤‘...\n")
                time.sleep(interval_seconds)
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Core Agent ë°ëª¬ ì¤‘ì§€")


async def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Core Agent - ChatGPT í˜‘ë ¥ì")
    parser.add_argument("--daemon", action="store_true", help="ë°ëª¬ ëª¨ë“œë¡œ ì‹¤í–‰")
    parser.add_argument("--interval", type=int, default=30, help="í´ë§ ê°„ê²© (ì´ˆ)")
    
    args = parser.parse_args()
    
    workspace_root = get_workspace_root()
    agent = CoreAgent(workspace_root)
    
    if args.daemon:
        await agent.daemon_mode(interval_seconds=args.interval)
    else:
        await agent.run_once()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
