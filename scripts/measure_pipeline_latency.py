#!/usr/bin/env python3
"""
Pipeline Latency ì‹¤ì¸¡ ë¶„ì„

Resonance Ledgerì—ì„œ ìµœê·¼ ì‘ì—…ì˜ ë ˆì´í„´ì‹œë¥¼ ë¶„ì„í•˜ì—¬
ì‹¤ì œ ë³‘ëª© ì§€ì ì„ ëª…í™•íˆ íŒŒì•…í•©ë‹ˆë‹¤.
"""
import json
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict


def analyze_pipeline_latency(hours: int = 1):
    """ìµœê·¼ Nì‹œê°„ ë™ì•ˆì˜ Pipeline ë ˆì´í„´ì‹œ ë¶„ì„"""
    ledger_path = Path(__file__).parent.parent / "fdo_agi_repo" / "memory" / "resonance_ledger.jsonl"
    
    if not ledger_path.exists():
        print(f"âŒ Ledger not found: {ledger_path}")
        return
    
    # ì‹œê°„ í•„í„°
    cutoff = datetime.now() - timedelta(hours=hours)
    
    # ì‘ì—…ë³„ íƒ€ì´ë° ì €ì¥
    tasks = defaultdict(lambda: {
        "thesis_dur": None,
        "antithesis_dur": None,
        "synthesis_dur": None,
        "total_dur": None,
        "cache_hits": [],
        "cache_misses": []
    })
    
    # Ledger íŒŒì‹±
    with open(ledger_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                event = json.loads(line)
            except json.JSONDecodeError:
                continue
            
            # ì‹œê°„ í•„í„°
            if "timestamp" in event:
                try:
                    event_time = datetime.fromisoformat(event["timestamp"].replace("Z", "+00:00"))
                    if event_time < cutoff:
                        continue
                except Exception:
                    pass
            
            task_id = event.get("task_id")
            if not task_id:
                continue
            
            # Duration ì¶”ì¶œ
            event_type = event.get("event", "")
            duration = event.get("duration_sec")
            
            if duration is not None:
                if event_type == "thesis_end":
                    tasks[task_id]["thesis_dur"] = duration
                elif event_type == "antithesis_end":
                    tasks[task_id]["antithesis_dur"] = duration
                elif event_type == "synthesis_end":
                    tasks[task_id]["synthesis_dur"] = duration
                elif event_type == "total_latency":
                    tasks[task_id]["total_dur"] = duration
            
            # Cache ì´ë²¤íŠ¸
            if "_cache_hit" in event_type:
                tasks[task_id]["cache_hits"].append(event_type.replace("_cache_hit", ""))
            elif "_cache_miss" in event_type:
                tasks[task_id]["cache_misses"].append(event_type.replace("_cache_miss", ""))
    
    if not tasks:
        print(f"âš ï¸  No tasks found in last {hours} hour(s)")
        return
    
    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*70}")
    print(f"Pipeline Latency Analysis (Last {hours} hour(s))")
    print(f"{'='*70}\n")
    
    total_thesis = []
    total_antithesis = []
    total_synthesis = []
    total_pipeline = []
    
    for task_id, data in sorted(tasks.items(), key=lambda x: x[0])[-5:]:  # ìµœê·¼ 5ê°œë§Œ
        print(f"Task: {task_id}")
        
        if data["thesis_dur"] is not None:
            print(f"  Thesis:     {data['thesis_dur']:.2f}s")
            total_thesis.append(data["thesis_dur"])
        
        if data["antithesis_dur"] is not None:
            print(f"  Antithesis: {data['antithesis_dur']:.2f}s")
            total_antithesis.append(data["antithesis_dur"])
        
        if data["synthesis_dur"] is not None:
            print(f"  Synthesis:  {data['synthesis_dur']:.2f}s")
            total_synthesis.append(data["synthesis_dur"])
        
        if data["total_dur"] is not None:
            print(f"  Total:      {data['total_dur']:.2f}s")
            total_pipeline.append(data["total_dur"])
        
        if data["cache_hits"]:
            print(f"  âœ… Cache Hits: {', '.join(data['cache_hits'])}")
        if data["cache_misses"]:
            print(f"  âŒ Cache Misses: {', '.join(data['cache_misses'])}")
        
        print()
    
    # í†µê³„ ìš”ì•½
    if total_thesis:
        avg_thesis = sum(total_thesis) / len(total_thesis)
        print(f"Avg Thesis:     {avg_thesis:.2f}s")
    
    if total_antithesis:
        avg_antithesis = sum(total_antithesis) / len(total_antithesis)
        print(f"Avg Antithesis: {avg_antithesis:.2f}s")
    
    if total_synthesis:
        avg_synthesis = sum(total_synthesis) / len(total_synthesis)
        print(f"Avg Synthesis:  {avg_synthesis:.2f}s")
    
    if total_pipeline:
        avg_pipeline = sum(total_pipeline) / len(total_pipeline)
        print(f"\nğŸ¯ Avg Total Pipeline: {avg_pipeline:.2f}s")
        
        if total_thesis and total_antithesis and total_synthesis:
            avg_sum = avg_thesis + avg_antithesis + avg_synthesis
            overhead = avg_pipeline - avg_sum
            print(f"   (Overhead: {overhead:.2f}s = {overhead/avg_pipeline*100:.1f}%)")
    
    print(f"\n{'='*70}\n")
    
    # ë³‘ëª© íŒë‹¨
    if total_thesis and total_antithesis and total_synthesis:
        durations = [
            ("Thesis", avg_thesis),
            ("Antithesis", avg_antithesis),
            ("Synthesis", avg_synthesis)
        ]
        durations.sort(key=lambda x: x[1], reverse=True)
        
        print("ğŸ” Bottleneck Analysis:")
        for i, (name, dur) in enumerate(durations, 1):
            pct = dur / (avg_thesis + avg_antithesis + avg_synthesis) * 100
            print(f"  {i}. {name}: {dur:.2f}s ({pct:.1f}%)")
        
        print(f"\nğŸ’¡ Optimization Opportunity:")
        if all(d[1] > 1.5 for d in durations):
            print("   All phases are slow (>1.5s) â†’ LLM API latency is the bottleneck")
            print("   Consider: Parallel execution, caching, or faster models")
        else:
            print(f"   {durations[0][0]} is the slowest â†’ Focus optimization there")


if __name__ == "__main__":
    import sys
    hours = int(sys.argv[1]) if len(sys.argv) > 1 else 1
    analyze_pipeline_latency(hours)
