# 성능 분석: 현재 시스템 vs VS Code CLI (Claude/GPT Codex)

**작성일**: 2025-11-03  
**분석 대상**: 작업 속도 및 결과물 품질 비교

---

## 📊 핵심 문제

**현상**:

- 현재 시스템이 VS Code의 Claude CLI/GPT Codex보다 **현격하게 느림**
- 결과물의 **품질이 낮음**

**질문**:

- 시스템 미성숙 문제인가?
- 구조적/설계적 결함인가?

---

## 🏗️ 아키텍처 비교

### VS Code CLI (Claude/GPT Codex)

```
사용자 입력
    ↓
VS Code Extension (직접 호출)
    ↓
LLM API (Claude/OpenAI)
    ↓ (스트리밍)
실시간 응답 표시
```

**특징**:

- ✅ **레이턴시**: ~100-500ms (API 호출만)
- ✅ **스트리밍**: 실시간 토큰 표시
- ✅ **오버헤드**: 최소 (직접 통신)
- ✅ **단순성**: 2-3 레이어
- ✅ **특화 프롬프트**: 코딩에 최적화

### 현재 시스템 (AGI Framework)

```
사용자 입력
    ↓
VS Code / Terminal
    ↓
Task Queue Server (8091)
    ↓ (폴링: 0.3-0.5초 간격)
RPA Worker
    ↓
Pipeline 처리
    ├─ 프롬프트 생성
    ├─ Resonance 체크
    ├─ BQI 판단
    └─ 컨텍스트 로드
    ↓
LLM API (Vertex AI/OpenAI)
    ↓
후처리
    ├─ 레조넌스 기록
    ├─ 모니터링 로그
    ├─ 캐시 저장
    └─ 결과 검증
    ↓
결과 반환 (큐 → 클라이언트)
```

**특징**:

- ❌ **레이턴시**: ~2-5초 (큐잉 + 처리)
- ❌ **스트리밍**: 없음 (배치 처리)
- ❌ **오버헤드**: 높음 (6-8 레이어)
- ❌ **복잡성**: 10+ 컴포넌트
- ⚠️ **범용 프롬프트**: 특화되지 않음

---

## 🐌 레이턴시 분석

### 병목 지점 (추정치)

| 단계 | 시간 | 누적 | 설명 |
|------|------|------|------|
| 큐 대기 | 0.3-0.5초 | 0.5초 | Worker 폴링 간격 |
| 프롬프트 생성 | 0.1-0.2초 | 0.7초 | 컨텍스트 수집, 템플릿 |
| Resonance 체크 | 0.1-0.3초 | 1.0초 | 레저 읽기, 정책 평가 |
| LLM API 호출 | 1-3초 | 4.0초 | 실제 추론 (최대 부분) |
| 후처리 | 0.2-0.5초 | 4.5초 | 로깅, 저장, 검증 |
| 결과 반환 | 0.1-0.2초 | 4.7초 | 큐에서 클라이언트로 |

**총 레이턴시**: **~4.7초** (VS Code CLI의 **5-10배**)

### Worker 폴링 간격 영향

현재 설정:

```python
# rpa_worker.py
--interval 0.3  # 300ms마다 큐 확인
```

문제:

- 최악의 경우 요청 후 **300ms 대기** (큐에 들어가자마자 Worker가 확인하지 못함)
- 평균 **150ms 오버헤드**

---

## 🎯 품질 문제 분석

### 1. 프롬프트 품질

**VS Code CLI**:

- 코딩 작업에 최적화된 system prompt
- 컨텍스트가 명확하고 간결
- 단일 목적 (코드 생성/수정)

**현재 시스템**:

- 범용 프롬프트 템플릿
- 과도한 컨텍스트 (불필요한 정보 포함)
- 다중 목적 (코딩, 분석, 모니터링 등)

### 2. 컨텍스트 전달

**VS Code CLI**:

- 현재 파일, 선택 영역만 전달
- 최소한의 workspace 정보
- 빠르고 정확

**현재 시스템**:

- 전체 레포지토리 스캔 가능성
- 과도한 메타데이터
- 느리고 노이즈가 많음

### 3. 응답 처리

**VS Code CLI**:

- 스트리밍으로 즉시 표시
- 사용자가 중간에 취소 가능
- 실시간 피드백

**현재 시스템**:

- 배치 처리 (전체 응답 대기)
- 취소 불가능
- 피드백 지연

---

## 🏛️ 구조적 문제

### 1. 과도한 추상화

**문제점**:

- Task Queue → Worker → Pipeline → Handler → Executor
- 각 레이어마다 직렬화/역직렬화
- 디버깅 어려움

**해결책**:

- Direct Execution Mode (큐 우회)
- 간소화된 파이프라인

### 2. 불필요한 중간 레이어

**현재**:

```
Input → Queue → Worker → Resonance → BQI → LLM → Monitor → Log → Output
```

**필수**:

```
Input → LLM → Output
```

**선택적** (성능 모드에서 스킵):

- Resonance 체크
- BQI 판단
- 상세 모니터링
- 캐시 저장

### 3. 동기화 오버헤드

**문제**:

- 여러 컴포넌트가 파일 I/O (레저, 로그, 캐시)
- 잠금 경쟁
- 디스크 I/O 병목

**해결책**:

- 메모리 캐시 우선
- 비동기 로깅
- 배치 쓰기

### 4. 로깅/모니터링 오버헤드

**현재**:

- 모든 요청마다 JSONL 쓰기
- 상세한 타이밍 측정
- 여러 파일 업데이트

**최적화**:

- 로그 레벨 조정
- 버퍼링
- 샘플링 (10%만 상세 로깅)

---

## 🚀 최적화 전략

### Phase 1: Quick Wins (즉시 적용)

1. **Worker 폴링 간격 단축**

   ```python
   --interval 0.1  # 300ms → 100ms
   ```

   효과: ~150ms 절약

2. **Fast Mode 플래그 추가**

   ```python
   @dataclass
   class TaskParams:
       fast_mode: bool = False  # Resonance, BQI, 상세 로그 스킵
   ```

   효과: ~500ms 절약

3. **로깅 레벨 조정**

   ```python
   LOG_LEVEL = "WARNING"  # INFO → WARNING (운영 시)
   ```

   효과: ~100ms 절약

**총 예상 개선**: ~750ms (4.7초 → **4초**)

### Phase 2: 구조 개선 (단기)

4. **Direct Execution API 추가**

   ```python
   # 큐 우회, 직접 실행
   POST /api/v2/execute/direct
   ```

   효과: 큐 오버헤드 제거 (~500ms 절약)

5. **스트리밍 지원**

   ```python
   # WebSocket 또는 SSE
   GET /api/v2/stream/{task_id}
   ```

   효과: 체감 속도 대폭 향상

6. **병렬 처리**

   ```python
   # 컨텍스트 로드와 프롬프트 생성 병렬화
   asyncio.gather(load_context(), build_prompt())
   ```

   효과: ~200ms 절약

**총 예상 개선**: ~1.2초 (4초 → **2.8초**)

### Phase 3: 아키텍처 재설계 (중기)

7. **Hybrid Mode**
   - 간단한 요청: Direct Mode (큐 우회)
   - 복잡한 요청: Queue Mode (병렬 처리)

8. **Edge Cache**
   - 자주 쓰는 프롬프트/컨텍스트 캐싱
   - Redis 또는 메모리 캐시

9. **전용 Fast Worker**
   - 최소 기능 Worker (로깅 최소화)
   - 높은 우선순위 큐 전용

**목표**: **VS Code CLI와 동등한 성능 (1-2초)**

---

## 📈 벤치마크 계획

### 테스트 시나리오

1. **단순 코드 생성**
   - 입력: "Hello World 함수 작성"
   - 측정: 첫 토큰까지 시간, 전체 완료 시간

2. **파일 수정**
   - 입력: "이 함수에 에러 핸들링 추가"
   - 측정: 컨텍스트 로드 시간, 응답 시간

3. **복잡한 리팩토링**
   - 입력: "이 모듈을 3개 파일로 분리"
   - 측정: 전체 처리 시간, 결과 품질

### 측정 지표

- **TTFB** (Time To First Byte): 첫 응답까지
- **TTFC** (Time To First Char): 첫 문자까지 (스트리밍)
- **Total Time**: 전체 완료 시간
- **Quality Score**: 결과물 평가 (1-10)

### 비교 대상

- VS Code Claude CLI
- VS Code GitHub Copilot
- 현재 시스템 (기본)
- 현재 시스템 (Fast Mode)
- 현재 시스템 (Direct Mode)

---

## 🎯 결론 및 권장 사항

### 현재 상태 진단

**시스템 미성숙도**: ⭐⭐⭐☆☆ (중간)

- 기본 기능은 작동
- 최적화가 부족
- 프로덕션 레디는 아님

**구조적 문제**: ⭐⭐⭐⭐☆ (높음)

- 과도한 레이어
- 큐 중심 설계가 모든 케이스에 부적합
- 스트리밍 미지원이 치명적

### 즉시 조치 (이번 주)

1. ✅ **Worker 폴링 간격 단축** (0.3초 → 0.1초)
2. ✅ **Fast Mode 플래그 추가**
3. ✅ **로깅 레벨 조정 옵션**

### 단기 목표 (이번 달)

4. ✅ **Direct Execution API** 구현
5. ✅ **스트리밍 지원** (WebSocket/SSE)
6. ✅ **벤치마크 스크립트** 작성

### 중기 목표 (다음 달)

7. ✅ **Hybrid Mode** 설계 및 구현
8. ✅ **Edge Cache** 도입
9. ✅ **전용 Fast Worker** 배포

### 성공 기준

- **레이턴시**: 2초 이하 (현재 4.7초)
- **TTFC**: 500ms 이하 (스트리밍 시작)
- **품질**: VS Code CLI와 동등 (주관적 평가)
- **안정성**: 99% 가용성 유지

---

## 🔧 다음 단계

1. **성능 프로파일링 스크립트 작성**
   - 각 레이어별 시간 측정
   - 병목 지점 식별

2. **Fast Mode 구현**
   - 불필요한 레이어 스킵
   - 간소화된 파이프라인

3. **Direct API 프로토타입**
   - 큐 우회 엔드포인트
   - 빠른 응답 검증

4. **벤치마크 실행 및 분석**
   - VS Code CLI와 직접 비교
   - 데이터 기반 의사결정

---

**다음 리뷰**: 2025-11-10 (1주 후)
**책임자**: System Architect
**상태**: 🚧 진행 중
