# Week 2 íŒ¨í‚¤ì§€ ë¦¬ë·° ë° í†µí•© ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-10-12
**ì‘ì„±ì**: ì„¸ë‚˜ (Sena)
**ëª©ì **: ë£¨ë¹›ì˜ E2 ì‹¤í—˜ ì¤€ë¹„ë¥¼ ìœ„í•œ Week 2 íŒ¨í‚¤ì§€ ì‚¬ì „ ê²€í† 

---

## ğŸ“¦ Week 2 íŒ¨í‚¤ì§€ ê°œìš”

### íŒ¨í‚¤ì§€ ëª©ë¡
```
Week 2 (4ê°œ íŒ¨í‚¤ì§€, ì´ 75KB):
â”œâ”€ fdo_agi_repo_W2_rag.zip  (16KB) - RAG ì‹œìŠ¤í…œ
â”œâ”€ fdo_agi_repo_W2_eval.zip (20KB) - í‰ê°€ ì‹œìŠ¤í…œ ê°•í™”
â”œâ”€ fdo_agi_repo_W2_exec.zip (22KB) - ì‹¤í–‰ ë„êµ¬ í™•ì¥
â””â”€ fdo_agi_repo_W2_web.zip  (17KB) - ì›¹ ê²€ìƒ‰ í†µí•©
```

### ì£¼ìš” ëª©í‘œ
- **E1 ë¬¸ì œ í•´ê²°**: Verifiability 0.10 â†’ 0.60
- **ë„êµ¬ í™œì„±í™”**: RAG, ì›¹ ê²€ìƒ‰, ì½”ë“œ ì‹¤í–‰
- **í‰ê°€ ê°•í™”**: ì¦ê±° ê¸°ë°˜ í’ˆì§ˆ ì¸¡ì •

---

## ğŸ” W2_RAG: RAG ì‹œìŠ¤í…œ

### íŒŒì¼ êµ¬ì¡°
```
tools/rag/
â”œâ”€ embed.py       - ì„ë² ë”© ìƒì„± (512ì°¨ì› í•´ì‹œ ê¸°ë°˜)
â”œâ”€ indexer.py     - ë¬¸ì„œ ì¸ë±ì‹±
â””â”€ retriever.py   - ìœ ì‚¬ë„ ê²€ìƒ‰
```

### ì½”ë“œ ë¶„ì„

#### 1. embed.py - ê²½ëŸ‰ ì„ë² ë”©
```python
DIM = 512  # ë°ëª¨ìš© ì»´íŒ©íŠ¸ ì°¨ì›

def embed(text: str, dim: int = DIM) -> np.ndarray:
    """
    í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© (MD5):
    1. í…ìŠ¤íŠ¸ í† í°í™” (A-Z, ê°€-í£, 0-9)
    2. ê° í† í°ì„ MD5 í•´ì‹œ â†’ 512ì°¨ì› ë²„í‚·
    3. log-TF ê°€ì¤‘ì¹˜ (1 + log(1 + count))
    4. L2 ì •ê·œí™”
    """
    vec = np.zeros(dim, dtype=np.float32)
    counts = {}
    for token in tokenize(text):
        counts[token] = counts.get(token, 0) + 1

    for token, count in counts.items():
        bucket = hash(token) % dim
        vec[bucket] += 1.0 + math.log(1.0 + count)

    vec /= np.linalg.norm(vec)  # L2 normalize
    return vec
```

**íŠ¹ì§•**:
- âœ… **ê²½ëŸ‰**: ì™¸ë¶€ ëª¨ë¸ ë¶ˆí•„ìš” (GPT/BERT ì—†ìŒ)
- âœ… **ë¹ ë¦„**: í•´ì‹œ ê¸°ë°˜, CPUë§Œìœ¼ë¡œ ë™ì‘
- âœ… **í•œê¸€ ì§€ì›**: ìœ ë‹ˆì½”ë“œ í† í° íŒ¨í„´
- âŒ **ì˜ë¯¸ë¡ ì  í•œê³„**: ë‹¨ìˆœ í•´ì‹œ, ë¬¸ë§¥ ì´í•´ ì—†ìŒ
- âŒ **ì €ì°¨ì›**: 512ì°¨ì› (ì¼ë°˜ì ìœ¼ë¡œ 768~1536)

**E2 ì í•©ì„±**: âš ï¸ ë°ëª¨ìš©ìœ¼ë¡œëŠ” OK, í”„ë¡œë•ì…˜ì—ì„œëŠ” ì—…ê·¸ë ˆì´ë“œ í•„ìš”

---

#### 2. retriever.py - ê²€ìƒ‰ ì‹œìŠ¤í…œ
```python
def rag_query(query: str, top_k: int = 5,
              index_dir: str = "memory/vectorstore"):
    """
    RAG ì¿¼ë¦¬:
    1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    2. FAISS ë˜ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ top-k ê²€ìƒ‰
    3. ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë°˜í™˜
    """
    mat, meta, index = _load_index(index_dir)
    qv = embed(query)

    if index is not None:  # FAISS ì‚¬ìš© ê°€ëŠ¥ ì‹œ
        D, I = index.search(qv, top_k)
    else:  # Fallback: NumPy ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        I, D = _cosine_topk(mat, qv, top_k)

    hits = [{"id": meta[i]["id"],
             "source": meta[i]["path"],
             "score": float(D[i]),
             "preview": meta[i].get("preview", "")}
            for i in I]

    return {"ok": True, "hits": hits}
```

**íŠ¹ì§•**:
- âœ… **FAISS ì§€ì›**: ì„ íƒì  (ì—†ìœ¼ë©´ NumPy fallback)
- âœ… **ë©”íƒ€ë°ì´í„°**: source, preview í¬í•¨
- âœ… **ê°„ë‹¨í•œ API**: 1ì¤„ë¡œ ê²€ìƒ‰ ê°€ëŠ¥
- âš ï¸ **ì¬ë­í‚¹ ì—†ìŒ**: top-kë§Œ ë°˜í™˜, ì¬ì •ë ¬ ë¯¸ì§€ì›

**E2 í†µí•©**:
```python
# Thesis í˜ë¥´ì†Œë‚˜ì—ì„œ ì‚¬ìš©
from tools.rag.retriever import rag_query

results = rag_query("empathic AI coach for writers", top_k=3)
for hit in results["hits"]:
    print(f"Source: {hit['source']}, Score: {hit['score']:.2f}")
    print(f"Preview: {hit['preview']}\n")
```

---

### RAG ì¸ë±ì‹± (ì‚¬ì „ ì‘ì—…)

Week 2 íŒ¨í‚¤ì§€ ì‚¬ìš© ì „ ì¸ë±ìŠ¤ ìƒì„± í•„ìš”:

```bash
# 1. ë¬¸ì„œ ì¸ë±ì‹±
python scripts/index_docs.py \
  --input docs/ \
  --output memory/vectorstore/

# 2. ì¸ë±ìŠ¤ í™•ì¸
ls memory/vectorstore/
# vectors.npy     - ì„ë² ë”© í–‰ë ¬
# meta.json       - ë©”íƒ€ë°ì´í„° (path, preview)
# faiss.index     - FAISS ì¸ë±ìŠ¤ (ì„ íƒ)
```

**ì¸ë±ì‹± ëŒ€ìƒ**:
- `docs/` ë””ë ‰í† ë¦¬ì˜ Markdown íŒŒì¼
- Week 1 ìŠ¤ìºí´ë“œ ë¬¸ì„œ
- ê¸°ì¡´ AGI ì„¤ê³„ ë¬¸ì„œ (Sena, Core)

---

## ğŸ” W2_EVAL: í‰ê°€ ì‹œìŠ¤í…œ ê°•í™”

### íŒŒì¼ êµ¬ì¡°
```
orchestrator/
â”œâ”€ evaluator.py         - í’ˆì§ˆ í‰ê°€ (ì¦ê±° ê¸°ë°˜)
â””â”€ self_correction.py   - RUNE ìë™ ìƒì„±
```

### ì½”ë“œ ë¶„ì„

#### 1. evaluator.py - ì¦ê±° ê¸°ë°˜ í‰ê°€
```python
def evaluate(outputs: List[PersonaOutput],
             cfg: Dict[str, Any] | None = None) -> EvalReport:
    """
    í’ˆì§ˆ í‰ê°€ (4ê°€ì§€ ë©”íŠ¸ë¦­):
    1. Evidence (45%): ì¸ìš© ê°œìˆ˜, ë‹¤ì–‘ì„±, ì‹ ë¢°ë„
    2. Readability (25%): ë‹¨ì–´ ìˆ˜, ë¬¸ì¥ ìˆ˜ íœ´ë¦¬ìŠ¤í‹±
    3. Logic (15%): ë…¼ë¦¬ í‘œì§€ì–´ ì¡´ì¬ ì—¬ë¶€
    4. Trust (15%): í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë„ë©”ì¸ ë¹„ìœ¨

    í’ˆì§ˆ = weighted_sum - redundancy_penalty
    """
    # 1. Evidence ë©”íŠ¸ë¦­
    total_cites, diversity, trust = _evidence_metrics(outputs, whitelist)
    evidence_score = 0.4 + 0.2*min(total, 4) + 0.4*diversity

    # 2. Readability (30-200 ë‹¨ì–´, 2-8 ë¬¸ì¥ ì„ í˜¸)
    readability = _readability(summaries)

    # 3. Logic hints (ë…¼ë¦¬ í‘œì§€ì–´)
    logic = _logic_hint(summaries)

    # 4. Trust (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë„ë©”ì¸)
    trust_score = sum(1.0 if whitelisted else 0.6) / total_cites

    # 5. Redundancy penalty
    redundancy = _redundancy_penalty(outputs)  # 0~0.3

    # ì¢…í•© í’ˆì§ˆ
    quality = (0.45*evidence_score + 0.25*readability
               + 0.15*logic + 0.15*trust_score - redundancy)

    # ì¦ê±° OK ì¡°ê±´: >=2 ì¸ìš©, trust >=0.7
    evidence_ok = total_cites >= 2 and trust_score >= 0.7

    return EvalReport(quality=quality, evidence_ok=evidence_ok, ...)
```

**E1 ë¬¸ì œ í•´ê²°**:
- âŒ **E1**: Verifiability 0.10 (1/10 facts checked)
- âœ… **W2**: ì¦ê±° ê¸°ë°˜ í‰ê°€, ìµœì†Œ 2ê°œ ì¸ìš© ê°•ì œ
- âœ… **W2**: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ë¡œ ì‹ ë¢°ë„ ì¸¡ì •

**ê°œì„  í¬ì¸íŠ¸**:
- âœ… **Evidence-first**: 45% ê°€ì¤‘ì¹˜ (ê°€ì¥ ë†’ìŒ)
- âœ… **Redundancy penalty**: ì¤‘ë³µ ì¸ìš© í˜ë„í‹°
- âœ… **Trust score**: ë„ë©”ì¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
- âš ï¸ **Logic hints**: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°œì„  ì—¬ì§€)

---

#### 2. self_correction.py - ìë™ RUNE

```python
def rune_from_eval(eval_report: EvalReport) -> RUNEReport:
    """
    Eval ê²°ê³¼ â†’ RUNE ìë™ ìƒì„±:
    - quality < 0.7 ë˜ëŠ” evidence_ok == False â†’ replan
    - êµ¬ì²´ì  ê¶Œì¥ ì‚¬í•­ ìƒì„±
    """
    q = eval_report.quality
    replan = (not eval_report.evidence_ok) or q < 0.7

    recs = []
    if not eval_report.evidence_ok:
        recs.append("ê·¼ê±° 2ê°œ ì´ìƒ í™•ë³´(í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í¬í•¨)")
    if q < 0.7:
        recs.append("ê°€ë…ì„±/ë…¼ë¦¬ í‘œì§€ì–´ ë³´ê°• ë° ì¤‘ë³µ ê·¼ê±° ì œê±°")
    if not recs:
        recs.append("ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰")

    return RUNEReport(
        impact=max(0.2, q),
        transparency=0.75 if eval_report.evidence_ok else 0.55,
        confidence=0.55 if q >= 0.7 else 0.4,
        recommendations=recs,
        replan=replan
    )
```

**E1 vs W2 ë¹„êµ**:

| í•­ëª© | E1 Baseline | W2 Eval |
|------|-------------|---------|
| **Verifiability** | 0.10 (1/10) | ê°•ì œ â‰¥2 ì¸ìš© |
| **Trust** | ì¸¡ì • ì•ˆ í•¨ | í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ |
| **Redundancy** | ì²´í¬ ì•ˆ í•¨ | í˜ë„í‹° 0~0.3 |
| **RUNE ìƒì„±** | ìˆ˜ë™ | ìë™ (quality < 0.7 â†’ replan) |
| **ê¶Œì¥ ì‚¬í•­** | ì¼ë°˜ì  | êµ¬ì²´ì  (ì˜ˆ: "ê·¼ê±° 2ê°œ í™•ë³´") |

---

### í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì„¤ì •

`configs/phase_controller_e2.yaml`ì— ì¶”ê°€:

```yaml
evaluation:
  whitelist:
    - "arxiv.org"
    - "scholar.google.com"
    - "github.com"
    - "wikipedia.org"
    - "openai.com"
    - "anthropic.com"
    # í”„ë¡œì íŠ¸ë³„ ì‹ ë¢° ë„ë©”ì¸ ì¶”ê°€

  weights:
    evidence: 0.45
    readability: 0.25
    logic: 0.15
    trust: 0.15

  thresholds:
    min_citations: 2
    min_trust: 0.7
    replan_quality: 0.7
```

---

## ğŸ”„ E1 â†’ E2 í†µí•© ê³„íš

### Phase 1: RAG ì¸ë±ì‹± (ì‚¬ì „ ì‘ì—…)
```bash
# 1. ê¸°ì¡´ ë¬¸ì„œ ìˆ˜ì§‘
mkdir -p memory/vectorstore_source
cp docs/*.md memory/vectorstore_source/
cp outputs/E1_*.md memory/vectorstore_source/

# 2. ì¸ë±ìŠ¤ ìƒì„±
python scripts/index_docs.py \
  --input memory/vectorstore_source/ \
  --output memory/vectorstore/

# 3. ê²€ì¦
python -c "
from tools.rag.retriever import rag_query
results = rag_query('AGI design', top_k=3)
for hit in results['hits']:
    print(f\"{hit['source']}: {hit['score']:.2f}\")
"
```

**ì˜ˆìƒ ì¶œë ¥**:
```
docs/AGI_INTEGRATION_SENA_CORE_v1.0.md: 0.85
docs/LUBIT_CORE_FDO_AGI_INTEGRATION_v1.0.md: 0.78
outputs/E1_HIGH_RESIDUAL_QUALITATIVE_ANALYSIS.md: 0.72
```

---

### Phase 2: Evaluator í†µí•©

**í˜„ì¬ (E1)**:
```python
# orchestration/persona_orchestrator.py
evaluation = {
    "quality": 0.8,
    "evidence_ok": True,  # ë”ë¯¸
    "risks": []
}
```

**ì—…ê·¸ë ˆì´ë“œ (E2)**:
```python
from orchestrator.evaluator import evaluate

# PersonaOutput ìˆ˜ì§‘
outputs = [thesis_output, antithesis_output, synthesis_output]

# í‰ê°€ ì‹¤í–‰
eval_report = evaluate(outputs, cfg={
    "whitelist": ["arxiv.org", "github.com", ...],
    "weights": {"evidence": 0.45, ...}
})

# RUNE ìë™ ìƒì„±
from orchestrator.self_correction import rune_from_eval
rune_report = rune_from_eval(eval_report)

# ì¬ê³„íš ê²°ì •
if rune_report.replan:
    print("Quality too low, replanning...")
    # ë£¨í”„ ì¬ì‹¤í–‰ ë¡œì§
```

---

### Phase 3: Persona ë„êµ¬ í™œì„±í™”

**Thesis í˜ë¥´ì†Œë‚˜ì— RAG ì¶”ê°€**:

```python
# personas/thesis.py (ê¸°ì¡´)
class ThesisPersona:
    def generate_response(self, prompt):
        # í˜„ì¬: LLM ì§ì ‘ í˜¸ì¶œ
        return self.backend.call(prompt)

# personas/thesis.py (E2 ì—…ê·¸ë ˆì´ë“œ)
class ThesisPersona:
    def generate_response(self, prompt):
        # 1. RAGë¡œ ë°°ê²½ ì¡°ì‚¬
        rag_results = rag_query(prompt, top_k=3)
        context = "\n".join([
            f"[{hit['source']}] {hit['preview']}"
            for hit in rag_results['hits']
        ])

        # 2. Context-enriched í”„ë¡¬í”„íŠ¸
        enriched_prompt = f"""
        Background research:
        {context}

        Based on the above context, {prompt}

        Requirements:
        - Cite at least 2 sources from the research
        - Provide concrete examples
        """

        return self.backend.call(enriched_prompt)
```

**ì˜ˆìƒ íš¨ê³¼**:
- Verifiability: 0.10 â†’ 0.60 (+500%)
- ì¸ìš© ê°œìˆ˜: 0~1ê°œ â†’ 2~3ê°œ
- Quality ì ìˆ˜: 0.67 â†’ 0.80

---

## ğŸ“Š E2 ì‹¤í—˜ ì˜ˆìƒ ê²°ê³¼

### Before (E1 Baseline)
```
Total turns: 48
Tool calls: 0
Average quality: 0.67
Verifiability: 0.10 (1/10 facts checked)
Evidence: 0~1 citations per turn
Risks: ["ê·¼ê±° ì—†ìŒ", "ê·¼ê±° ë¶€ì¡±(1ê°œ)"] (85%)
```

### After (E2 with W2 packages)
```
Total turns: 48
Tool calls: 12~18 (RAG ì£¼ë¡œ)
  â”œâ”€ Thesis: 4~6 RAG calls
  â”œâ”€ Antithesis: 3~5 RAG calls
  â””â”€ Synthesis: 2~3 RAG calls
Average quality: 0.80 (+19%)
Verifiability: 0.60 (+500%)
Evidence: 2~3 citations per turn
Risks: ["ì¤‘ë³µ ê·¼ê±° ê³¼ë‹¤"] (15%) â† ëŒ€í­ ê°ì†Œ
```

### Metrics ë¹„êµ

| ë©”íŠ¸ë¦­ | E1 | E2 ëª©í‘œ | ê°œì„ ìœ¨ |
|--------|-----|---------|--------|
| **Quality** | 0.67 | 0.80 | +19% |
| **Verifiability** | 0.10 | 0.60 | +500% |
| **Evidence OK** | 10% | 80% | +700% |
| **Citations/turn** | 0.5 | 2.5 | +400% |
| **Replan rate** | 15% | 10% | -33% |

---

## ğŸ› ï¸ í†µí•© ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‚¬ì „ ì¤€ë¹„ (Phase 0)
- [ ] Week 2 íŒ¨í‚¤ì§€ 4ê°œ ì••ì¶• í•´ì œ ì™„ë£Œ
- [ ] `memory/vectorstore/` ë””ë ‰í† ë¦¬ ìƒì„±
- [ ] ì¸ë±ì‹±í•  ë¬¸ì„œ ìˆ˜ì§‘ (`docs/`, ê¸°ì¡´ ë¦¬í¬íŠ¸)

### RAG ì„¤ì • (Phase 1)
- [ ] `python scripts/index_docs.py` ì‹¤í–‰
- [ ] `vectors.npy`, `meta.json` ìƒì„± í™•ì¸
- [ ] RAG ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ (`rag_query("test")`)
- [ ] (ì„ íƒ) FAISS ì„¤ì¹˜ ë° ì¸ë±ìŠ¤ ìƒì„±

### Evaluator í†µí•© (Phase 2)
- [ ] `orchestrator/evaluator.py` ë³µì‚¬
- [ ] í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì„¤ì • (`configs/`)
- [ ] `evaluate()` í•¨ìˆ˜ í†µí•©
- [ ] `rune_from_eval()` ìë™í™”
- [ ] ì¬ê³„íš ë¡œì§ ì—°ê²°

### Persona ì—…ê·¸ë ˆì´ë“œ (Phase 3)
- [ ] Thesisì— RAG í˜¸ì¶œ ì¶”ê°€
- [ ] Antithesisì— RAG í˜¸ì¶œ ì¶”ê°€
- [ ] Synthesisì— RAG í˜¸ì¶œ ì¶”ê°€ (ì„ íƒ)
- [ ] ë„êµ¬ í˜¸ì¶œ ë¡œê¹… í™œì„±í™”

### ì‹¤í—˜ ì‹¤í–‰ (Phase 4)
- [ ] `phase_controller_e2.yaml` ì ìš©
- [ ] E2 ì‹¤í—˜ ì‹¤í–‰ (ë™ì¼ íƒœìŠ¤í¬ë¡œ E1ê³¼ ë¹„êµ)
- [ ] ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (`outputs/persona_metrics/E2/`)
- [ ] E1 vs E2 ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±

---

## ğŸš¨ ì£¼ì˜ ì‚¬í•­

### 1. ì„ë² ë”© ëª¨ë¸ í•œê³„
**ë¬¸ì œ**: í•´ì‹œ ê¸°ë°˜ ì„ë² ë”©ì€ ì˜ë¯¸ë¡ ì  ì´í•´ ë¶€ì¡±
```python
embed("empathic AI")  # í•´ì‹œ: bucket_3452, bucket_8791
embed("caring AI")    # í•´ì‹œ: bucket_7261, bucket_8791
# ìœ ì‚¬ë„ ë‚®ìŒ (ì˜ë¯¸ëŠ” ìœ ì‚¬í•˜ì§€ë§Œ ë‹¨ì–´ê°€ ë‹¤ë¦„)
```

**í•´ê²°ì±… (E3 ê³ ë ¤)**:
- OpenAI `text-embedding-ada-002` í†µí•©
- Sentence-BERT ë¡œì»¬ ëª¨ë¸
- í˜„ì¬ëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜ì¤€ìœ¼ë¡œ ì¶©ë¶„

---

### 2. ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
**ë¬¸ì œ**: ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì‹œ ì¬ì¸ë±ì‹± í•„ìš”

**í•´ê²°ì±…**:
```bash
# ì¦ë¶„ ì¸ë±ì‹± (ê°„ë‹¨í•œ ë°©ë²•)
python scripts/index_docs.py \
  --input docs/new/ \
  --output memory/vectorstore/ \
  --append  # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
```

---

### 3. í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬
**ë¬¸ì œ**: ë„ë©”ì¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ê°€ ê³ ì •ë˜ì–´ ìˆìŒ

**í•´ê²°ì±…**:
```yaml
# configs/whitelist.yaml
trusted_domains:
  academic:
    - "arxiv.org"
    - "scholar.google.com"
  tech:
    - "github.com"
    - "stackoverflow.com"
  company:
    - "openai.com"
    - "anthropic.com"
  project_specific:
    - "your-internal-docs.com"
```

---

## ğŸ“ˆ ì˜ˆìƒ íƒ€ì„ë¼ì¸

### Week 2 í†µí•© (E2 ì‹¤í—˜)
```
Day 1: RAG ì¸ë±ì‹± (2ì‹œê°„)
  â”œâ”€ ë¬¸ì„œ ìˆ˜ì§‘
  â”œâ”€ ì¸ë±ìŠ¤ ìƒì„±
  â””â”€ í…ŒìŠ¤íŠ¸

Day 1: Evaluator í†µí•© (2ì‹œê°„)
  â”œâ”€ evaluator.py ë³µì‚¬
  â”œâ”€ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì„¤ì •
  â””â”€ ìë™ RUNE ì—°ê²°

Day 2: Persona ì—…ê·¸ë ˆì´ë“œ (3ì‹œê°„)
  â”œâ”€ RAG í˜¸ì¶œ ì½”ë“œ ì¶”ê°€
  â”œâ”€ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
  â””â”€ í…ŒìŠ¤íŠ¸

Day 2: E2 ì‹¤í—˜ ì‹¤í–‰ (2ì‹œê°„)
  â”œâ”€ E2 ì‹¤í–‰ (1ì‹œê°„)
  â”œâ”€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (30ë¶„)
  â””â”€ E1 vs E2 ë¹„êµ (30ë¶„)
```

**ì´ ì†Œìš” ì‹œê°„**: 2ì¼ (9ì‹œê°„)

---

## ğŸ¯ í•µì‹¬ ìš”ì•½

### Week 2ê°€ í•´ê²°í•˜ëŠ” E1 ë¬¸ì œ
1. âœ… **Verifiability 0.10 â†’ 0.60**: ì¦ê±° ê¸°ë°˜ í‰ê°€
2. âœ… **ë„êµ¬ ë¯¸ì‚¬ìš© â†’ RAG í™œì„±í™”**: ë°°ê²½ ì§€ì‹ í†µí•©
3. âœ… **ìë™ RUNE ìƒì„±**: quality < 0.7 â†’ ì¬ê³„íš
4. âœ… **í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸**: ì‹ ë¢°ë„ ì¸¡ì •

### í†µí•© ìš°ì„ ìˆœìœ„
1. ğŸ”´ **High**: RAG ì¸ë±ì‹±, Evaluator í†µí•©
2. ğŸŸ¡ **Medium**: Persona RAG í˜¸ì¶œ
3. ğŸŸ¢ **Low**: FAISS ìµœì í™”, ì„ë² ë”© ì—…ê·¸ë ˆì´ë“œ (E3)

### ë‹¤ìŒ ë‹¨ê³„
- **ë£¨ë¹›**: E2 ì‹¤í—˜ ì‹¤í–‰ (phase_controller_e2.yaml)
- **ì„¸ë‚˜**: Week 3 íŒ¨í‚¤ì§€ ì‚¬ì „ ê²€í†  (ì¤€ë¹„)

---

**ì‘ì„±ì**: ì„¸ë‚˜ (Sena)
**ë²„ì „**: 1.0
**ìƒíƒœ**: ë£¨ë¹› E2 ì‹¤í—˜ ëŒ€ê¸° ì¤‘
**í† í° ì‚¬ìš©**: 85K / 200K (42%)
