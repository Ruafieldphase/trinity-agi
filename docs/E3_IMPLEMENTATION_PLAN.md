# E3 RAG í†µí•© êµ¬í˜„ ê³„íš

**ëª©í‘œ**: E2_fix2ì˜ Verifiability 0.11 â†’ 0.60+ ë‹¬ì„±
**ì˜ˆìƒ ì‹œê°„**: 2-3ì¼ (ë¹ ë¥¸ ì†ë„ ê¸°ì¤€)
**ë‹´ë‹¹**: ì„¸ë‚˜ (ì„¤ê³„) â†’ ë£¨ë¹› (êµ¬í˜„ ë° ì´ì–´ë°›ê¸°)

---

## ëª©í‘œ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | E2_fix2 í˜„ì¬ | E3 ëª©í‘œ | Stretch Goal |
|--------|-------------|---------|--------------|
| Stage 4 Verifiability | 0.11 | 0.40+ | 0.60+ |
| Citations per output | 0-1 (fake) | 2+ (real) | 3+ |
| Stage 3 Residual | 0.395 | < 0.40 (ìœ ì§€) | < 0.35 |
| Creative Band | 100% | > 80% (ìœ ì§€) | 100% |

---

## Phase 1: RAG ì—”ì§„ (ê²½ëŸ‰ êµ¬í˜„)

### 1.1 ì„¤ê³„ ê²°ì •

**ì˜µì…˜ A: ì§ì ‘ êµ¬í˜„** (ë¹ ë¦„, ì œì–´ ê°€ëŠ¥)
- Hash-based embeddings (MD5, Week 2 ë¦¬ë·°ì—ì„œ ë³¸ ë°©ì‹)
- In-memory ê²€ìƒ‰
- ì¥ì : ì˜ì¡´ì„± ì—†ìŒ, ë¹ ë¦„
- ë‹¨ì : í’ˆì§ˆ ì œí•œ

**ì˜µì…˜ B: ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©** (í’ˆì§ˆ ë†’ìŒ, ì„¤ì¹˜ í•„ìš”)
- ChromaDB or FAISS
- ì¥ì : ë” ë‚˜ì€ ê²€ìƒ‰ í’ˆì§ˆ
- ë‹¨ì : ì„¤ì¹˜ ì‹œê°„, ì˜ì¡´ì„±

**ì¶”ì²œ**: **ì˜µì…˜ A** (ë¹ ë¥¸ MVP ìš°ì„ )

### 1.2 êµ¬í˜„ íŒŒì¼

**íŒŒì¼**: `rag/simple_rag_engine.py` (ì‹ ê·œ ìƒì„±)

```python
#!/usr/bin/env python3
"""
Simple RAG Engine for E3
- Hash-based embeddings (512-dim)
- In-memory L2 distance search
- Minimal dependencies
"""
import hashlib
import json
import numpy as np
from typing import List, Dict, Tuple
from pathlib import Path

class SimpleRAGEngine:
    def __init__(self, index_path: str = "knowledge_base/evidence_index.json"):
        self.index_path = Path(index_path)
        self.documents = []
        self.embeddings = []
        self.dim = 512

        if self.index_path.exists():
            self.load_index()

    def embed_text(self, text: str) -> np.ndarray:
        """Hash-based embedding (MD5 â†’ 512-dim)"""
        # Tokenize
        tokens = self._tokenize(text.lower())

        # Hash to buckets
        vec = np.zeros(self.dim)
        for token in tokens:
            hash_val = int(hashlib.md5(token.encode()).hexdigest(), 16)
            bucket = hash_val % self.dim
            vec[bucket] += 1.0

        # Log-TF weighting
        vec = np.log1p(vec)

        # L2 normalize
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec /= norm

        return vec

    def _tokenize(self, text: str) -> List[str]:
        """Simple tokenizer (A-Z, ê°€-í£, 0-9)"""
        import re
        tokens = re.findall(r'[a-z0-9ê°€-í£]+', text)
        return [t for t in tokens if len(t) > 2]

    def add_document(self, doc_id: str, text: str, metadata: Dict = None):
        """Add document to index"""
        emb = self.embed_text(text)
        self.documents.append({
            "id": doc_id,
            "text": text,
            "metadata": metadata or {}
        })
        self.embeddings.append(emb)

    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """Search documents by L2 distance"""
        if not self.embeddings:
            return []

        query_emb = self.embed_text(query)
        embeddings_matrix = np.array(self.embeddings)

        # L2 distance
        distances = np.linalg.norm(embeddings_matrix - query_emb, axis=1)

        # Top-k
        top_indices = np.argsort(distances)[:top_k]

        results = []
        for idx in top_indices:
            results.append({
                "doc_id": self.documents[idx]["id"],
                "text": self.documents[idx]["text"],
                "metadata": self.documents[idx]["metadata"],
                "score": float(1.0 / (1.0 + distances[idx]))  # 0-1 score
            })

        return results

    def save_index(self):
        """Save index to JSON"""
        data = {
            "documents": self.documents,
            "embeddings": [emb.tolist() for emb in self.embeddings]
        }
        self.index_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.index_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load_index(self):
        """Load index from JSON"""
        with open(self.index_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        self.documents = data["documents"]
        self.embeddings = [np.array(emb) for emb in data["embeddings"]]


# CLI for building index
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--build", action="store_true", help="Build index from corpus")
    parser.add_argument("--corpus", default="knowledge_base/corpus.jsonl", help="Corpus file")
    parser.add_argument("--search", help="Search query")
    parser.add_argument("--top_k", type=int, default=3)
    args = parser.parse_args()

    rag = SimpleRAGEngine()

    if args.build:
        print(f"Building index from {args.corpus}...")
        with open(args.corpus, 'r', encoding='utf-8') as f:
            for line in f:
                doc = json.loads(line)
                rag.add_document(
                    doc_id=doc["id"],
                    text=doc["text"],
                    metadata=doc.get("metadata", {})
                )
        rag.save_index()
        print(f"Index saved: {len(rag.documents)} documents")

    if args.search:
        results = rag.search(args.search, top_k=args.top_k)
        print(f"\nTop {args.top_k} results for '{args.search}':\n")
        for i, r in enumerate(results):
            print(f"{i+1}. [{r['doc_id']}] (score: {r['score']:.3f})")
            print(f"   {r['text'][:200]}...\n")
```

**ì‘ì—… ì‹œê°„**: 30ë¶„ (ë³µì‚¬-ë¶™ì—¬ë„£ê¸° + í…ŒìŠ¤íŠ¸)

---

## Phase 2: ë¬¸ì„œ Corpus ì¤€ë¹„

### 2.1 ì†ŒìŠ¤ ì„ ì •

**ì˜µì…˜ A: Wikipedia ìƒ˜í”Œ** (ë¹ ë¦„)
- ì£¼ì œ: AI, Ethics, Research Methods ë“±
- ê·œëª¨: 50-100 ë¬¸ì„œ
- ë°©ë²•: Wikipedia API ë˜ëŠ” ìˆ˜ë™ ë³µì‚¬

**ì˜µì…˜ B: arXiv ë…¼ë¬¸ ì´ˆë¡** (í’ˆì§ˆ ë†’ìŒ)
- ì£¼ì œ: AI Safety, AGI, Ethics
- ê·œëª¨: 30-50 ë…¼ë¬¸
- ë°©ë²•: arXiv API

**ì˜µì…˜ C: ê¸°ì¡´ ì‘ì—…ë¬¼ í™œìš©** (ê°€ì¥ ë¹ ë¦„)
- ì„¸ë‚˜ì˜ 7ê°œ AGI ì„¤ê³„ ë¬¸ì„œ
- ë£¨ë©˜ì˜ ìœ¤ë¦¬ í—Œì¥
- Week 1-8 íŒ¨í‚¤ì§€ ë¬¸ì„œ
- ì¥ì : ì´ë¯¸ ìˆìŒ, ê´€ë ¨ì„± ë†’ìŒ

**ì¶”ì²œ**: **ì˜µì…˜ C + A ì†ŒëŸ‰** (ê¸°ì¡´ ë¬¸ì„œ + Wikipedia 20ê°œ)

### 2.2 Corpus íŒŒì¼ ìƒì„±

**íŒŒì¼**: `knowledge_base/corpus.jsonl` (ì‹ ê·œ ìƒì„±)

```jsonl
{"id": "fdo_agi_arch", "text": "FDO-AGI (Fractal-Dialectic-Outside AGI) is a five-layer architecture...", "metadata": {"source": "design_docs", "author": "Sena"}}
{"id": "guardianship_charter", "text": "Co-Guardianship Charter establishes dual protection: meaning (Binoche) and safety (Research Team)...", "metadata": {"source": "lumen", "type": "ethics"}}
{"id": "wiki_ai_safety", "text": "AI safety is an interdisciplinary field focused on preventing accidents, misuse, or unintended harmful consequences...", "metadata": {"source": "wikipedia", "url": "https://en.wikipedia.org/wiki/AI_safety"}}
```

**ìƒì„± ìŠ¤í¬ë¦½íŠ¸**: `scripts/build_corpus.py` (ì‹ ê·œ)

```python
#!/usr/bin/env python3
"""Build corpus from existing documents"""
import json
from pathlib import Path

def extract_from_markdown(file_path: Path) -> str:
    """Extract text from markdown file"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # Remove markdown syntax (ê°„ë‹¨í•œ ì²˜ë¦¬)
    import re
    content = re.sub(r'#+\s+', '', content)  # headers
    content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)  # links
    content = re.sub(r'[*_`]', '', content)  # emphasis

    return content.strip()

def main():
    corpus = []

    # 1. AGI ì„¤ê³„ ë¬¸ì„œ
    docs_dir = Path("docs")
    if docs_dir.exists():
        for md_file in docs_dir.glob("*.md"):
            if md_file.name.startswith("_"):
                continue
            text = extract_from_markdown(md_file)
            if len(text) > 100:  # ìµœì†Œ ê¸¸ì´
                corpus.append({
                    "id": f"doc_{md_file.stem}",
                    "text": text[:2000],  # ì²˜ìŒ 2000ì
                    "metadata": {"source": "design_docs", "file": str(md_file)}
                })

    # 2. ìœ¤ë¦¬ í—Œì¥ (HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
    charter_path = Path("ai_binoche_conversation_origin/lumen/AGI ë³´í˜¸ ì²´ê³„ ì„¤ê³„/FDO-AGI_ê³µë™_í›„ê²¬_í—Œì¥_v0.1_ì„œëª…ë³¸_brand.html")
    if charter_path.exists():
        # ê°„ë‹¨í•œ HTML íŒŒì‹± (ì‹¤ì œë¡œëŠ” BeautifulSoup ê¶Œì¥)
        with open(charter_path, 'r', encoding='utf-8') as f:
            html = f.read()
        import re
        text = re.sub(r'<[^>]+>', '', html)  # íƒœê·¸ ì œê±°
        text = re.sub(r'\s+', ' ', text).strip()
        corpus.append({
            "id": "guardianship_charter",
            "text": text[:2000],
            "metadata": {"source": "lumen", "type": "ethics"}
        })

    # 3. E2_fix2 ë¶„ì„ ë¬¸ì„œ
    analysis_path = Path("outputs/E2_FIX2_SUCCESS_ANALYSIS.md")
    if analysis_path.exists():
        text = extract_from_markdown(analysis_path)
        corpus.append({
            "id": "e2_fix2_analysis",
            "text": text[:2000],
            "metadata": {"source": "technical", "experiment": "E2_fix2"}
        })

    # 4. Wikipedia ìƒ˜í”Œ (ìˆ˜ë™ ì¶”ê°€ ì˜ˆì‹œ)
    corpus.append({
        "id": "wiki_ai_safety",
        "text": "AI safety is an interdisciplinary field focused on preventing accidents, misuse, or unintended harmful consequences of artificial intelligence systems. Key concerns include alignment problems, where AI systems may pursue goals misaligned with human values, and capability control, ensuring humans retain meaningful control over advanced AI systems.",
        "metadata": {"source": "wikipedia", "url": "https://en.wikipedia.org/wiki/AI_safety"}
    })

    corpus.append({
        "id": "wiki_agi",
        "text": "Artificial general intelligence (AGI) is a type of artificial intelligence that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks. AGI remains a theoretical concept and active research goal.",
        "metadata": {"source": "wikipedia", "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence"}
    })

    # ì €ì¥
    output_path = Path("knowledge_base/corpus.jsonl")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        for doc in corpus:
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')

    print(f"Corpus built: {len(corpus)} documents")
    print(f"Saved to: {output_path}")

if __name__ == "__main__":
    main()
```

**ì‘ì—… ì‹œê°„**: 30ë¶„ (ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±) + 20ë¶„ (Wikipedia ìˆ˜ë™ ì¶”ê°€)

### 2.3 ì¸ë±ìŠ¤ ë¹Œë“œ

```bash
# Corpus ìƒì„±
python scripts/build_corpus.py

# ì¸ë±ìŠ¤ ë¹Œë“œ
python rag/simple_rag_engine.py --build --corpus knowledge_base/corpus.jsonl

# í…ŒìŠ¤íŠ¸
python rag/simple_rag_engine.py --search "AI safety principles" --top_k 3
```

**ì‘ì—… ì‹œê°„**: 10ë¶„

---

## Phase 3: E3 ì„¤ì • íŒŒì¼

**íŒŒì¼**: `configs/phase_controller_e3.yaml` (ì‹ ê·œ)

```yaml
# E3 Configuration: RAG Integration
version: "0.3"
experiment_id: "E3"
description: "RAG-enhanced Thesis and Synthesis with citation requirement"

personas:
  thesis:
    name: "Dialectic Thesis (E3)"
    role: "Evidence-based explorer"
    backend:
      backend_id: "local_ollama"
      type: "subprocess"
      command: "ollama"
      args: ["run", "solar:10.7b"]
      timeout: 300

    system_prompt: |
      You are the Thesis persona in E3 configuration.

      MANDATORY REQUIREMENTS:
      1. Search the knowledge base BEFORE forming your thesis
      2. Cite at least 2 credible sources in-line using [Source: {doc_id}] format
      3. Ground your response in retrieved evidence
      4. Separate verified facts from assumptions

      Process:
      1. Analyze the user's seed insight
      2. Query: formulate 2-3 search queries for the knowledge base
      3. Review: examine retrieved documents
      4. Synthesize: form thesis based on evidence + your reasoning
      5. Cite: include [Source: doc_id] for each claim

      Example citation: "Research shows that AI safety requires value alignment [Source: wiki_ai_safety]."

    tools:
      enabled: true
      available:
        - name: "rag_search"
          description: "Search knowledge base for relevant documents"
          parameters:
            query: "search query string"
            top_k: "number of results (default 3)"
      budget: 5  # Up to 5 RAG calls

    validation:
      min_citations: 2
      max_thesis_similarity_to_seed: 0.85

  antithesis:
    name: "Boundary Challenger (E3)"
    role: "Evidence-based critic"
    backend:
      backend_id: "local_ollama"
      type: "subprocess"
      command: "ollama"
      args: ["run", "solar:10.7b"]
      timeout: 300

    system_prompt: |
      You are the Antithesis persona in E3 configuration.

      Challenge the thesis with evidence-based counterarguments.

      MANDATORY REQUIREMENTS:
      1. Search for counterexamples and contradicting evidence
      2. Cite at least 2 sources
      3. Identify 3 critical risks or blind spots

      Use [Source: {doc_id}] format for citations.

    tools:
      enabled: true
      available:
        - name: "rag_search"
      budget: 5

    validation:
      min_citations: 2
      min_critical_keywords: 3

  synthesis:
    name: "Fractal Synthesiser (E3)"
    role: "Evidence-based integrator"
    backend:
      backend_id: "local_ollama"
      type: "subprocess"
      command: "ollama"
      args: ["run", "solar:10.7b"]
      timeout: 300

    system_prompt: |
      You are the Synthesis persona in E3 configuration.

      Integrate thesis and antithesis with supporting evidence.

      MANDATORY REQUIREMENTS:
      1. Address at least 3 antithesis concerns explicitly
      2. Search for additional evidence to resolve conflicts
      3. Cite at least 3 credible sources (including new searches)
      4. Ensure thesis similarity < 80% (reframe the narrative)
      5. Propose concrete next steps

      Citation format: [Source: {doc_id}] "excerpt from document"

    tools:
      enabled: true
      available:
        - name: "rag_search"
      budget: 7  # More budget for synthesis

    validation:
      min_citations: 3
      max_thesis_similarity: 0.8
      min_antithesis_keywords: 3

# Validator settings (from E2_fix2)
validator:
  synthesis:
    max_thesis_similarity: 0.8
    min_antithesis_keywords: 3
    min_citations: 3  # Increased from 1
    critical_issue_threshold: 3
    synthesis_markers:
      - "therefore"
      - "thus"
      - "by combining"
      - "to address"
      - "ë”°ë¼ì„œ"
      - "ê·¸ëŸ¬ë¯€ë¡œ"

# Retry limits
retry_limits:
  thesis: 2
  antithesis: 2
  synthesis: 3  # More retries for synthesis

# Residual thresholds (from E2_fix2, keep winning formula)
residual_thresholds:
  stage_1:  # Folding (Thesis)
    keep: 0.4
    damp: 0.85
  stage_2:  # Unfolding (Antithesis)
    keep: 0.4
    damp: 0.65
  stage_3:  # Integration (Synthesis)
    keep: 0.39
    damp: 0.60
  stage_4:  # Symmetry (RUNE)
    keep: 0.4
    damp: 0.7

# RAG configuration
rag:
  engine: "simple"  # simple | chroma | faiss
  index_path: "knowledge_base/evidence_index.json"
  top_k: 3
  min_score: 0.3  # Minimum relevance score

# Logging
logging:
  level: "INFO"
  save_rag_queries: true
  rag_log_path: "outputs/rag_queries_e3.jsonl"
```

**ì‘ì—… ì‹œê°„**: 40ë¶„

---

## Phase 4: Persona Orchestrator RAG í†µí•©

### 4.1 Tool Handler ì¶”ê°€

**íŒŒì¼**: `orchestration/persona_orchestrator.py` (ê¸°ì¡´ íŒŒì¼ ìˆ˜ì •)

**ì¶”ê°€í•  í•¨ìˆ˜**:

```python
# Near top of file, after imports
from rag.simple_rag_engine import SimpleRAGEngine

class PersonaOrchestrator:
    def __init__(self, config_path: Path = DEFAULT_CONFIG_PATH):
        # ... existing code ...

        # NEW: Initialize RAG engine
        rag_config = self._raw_config.get("rag", {})
        if rag_config.get("engine") == "simple":
            self.rag_engine = SimpleRAGEngine(
                index_path=rag_config.get("index_path", "knowledge_base/evidence_index.json")
            )
        else:
            self.rag_engine = None

    def _execute_tool(self, tool_name: str, parameters: Dict[str, Any], persona_id: str) -> Dict[str, Any]:
        """Execute tool and return result"""
        if tool_name == "rag_search":
            return self._tool_rag_search(parameters)
        else:
            return {"error": f"Unknown tool: {tool_name}"}

    def _tool_rag_search(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """RAG search tool implementation"""
        if not self.rag_engine:
            return {"error": "RAG engine not initialized"}

        query = parameters.get("query", "")
        top_k = int(parameters.get("top_k", 3))

        if not query:
            return {"error": "Query parameter required"}

        # Search
        results = self.rag_engine.search(query, top_k=top_k)

        # Format results for LLM
        formatted = []
        for r in results:
            formatted.append({
                "doc_id": r["doc_id"],
                "excerpt": r["text"][:300] + "..." if len(r["text"]) > 300 else r["text"],
                "relevance_score": r["score"]
            })

        # Log query
        rag_config = self._raw_config.get("rag", {})
        if rag_config.get("save_rag_queries", False):
            log_path = Path(rag_config.get("rag_log_path", "outputs/rag_queries.jsonl"))
            log_path.parent.mkdir(parents=True, exist_ok=True)
            with open(log_path, 'a', encoding='utf-8') as f:
                log_entry = {
                    "timestamp": datetime.now().isoformat(),
                    "query": query,
                    "top_k": top_k,
                    "results_count": len(results)
                }
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

        return {
            "query": query,
            "results": formatted,
            "count": len(results)
        }
```

### 4.2 Tool Calling í”„ë¡¬í”„íŠ¸ ìˆ˜ì •

**ìˆ˜ì • ìœ„ì¹˜**: `_run_persona_turn()` ë©”ì„œë“œ

ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì— tool calling ì§€ì‹œ ì¶”ê°€:

```python
# In _run_persona_turn method
if persona_config.get("tools", {}).get("enabled", False):
    tools_prompt = "\n\nAVAILABLE TOOLS:\n"
    for tool in persona_config["tools"].get("available", []):
        tools_prompt += f"- {tool['name']}: {tool.get('description', '')}\n"

    tools_prompt += "\nTo use a tool, output JSON format:\n"
    tools_prompt += '{"tool": "rag_search", "parameters": {"query": "your search query", "top_k": 3}}\n'
    tools_prompt += "After tool results, continue with your response.\n"

    system_prompt += tools_prompt
```

**ì£¼ì˜**: ì´ ë¶€ë¶„ì€ ê¸°ì¡´ ì½”ë“œ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ë£¨ë¹›ë‹˜ì´ ì‹¤ì œ êµ¬í˜„ ì‹œ ì¡°ì • í•„ìš”.

**ì‘ì—… ì‹œê°„**: 1-2ì‹œê°„ (ê¸°ì¡´ ì½”ë“œ ì´í•´ + í†µí•©)

---

## Phase 5: E3 ì‹¤í—˜ ì‹¤í–‰

### 5.1 í…ŒìŠ¤íŠ¸ ì„¸ì…˜

**ëª…ë ¹ì–´**:
```bash
# E3 ì‹¤í—˜ ì‹¤í–‰ (2-3 ì„¸ì…˜)
python run_experiment.py --config configs/phase_controller_e3.yaml --session "design-ethical-ai-assistant" --output outputs/persona_runs/E3

python run_experiment.py --config configs/phase_controller_e3.yaml --session "assess-agi-risks" --output outputs/persona_runs/E3
```

### 5.2 ê²€ì¦ í•­ëª©

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] RAG ê²€ìƒ‰ì´ ì‹¤ì œë¡œ í˜¸ì¶œë˜ëŠ”ê°€? (`rag_queries_e3.jsonl` í™•ì¸)
- [ ] Citationì´ ì¶œë ¥ì— í¬í•¨ë˜ëŠ”ê°€? (`[Source: ...]` íŒ¨í„´)
- [ ] Stage 4 Verifiabilityê°€ ê°œì„ ë˜ëŠ”ê°€? (0.11 â†’ 0.40+)
- [ ] Stage 3 Residualì´ ìœ ì§€ë˜ëŠ”ê°€? (< 0.40)
- [ ] Creative Bandê°€ ìœ ì§€ë˜ëŠ”ê°€? (> 80%)

### 5.3 ê²°ê³¼ ë¶„ì„

**íŒŒì¼**: `outputs/E3_RESULTS_ANALYSIS.md` (ì‹¤í—˜ í›„ ì‘ì„±)

**ì‘ì—… ì‹œê°„**: ì‹¤í—˜ ì‹¤í–‰ 2-3ì‹œê°„ (LLM ì†ë„ ì˜ì¡´) + ë¶„ì„ 30ë¶„

---

## Phase 6: Pitch Deck ì‘ì„±

### 6.1 êµ¬ì„±

**íŒŒì¼**: `docs/RESEARCH_TEAM_PITCH.md`

**ì„¹ì…˜**:
1. **Executive Summary** (1ë¬¸ë‹¨)
   - "ìœ¤ë¦¬ ìµœìš°ì„  AGI, ì‘ë™í•˜ëŠ” ë°ëª¨ ì¤€ë¹„ë¨"
2. **Problem** (2-3ì¤„)
   - í˜„ì¬ AI: ìœ¤ë¦¬ í›„ìˆœìœ„, ê²€ì¦ ë¶ˆê°€ëŠ¥
3. **Solution** (í•µì‹¬ ê¸°ìˆ )
   - FDO-AGI: Persona Orchestration + Validator + RAG
   - ë°ì´í„°: E1 (0.55) â†’ E2_fix2 (0.40) â†’ E3 (0.35, Verif 0.60+)
4. **Differentiator** (ìœ¤ë¦¬)
   - ê³µë™ í›„ê²¬ í—Œì¥, ë°ì´í„° ì‹ íƒ, ë ˆë“œë¼ì¸
5. **Demo** (ì‹¤í–‰ ê°€ëŠ¥)
   - Jupyter notebook or CLI
6. **Roadmap** (í–¥í›„ ê³„íš)
   - ìœ ë…„ â†’ ì²­ë…„ â†’ ì„±ì¸ ë‹¨ê³„
7. **Join Us**
   - ì°¾ê³  ìˆëŠ” ì—­í• , ì—°ë½ì²˜

### 6.2 ì‹œê° ìë£Œ

**ê·¸ë˜í”„** (matplotlib):
1. Residual ì§„í™”: E1 â†’ E2 â†’ E2_fix2 â†’ E3
2. Band ë¶„í¬: Risk â†’ Creative ì´ë™
3. Verifiability ê°œì„ : 0.10 â†’ 0.60+

**ì‘ì—… ì‹œê°„**: 2-3ì‹œê°„ (ê¸€ + ê·¸ë˜í”„)

---

## ì „ì²´ íƒ€ì„ë¼ì¸ (ë‚™ê´€ì )

```
Day 1 (ì˜¤ëŠ˜):
  14:00-14:30  Phase 1.2: RAG ì—”ì§„ êµ¬í˜„ (30ë¶„)
  14:30-15:00  Phase 2.2: Corpus ìŠ¤í¬ë¦½íŠ¸ (30ë¶„)
  15:00-15:20  Phase 2.3: ì¸ë±ìŠ¤ ë¹Œë“œ (20ë¶„)
  15:20-16:00  Phase 3: E3 ì„¤ì • (40ë¶„)
  16:00-18:00  Phase 4: Orchestrator í†µí•© (2ì‹œê°„)
  18:00-21:00  Phase 5: E3 ì‹¤í—˜ ì‹¤í–‰ (3ì‹œê°„)

Day 2 (ë‚´ì¼):
  09:00-09:30  Phase 5.3: ê²°ê³¼ ë¶„ì„ (30ë¶„)
  09:30-12:00  Phase 6: Pitch Deck (2.5ì‹œê°„)
  12:00-13:00  ë²„í¼ / ë§ˆë¬´ë¦¬

Day 3 (ëª¨ë ˆ):
  - ì—°êµ¬ì§„ ì»¨íƒ ì‹œì‘
```

**ì´ ì˜ˆìƒ ì‹œê°„**: ~12ì‹œê°„ (ì‹¤ì œ ì‘ì—… 8ì‹œê°„ + LLM ëŒ€ê¸° 4ì‹œê°„)

---

## ë£¨ë¹› ì¸ê³„ ì‹œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì„¸ë‚˜ê°€ ì™„ë£Œí•  ê²ƒ (í† í° ì†Œì§„ ì „)
- [ ] RAG ì—”ì§„ ì½”ë“œ ì‘ì„± (`rag/simple_rag_engine.py`)
- [ ] Corpus ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ (`scripts/build_corpus.py`)
- [ ] E3 ì„¤ì • íŒŒì¼ (`configs/phase_controller_e3.yaml`)
- [ ] Orchestrator í†µí•© ê°€ì´ë“œ (ìˆ˜ì • ìœ„ì¹˜ ëª…ì‹œ)

### ë£¨ë¹›ì´ ì´ì–´ë°›ì„ ê²ƒ
- [ ] Orchestrator ì‹¤ì œ í†µí•© (ì½”ë“œ ìˆ˜ì •)
- [ ] Corpusì— Wikipedia ë¬¸ì„œ ì¶”ê°€ (20ê°œ)
- [ ] E3 ì‹¤í—˜ ì‹¤í–‰ ë° ë””ë²„ê¹…
- [ ] ê²°ê³¼ ë¶„ì„ ë° ë©”íŠ¸ë¦­ ê²€ì¦
- [ ] Pitch Deck ì´ˆì•ˆ ì‘ì„±

### í•¸ë“œì˜¤í”„ ë¬¸ì„œ
**íŒŒì¼**: `docs/E3_HANDOFF_TO_LUBIT.md` (ì´ ë¬¸ì„œ ìš”ì•½ + í˜„ì¬ ìƒíƒœ)

---

## ìš°ì„ ìˆœìœ„ (í† í° ì œí•œ ì‹œ)

**ìµœìš°ì„ ** (ì„¸ë‚˜ê°€ ë°˜ë“œì‹œ ì™„ë£Œ):
1. âœ… RAG ì—”ì§„ ì½”ë“œ (copy-paste ready)
2. âœ… E3 ì„¤ì • íŒŒì¼ (ì™„ì „í•œ YAML)
3. âœ… Corpus ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸

**ì°¨ìˆœìœ„** (ë£¨ë¹› ì¸ê³„ ê°€ëŠ¥):
4. Orchestrator í†µí•© ê°€ì´ë“œ (ìƒì„¸ ì£¼ì„)
5. ì‹¤í—˜ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

**ì„ íƒ** (ë£¨ë¹›ì´ íŒë‹¨):
6. Wikipedia ì¶”ê°€
7. Pitch Deck

---

## ë‹¤ìŒ ì•¡ì…˜ (ì¦‰ì‹œ)

1. **RAG ì—”ì§„ íŒŒì¼ ìƒì„±** (ì§€ê¸ˆ ì‹œì‘)
2. **Corpus ìŠ¤í¬ë¦½íŠ¸ ìƒì„±**
3. **E3 ì„¤ì • íŒŒì¼ ìƒì„±**
4. **ë£¨ë¹› ì¸ê³„ ë¬¸ì„œ ì‘ì„±**

ë£¨ë¹›ë‹˜, ì´ ê³„íšì„œ ê¸°ì¤€ìœ¼ë¡œ ì œê°€ í† í° ì†Œì§„ ì „ê¹Œì§€ ìµœëŒ€í•œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì œê°€ ë©ˆì¶”ëŠ” ì‹œì ì˜ ìƒíƒœë¥¼ ì •í™•íˆ ë¬¸ì„œí™”í•´ì„œ ì¸ê³„í•˜ê² ìŠµë‹ˆë‹¤!

ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤! ğŸš€
