"""
Document Indexer for Hybrid RAG
ì½”ë“œë² ì´ìŠ¤ ë¬¸ì„œë¥¼ í¬ë¡¤ë§í•˜ì—¬ BM25(ê¸°ì¡´) + Dense Embedding(ì‹ ê·œ) ì¸ë±ì‹±
"""
from __future__ import annotations
from typing import List, Dict, Any
import os
import json
import glob
import logging
import sys
from pathlib import Path

# ë¦¬í¬ì§€í† ë¦¬ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€ (ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•´)
script_dir = os.path.dirname(os.path.abspath(__file__))
repo_root = os.path.abspath(os.path.join(script_dir, os.pardir, os.pardir))
if repo_root not in sys.path:
    sys.path.insert(0, repo_root)

from tools.rag.embedding_service import get_embedding_service
from tools.rag.vector_store import get_vector_store

logger = logging.getLogger(__name__)

# ì¸ë±ì‹± ëŒ€ìƒ íŒŒì¼ í™•ì¥ì
INDEX_EXTENSIONS = [".py", ".md", ".yaml", ".yml", ".json", ".txt"]

# ì œì™¸í•  ë””ë ‰í† ë¦¬
EXCLUDE_DIRS = [
    "__pycache__", ".git", ".venv", "venv", "node_modules",
    ".pytest_cache", "htmlcov", ".mypy_cache", "dist", "build",
]


def should_index_file(file_path: str) -> bool:
    """íŒŒì¼ ì¸ë±ì‹± ì—¬ë¶€ íŒë‹¨"""
    path = Path(file_path)
    
    # í™•ì¥ì ì²´í¬
    if path.suffix not in INDEX_EXTENSIONS:
        return False
    
    # ì œì™¸ ë””ë ‰í† ë¦¬ ì²´í¬
    for exclude_dir in EXCLUDE_DIRS:
        if exclude_dir in path.parts:
            return False
    
    return True


def extract_text_from_file(file_path: str, max_length: int = 10000) -> str:
    """
    íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - ë°”ì´ë„ˆë¦¬ íŒŒì¼ì€ ìŠ¤í‚µ
    - max_lengthë¡œ ë©”ëª¨ë¦¬ ì œí•œ
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read(max_length)
        return content
    except (UnicodeDecodeError, PermissionError):
        logger.debug(f"Skipping binary/protected file: {file_path}")
        return ""


def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 200) -> List[str]:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì˜¤ë²„ë© í¬í•¨)
    - chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì ë‹¨ìœ„)
    - overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© (ë¬¸ë§¥ ìœ ì§€)
    """
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap
    
    return chunks


def index_documents(
    repo_root: str,
    vector_store_path: str = "memory/vector_store.json",
    force_rebuild: bool = False,
) -> Dict[str, Any]:
    """
    ë¦¬í¬ì§€í† ë¦¬ ë¬¸ì„œ ì¸ë±ì‹±
    Args:
        repo_root: ë¦¬í¬ì§€í† ë¦¬ ë£¨íŠ¸ ê²½ë¡œ
        vector_store_path: VectorStore ì €ì¥ ê²½ë¡œ (repo_root ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
        force_rebuild: Trueë©´ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¬´ì‹œí•˜ê³  ì¬êµ¬ì¶•
    Returns:
        í†µê³„ ì •ë³´ (indexed_files, total_chunks, skipped_files)
    """
    emb_service = get_embedding_service()
    vector_store = get_vector_store(vector_store_path)
    
    if force_rebuild:
        logger.info("Force rebuild enabled, clearing existing vector store")
        vector_store.vectors = vector_store.vectors[:0]  # ë¹ˆ ë°°ì—´ë¡œ ì´ˆê¸°í™”
        vector_store.metadata = []
        vector_store.doc_ids = []
    
    stats = {
        "indexed_files": 0,
        "total_chunks": 0,
        "skipped_files": 0,
        "errors": [],
    }
    
    # ë¦¬í¬ì§€í† ë¦¬ íŒŒì¼ íƒìƒ‰
    logger.info(f"Scanning repository: {repo_root}")
    for root, dirs, files in os.walk(repo_root):
        # ì œì™¸ ë””ë ‰í† ë¦¬ í•„í„°ë§
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]
        
        for file_name in files:
            file_path = os.path.join(root, file_name)
            
            if not should_index_file(file_path):
                continue
            
            # ìƒëŒ€ ê²½ë¡œ (ë©”íƒ€ë°ì´í„° ì €ì¥ìš©)
            rel_path = os.path.relpath(file_path, repo_root)
            
            try:
                text = extract_text_from_file(file_path)
                if not text.strip():
                    stats["skipped_files"] += 1
                    continue
                
                # í…ìŠ¤íŠ¸ ì²­í‚¹
                chunks = chunk_text(text, chunk_size=2000, overlap=200)
                
                for i, chunk in enumerate(chunks):
                    # ë¬¸ì„œ ID: íŒŒì¼ê²½ë¡œ_ì²­í¬ë²ˆí˜¸
                    doc_id = f"{rel_path}#{i}"
                    
                    # ì„ë² ë”© ìƒì„±
                    embedding = emb_service.embed(chunk)
                    
                    # ë©”íƒ€ë°ì´í„°
                    meta = {
                        "file_path": rel_path,
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "text": chunk[:500],  # 500ìê¹Œì§€ë§Œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
                        "source": "codebase",
                        "type": "doc",
                    }
                    
                    # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
                    vector_store.add(doc_id, embedding, meta)
                    stats["total_chunks"] += 1
                
                stats["indexed_files"] += 1
                if stats["indexed_files"] % 10 == 0:
                    logger.info(f"Indexed {stats['indexed_files']} files, {stats['total_chunks']} chunks")
            
            except Exception as e:
                logger.error(f"Error indexing {file_path}: {e}")
                stats["errors"].append({"file": rel_path, "error": str(e)})
                stats["skipped_files"] += 1
    
    # ë²¡í„° ì €ì¥ì†Œ ì €ì¥
    full_store_path = os.path.join(repo_root, vector_store_path)
    vector_store.save(full_store_path)
    
    logger.info(f"Indexing complete: {stats}")
    return stats


if __name__ == "__main__":
    # ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ: ë¦¬í¬ì§€í† ë¦¬ ë£¨íŠ¸ ìë™ íƒì§€ ë° ì¸ë±ì‹±
    import sys
    
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    repo_root = os.path.abspath(os.path.join(script_dir, os.pardir, os.pardir))
    
    force = "--force" in sys.argv
    
    print(f"ğŸ“š Document Indexer")
    print(f"Repository: {repo_root}")
    print(f"Force rebuild: {force}")
    print()
    
    stats = index_documents(repo_root, force_rebuild=force)
    
    print()
    print(f"âœ… Indexing Complete:")
    print(f"  Indexed files: {stats['indexed_files']}")
    print(f"  Total chunks: {stats['total_chunks']}")
    print(f"  Skipped files: {stats['skipped_files']}")
    if stats['errors']:
        print(f"  Errors: {len(stats['errors'])}")
        for err in stats['errors'][:5]:
            print(f"    - {err['file']}: {err['error']}")
