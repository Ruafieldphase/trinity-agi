#!/usr/bin/env python3
"""
A/B Testing Automation Tool for AGI Performance Optimization
ÍπÉÏΩîÏùò ÌîÑÎ°¨ÌîÑÌä∏ ÏïïÏ∂ï ÏµúÏ†ÅÌôî A/B ÌÖåÏä§Ìä∏ ÏûêÎèôÌôî
"""

import subprocess
import json
import time
import statistics
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import sys

sys.path.insert(0, str(Path(__file__).parent))
from metrics_collector import MetricsCollector
from slack_notifier import SlackNotifier


class ABTester:
    """A/B ÌÖåÏä§Ìä∏ ÏûêÎèôÌôî"""

    def __init__(self, repo_root: Optional[Path] = None):
        if repo_root is None:
            repo_root = Path(__file__).parent.parent

        self.repo_root = Path(repo_root)
        self.collector = MetricsCollector(repo_root)
        self.notifier = SlackNotifier()
        self.results = []

    def run_single_test(
        self,
        test_name: str,
        env_vars: Dict[str, str],
        task_title: str = "ab_test",
        task_goal: str = "AGI ÏÑ±Îä• ÌÖåÏä§Ìä∏"
    ) -> Dict[str, Any]:
        """
        Îã®Ïùº ÌÖåÏä§Ìä∏ Ïã§Ìñâ

        Args:
            test_name: ÌÖåÏä§Ìä∏ Ïù¥Î¶Ñ
            env_vars: ÌôòÍ≤ΩÎ≥ÄÏàò ÎîïÏÖîÎÑàÎ¶¨
            task_title: ÏûëÏóÖ Ï†úÎ™©
            task_goal: ÏûëÏóÖ Î™©Ìëú

        Returns:
            ÌÖåÏä§Ìä∏ Í≤∞Í≥º
        """
        print(f"\n{'='*60}")
        print(f"[Test] {test_name}")
        print(f"{'='*60}")

        # ÌôòÍ≤ΩÎ≥ÄÏàò Ï∂úÎ†•
        for key, value in env_vars.items():
            print(f"  {key}={value}")

        # ÏãúÏûë ÏãúÍ∞Ñ
        start_time = time.time()
        start_ts = datetime.now()

        # Ledger Ïù¥Î≤§Ìä∏ Ïàò Í∏∞Î°ù (before)
        ledger_path = self.repo_root / "memory" / "resonance_ledger.jsonl"
        if ledger_path.exists():
            with open(ledger_path, 'r', encoding='utf-8') as f:
                events_before = sum(1 for _ in f)
        else:
            events_before = 0

        # ÌÖåÏä§Ìä∏ Ïã§Ìñâ
        try:
            cmd = [
                "python", "-m", "scripts.run_task",
                "--title", task_title,
                "--goal", task_goal
            ]

            # ÌôòÍ≤ΩÎ≥ÄÏàò ÏÑ§Ï†ï
            import os
            env = os.environ.copy()
            env.update(env_vars)

            print(f"\n  ‚Üí Running: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                cwd=self.repo_root,
                env=env,
                capture_output=True,
                text=True,
                timeout=300  # 5Î∂Ñ ÌÉÄÏûÑÏïÑÏõÉ
            )

            success = result.returncode == 0
            print(f"  ‚Üí Exit code: {result.returncode}")

        except subprocess.TimeoutExpired:
            print(f"  -> [ERROR] Timeout (300s)")
            success = False
        except Exception as e:
            print(f"  -> [ERROR] Error: {e}")
            success = False

        # Ï¢ÖÎ£å ÏãúÍ∞Ñ
        end_time = time.time()
        duration = end_time - start_time

        # Ledger Ïù¥Î≤§Ìä∏ Ïàò Í∏∞Î°ù (after)
        if ledger_path.exists():
            with open(ledger_path, 'r', encoding='utf-8') as f:
                events_after = sum(1 for _ in f)
        else:
            events_after = 0

        new_events = events_after - events_before

        # Î©îÌä∏Î¶≠ ÏàòÏßë (ÏµúÍ∑º ÏÉùÏÑ±Îêú Ïù¥Î≤§Ìä∏Îßå)
        print(f"\n  ‚Üí Collecting metrics ({new_events} new events)...")
        time.sleep(2)  # Î©îÎ™®Î¶¨ flush ÎåÄÍ∏∞

        # ÏµúÍ∑º Ïù¥Î≤§Ìä∏ Í∏∞Î∞òÏúºÎ°ú Î©îÌä∏Î¶≠ Ï∂îÏ∂ú
        metrics = self._extract_metrics_from_recent_events(new_events)

        result_data = {
            'test_name': test_name,
            'env_vars': env_vars,
            'timestamp': start_ts.isoformat(),
            'duration_sec': round(duration, 2),
            'success': success,
            'new_events': new_events,
            'metrics': metrics
        }

        # Í≤∞Í≥º Ï∂úÎ†•
        print(f"\n  [Metrics]:")
        print(f"    - Confidence: {metrics.get('avg_confidence', 0):.3f}")
        print(f"    - Quality: {metrics.get('avg_quality', 0):.3f}")
        print(f"    - Second Pass Rate: {metrics.get('second_pass_rate', 0):.3f}")
        print(f"    - Duration: {duration:.2f}s")

        self.results.append(result_data)
        return result_data

    def _extract_metrics_from_recent_events(self, event_count: int) -> Dict[str, float]:
        """ÏµúÍ∑º NÍ∞ú Ïù¥Î≤§Ìä∏ÏóêÏÑú Î©îÌä∏Î¶≠ Ï∂îÏ∂ú"""
        ledger_path = self.repo_root / "memory" / "resonance_ledger.jsonl"

        if not ledger_path.exists():
            return {
                'avg_confidence': 0.0,
                'avg_quality': 0.0,
                'second_pass_rate': 0.0,
                'total_events': 0
            }

        # ÏµúÍ∑º NÍ∞ú Ïù¥Î≤§Ìä∏ ÏùΩÍ∏∞
        events = []
        with open(ledger_path, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()
            recent_lines = all_lines[-event_count:] if event_count > 0 else []

            for line in recent_lines:
                line = line.strip()
                if line:
                    try:
                        events.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue

        # Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        confidence_values = []
        quality_values = []
        second_pass_count = 0
        task_ids = set()

        for event in events:
            event_type = event.get('event')

            if event_type == 'meta_cognition':
                if 'confidence' in event:
                    confidence_values.append(event['confidence'])

            if event_type == 'eval':
                eval_data = event.get('eval', {})
                if 'quality' in eval_data and eval_data['quality'] is not None:
                    quality_values.append(eval_data['quality'])

            if event_type == 'second_pass':
                second_pass_count += 1

            task_id = event.get('task_id')
            if task_id:
                task_ids.add(task_id)

        return {
            'avg_confidence': round(statistics.mean(confidence_values), 3) if confidence_values else 0.0,
            'avg_quality': round(statistics.mean(quality_values), 3) if quality_values else 0.0,
            'second_pass_rate': round(second_pass_count / max(len(task_ids), 1), 3),
            'total_events': len(events),
            'total_tasks': len(task_ids)
        }

    def run_ab_test(
        self,
        config_a: Dict[str, str],
        config_b: Dict[str, str],
        iterations: int = 5,
        task_title: str = "ab_test",
        task_goal: str = "AGI ÏÑ±Îä• ÌÖåÏä§Ìä∏"
    ) -> Dict[str, Any]:
        """
        A/B ÌÖåÏä§Ìä∏ Ïã§Ìñâ

        Args:
            config_a: ÏÑ§Ï†ï A (ÌôòÍ≤ΩÎ≥ÄÏàò)
            config_b: ÏÑ§Ï†ï B (ÌôòÍ≤ΩÎ≥ÄÏàò)
            iterations: Í∞Å ÏÑ§Ï†ïÎãπ Ïã§Ìñâ ÌöüÏàò
            task_title: ÏûëÏóÖ Ï†úÎ™©
            task_goal: ÏûëÏóÖ Î™©Ìëú

        Returns:
            A/B ÎπÑÍµê Í≤∞Í≥º
        """
        print("\n" + "="*60)
        print("[A/B Testing Started]")
        print("="*60)
        print(f"Iterations: {iterations} per config")
        print(f"Total runs: {iterations * 2}")
        print()

        # Config A Ïã§Ìñâ
        print("\n[Running Config A...]")
        results_a = []
        for i in range(iterations):
            print(f"\n[A-{i+1}/{iterations}]")
            result = self.run_single_test(
                f"Config_A_Run_{i+1}",
                config_a,
                task_title,
                task_goal
            )
            results_a.append(result)
            time.sleep(5)  # Ïã§Ìñâ Í∞Ñ ÎåÄÍ∏∞

        # Config B Ïã§Ìñâ
        print("\n[Running Config B...]")
        results_b = []
        for i in range(iterations):
            print(f"\n[B-{i+1}/{iterations}]")
            result = self.run_single_test(
                f"Config_B_Run_{i+1}",
                config_b,
                task_title,
                task_goal
            )
            results_b.append(result)
            time.sleep(5)

        # ÌÜµÍ≥Ñ Î∂ÑÏÑù
        comparison = self._compare_results(config_a, config_b, results_a, results_b)

        # Í≤∞Í≥º Ï†ÄÏû•
        output_file = self.repo_root / "outputs" / f"ab_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        output_file.parent.mkdir(exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison, f, indent=2)

        print(f"\n[Results saved: {output_file}]")

        # Slack ÏïåÎ¶º Ï†ÑÏÜ°
        self._send_slack_notification(comparison)

        return comparison

    def _send_slack_notification(self, comparison: Dict[str, Any]):
        """A/B ÌÖåÏä§Ìä∏ Í≤∞Í≥ºÎ•º SlackÏúºÎ°ú ÏïåÎ¶º"""
        try:
            stats_a = comparison['stats_a']
            stats_b = comparison['stats_b']
            diff = comparison['difference']

            # Config Ïù¥Î¶Ñ
            config_a_val = comparison['config_a'].get('SYNTHESIS_SECTION_MAX_CHARS', '?')
            config_b_val = comparison['config_b'].get('SYNTHESIS_SECTION_MAX_CHARS', '?')

            # ÏäπÏûê Í≤∞Ï†ï
            winner = None
            if diff['quality']['absolute'] > 0.05:
                winner = f"Config B ({config_b_val})"
                reason = f"Quality +{diff['quality']['absolute']:.3f}"
            elif diff['quality']['absolute'] < -0.05:
                winner = f"Config A ({config_a_val})"
                reason = f"Quality {diff['quality']['absolute']:.3f}"

            blocks = [
                {
                    "type": "header",
                    "text": {
                        "type": "plain_text",
                        "text": "üî¨ A/B ÌÖåÏä§Ìä∏ ÏôÑÎ£å",
                        "emoji": True
                    }
                },
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"*ÏÑ§Ï†ï ÎπÑÍµê*\nConfig A: {config_a_val} vs Config B: {config_b_val}\n*Î∞òÎ≥µ ÌöüÏàò*: {comparison['iterations']}Ìöå"
                    }
                },
                {
                    "type": "divider"
                },
                {
                    "type": "section",
                    "fields": [
                        {
                            "type": "mrkdwn",
                            "text": f"*Quality*\nA: {stats_a['quality']['mean']:.3f} | B: {stats_b['quality']['mean']:.3f}\nÏ∞®Ïù¥: {diff['quality']['absolute']:+.3f}"
                        },
                        {
                            "type": "mrkdwn",
                            "text": f"*Confidence*\nA: {stats_a['confidence']['mean']:.3f} | B: {stats_b['confidence']['mean']:.3f}\nÏ∞®Ïù¥: {diff['confidence']['absolute']:+.3f}"
                        },
                        {
                            "type": "mrkdwn",
                            "text": f"*Duration*\nA: {stats_a['duration']['mean']:.1f}s | B: {stats_b['duration']['mean']:.1f}s\nÏ∞®Ïù¥: {diff['duration']['absolute']:+.1f}s"
                        }
                    ]
                }
            ]

            if winner:
                blocks.append({
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"üèÜ *ÏäπÏûê*: {winner}\n*Ïù¥Ïú†*: {reason}"
                    }
                })

            self.notifier.send_message(
                f"A/B ÌÖåÏä§Ìä∏ ÏôÑÎ£å: {config_a_val} vs {config_b_val}",
                blocks
            )
        except Exception as e:
            print(f"Slack notification failed: {e}")

    def _compare_results(
        self,
        config_a: Dict[str, str],
        config_b: Dict[str, str],
        results_a: List[Dict],
        results_b: List[Dict]
    ) -> Dict[str, Any]:
        """Í≤∞Í≥º ÎπÑÍµê Î∞è ÌÜµÍ≥Ñ Î∂ÑÏÑù"""

        def extract_metrics(results):
            confidence = [r['metrics']['avg_confidence'] for r in results]
            quality = [r['metrics']['avg_quality'] for r in results]
            second_pass = [r['metrics']['second_pass_rate'] for r in results]
            duration = [r['duration_sec'] for r in results]

            return {
                'confidence': {
                    'mean': round(statistics.mean(confidence), 3),
                    'stdev': round(statistics.stdev(confidence), 3) if len(confidence) > 1 else 0,
                    'min': round(min(confidence), 3),
                    'max': round(max(confidence), 3),
                    'values': confidence
                },
                'quality': {
                    'mean': round(statistics.mean(quality), 3),
                    'stdev': round(statistics.stdev(quality), 3) if len(quality) > 1 else 0,
                    'min': round(min(quality), 3),
                    'max': round(max(quality), 3),
                    'values': quality
                },
                'second_pass_rate': {
                    'mean': round(statistics.mean(second_pass), 3),
                    'stdev': round(statistics.stdev(second_pass), 3) if len(second_pass) > 1 else 0,
                    'min': round(min(second_pass), 3),
                    'max': round(max(second_pass), 3),
                    'values': second_pass
                },
                'duration': {
                    'mean': round(statistics.mean(duration), 2),
                    'stdev': round(statistics.stdev(duration), 2) if len(duration) > 1 else 0,
                    'min': round(min(duration), 2),
                    'max': round(max(duration), 2),
                    'values': duration
                }
            }

        stats_a = extract_metrics(results_a)
        stats_b = extract_metrics(results_b)

        # Ï∞®Ïù¥ Í≥ÑÏÇ∞
        def calc_diff(a_mean, b_mean):
            diff = b_mean - a_mean
            pct = (diff / a_mean * 100) if a_mean != 0 else 0
            return {
                'absolute': round(diff, 3),
                'percentage': round(pct, 1)
            }

        comparison = {
            'timestamp': datetime.now().isoformat(),
            'config_a': config_a,
            'config_b': config_b,
            'iterations': len(results_a),
            'stats_a': stats_a,
            'stats_b': stats_b,
            'difference': {
                'confidence': calc_diff(stats_a['confidence']['mean'], stats_b['confidence']['mean']),
                'quality': calc_diff(stats_a['quality']['mean'], stats_b['quality']['mean']),
                'second_pass_rate': calc_diff(stats_a['second_pass_rate']['mean'], stats_b['second_pass_rate']['mean']),
                'duration': calc_diff(stats_a['duration']['mean'], stats_b['duration']['mean'])
            },
            'raw_results': {
                'config_a': results_a,
                'config_b': results_b
            }
        }

        # Í≤∞Í≥º Ï∂úÎ†•
        self._print_comparison(comparison)

        return comparison

    def _print_comparison(self, comparison: Dict[str, Any]):
        """ÎπÑÍµê Í≤∞Í≥º Ï∂úÎ†•"""
        print("\n" + "="*60)
        print("[A/B Test Results]")
        print("="*60)

        stats_a = comparison['stats_a']
        stats_b = comparison['stats_b']
        diff = comparison['difference']

        print("\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
        print("‚îÇ Metric          ‚îÇ Config A     ‚îÇ Config B     ‚îÇ Difference   ‚îÇ")
        print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")

        # Confidence
        print(f"‚îÇ Confidence      ‚îÇ {stats_a['confidence']['mean']:.3f} ¬±{stats_a['confidence']['stdev']:.3f} "
              f"‚îÇ {stats_b['confidence']['mean']:.3f} ¬±{stats_b['confidence']['stdev']:.3f} "
              f"‚îÇ {diff['confidence']['absolute']:+.3f} ({diff['confidence']['percentage']:+.1f}%) ‚îÇ")

        # Quality
        print(f"‚îÇ Quality         ‚îÇ {stats_a['quality']['mean']:.3f} ¬±{stats_a['quality']['stdev']:.3f} "
              f"‚îÇ {stats_b['quality']['mean']:.3f} ¬±{stats_b['quality']['stdev']:.3f} "
              f"‚îÇ {diff['quality']['absolute']:+.3f} ({diff['quality']['percentage']:+.1f}%) ‚îÇ")

        # Second Pass Rate
        print(f"‚îÇ Second Pass     ‚îÇ {stats_a['second_pass_rate']['mean']:.3f} ¬±{stats_a['second_pass_rate']['stdev']:.3f} "
              f"‚îÇ {stats_b['second_pass_rate']['mean']:.3f} ¬±{stats_b['second_pass_rate']['stdev']:.3f} "
              f"‚îÇ {diff['second_pass_rate']['absolute']:+.3f} ({diff['second_pass_rate']['percentage']:+.1f}%) ‚îÇ")

        # Duration
        print(f"‚îÇ Duration (s)    ‚îÇ {stats_a['duration']['mean']:.1f} ¬±{stats_a['duration']['stdev']:.1f}   "
              f"‚îÇ {stats_b['duration']['mean']:.1f} ¬±{stats_b['duration']['stdev']:.1f}   "
              f"‚îÇ {diff['duration']['absolute']:+.1f} ({diff['duration']['percentage']:+.1f}%)  ‚îÇ")

        print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")

        # ÏäπÏûê Í≤∞Ï†ï
        print("\n[Winner]:")
        winners = []
        if diff['quality']['absolute'] > 0:
            winners.append("Quality: Config B")
        elif diff['quality']['absolute'] < 0:
            winners.append("Quality: Config A")

        if diff['confidence']['absolute'] > 0:
            winners.append("Confidence: Config B")
        elif diff['confidence']['absolute'] < 0:
            winners.append("Confidence: Config A")

        if diff['duration']['absolute'] < 0:  # Îπ†Î•∏Í≤å Ï¢ãÏùå
            winners.append("Speed: Config B")
        elif diff['duration']['absolute'] > 0:
            winners.append("Speed: Config A")

        if winners:
            for w in winners:
                print(f"  - {w}")
        else:
            print("  - No significant difference")


def main():
    """CLI"""
    import argparse

    parser = argparse.ArgumentParser(description='AGI A/B Testing Tool')
    parser.add_argument(
        '--iterations', '-n',
        type=int,
        default=5,
        help='Number of iterations per config (default: 5)'
    )

    args = parser.parse_args()

    tester = ABTester()

    # ÏòàÏãú: SYNTHESIS_SECTION_MAX_CHARS 900 vs 800 ÎπÑÍµê
    config_a = {
        'SYNTHESIS_SECTION_MAX_CHARS': '900'
    }

    config_b = {
        'SYNTHESIS_SECTION_MAX_CHARS': '800'
    }

    print("\n[A/B Test: SYNTHESIS_SECTION_MAX_CHARS]")
    print(f"  Config A: 900 (Í∏∞Î≥∏Í∞í)")
    print(f"  Config B: 800 (ÏµúÏ†ÅÌôî)")
    print(f"  Iterations: {args.iterations}\n")

    result = tester.run_ab_test(
        config_a,
        config_b,
        iterations=args.iterations,
        task_title="ab_test_synthesis",
        task_goal="AGI ÏûêÍ∏∞ÍµêÏ†ï Î£®ÌîÑ ÏÑ§Î™Ö 3Î¨∏Ïû•"
    )


if __name__ == '__main__':
    main()
