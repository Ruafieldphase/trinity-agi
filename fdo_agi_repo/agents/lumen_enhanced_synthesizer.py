#!/usr/bin/env python3
"""
lumen_enhanced_synthesizer.py
ë£¨ë©˜ (í•©/åˆ) ê°•í™”íŒ - í˜ë¥´ì†Œë‚˜ì™€ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í†µí•©

ì—­í• : "ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?" - í†µí•©ê³¼ ì¡°í™”
- ì •(ë£¨ì•„)ì˜ ê´€ì°° í†µí•©
- ë°˜(ì—˜ë¡œ)ì˜ ê²€ì¦ í†µí•©
- í˜ë¥´ì†Œë‚˜ ëª¨ë¸ í•™ìŠµ í†µí•©
- ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
- í•©(ë£¨ë©˜)ì˜ ì‹¤í–‰ ê°€ëŠ¥í•œ í†µì°° ìƒì„±
"""

import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import math

class LumenEnhancedSynthesizer:
    """ë£¨ë©˜ (í•©) ê°•í™”íŒ - ë‹¤ì°¨ì› í†µí•©ì"""
    
    def __init__(self, lua_path: str, elo_path: str):
        self.lua_path = Path(lua_path)
        self.elo_path = Path(elo_path)
        
        self.lua_data = self._load_json(self.lua_path)
        self.elo_data = self._load_json(self.elo_path)
        
        # í™•ì¥ ë°ì´í„° ì†ŒìŠ¤
        self.binoche_persona = self._try_load_persona()
        self.conversation_context = self._try_load_conversations()
        self.ensemble_metrics = self._try_load_ensemble()
    
    def _load_json(self, path: Path) -> Dict[str, Any]:
        """JSON ë¡œë“œ (BOM ì²˜ë¦¬)"""
        if not path.exists():
            raise FileNotFoundError(f"File not found: {path}")
        
        with open(path, 'r', encoding='utf-8-sig') as f:
            return json.load(f)
    
    def _try_load_persona(self) -> Optional[Dict[str, Any]]:
        """ë¹„ë…¸ìŠˆ í˜ë¥´ì†Œë‚˜ ëª¨ë¸ ë¡œë“œ ì‹œë„"""
        persona_paths = [
            Path('fdo_agi_repo/outputs/binoche_persona.json'),
            Path('outputs/binoche_persona.json')
        ]
        
        for p in persona_paths:
            if p.exists():
                try:
                    with open(p, 'r', encoding='utf-8-sig') as f:
                        return json.load(f)
                except:
                    pass
        return None
    
    def _try_load_conversations(self) -> Optional[Dict[str, Any]]:
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì‹œë„"""
        conv_paths = [
            Path('outputs/conversation_timeline_2025-10-27.json'),
            Path('outputs/conversation_analysis_latest.json')
        ]
        
        for p in conv_paths:
            if p.exists():
                try:
                    with open(p, 'r', encoding='utf-8-sig') as f:
                        return json.load(f)
                except:
                    pass
        return None
    
    def _try_load_ensemble(self) -> Optional[Dict[str, Any]]:
        """ì•™ìƒë¸” ë©”íŠ¸ë¦­ ë¡œë“œ ì‹œë„"""
        ensemble_paths = [
            Path('fdo_agi_repo/outputs/ensemble_success_metrics.json'),
            Path('outputs/ensemble_success_metrics.json')
        ]
        
        for p in ensemble_paths:
            if p.exists():
                try:
                    with open(p, 'r', encoding='utf-8-sig') as f:
                        return json.load(f)
                except:
                    pass
        return None
    
    def synthesize(self) -> Dict[str, Any]:
        """í™•ì¥ëœ ì •ë°˜í•© í†µí•©"""
        print("ğŸŒŸ ë£¨ë©˜ (í•©) ê°•í™”íŒ - ë‹¤ì°¨ì› í†µí•© ì‹œì‘")
        print("   í•©(åˆ): ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?")
        print()
        
        # 1. ì •(æ­£) - ë£¨ì•„ì˜ ê´€ì°° ìš”ì•½
        lua_summary = self._summarize_lua()
        print("ğŸ“‹ ì •(æ­£) - ë£¨ì•„ì˜ ê´€ì°°:")
        print(f"   ì´ë²¤íŠ¸: {lua_summary['total_events']}ê°œ")
        print(f"   ì´ë²¤íŠ¸ íƒ€ì…: {lua_summary['event_types']}ê°œ")
        print(f"   í™œë™ Task: {lua_summary['unique_tasks']}ê°œ")
        print()
        
        # 2. ë°˜(å) - ì—˜ë¡œì˜ ê²€ì¦ ìš”ì•½
        elo_summary = self._summarize_elo()
        print("ğŸ”¬ ë°˜(å) - ì—˜ë¡œì˜ ê²€ì¦:")
        print(f"   ì—”íŠ¸ë¡œí”¼: {elo_summary['entropy']:.3f}")
        print(f"   ì •ë³´ ë°€ë„: {elo_summary['information_density']:.1%}")
        print(f"   ì´ìƒì¹˜: {elo_summary['anomaly_count']}ê±´")
        print()
        
        # 3. ğŸ†• í˜ë¥´ì†Œë‚˜ ë¶„ì„
        persona_summary = self._analyze_persona()
        if persona_summary:
            print("ğŸ­ í˜ë¥´ì†Œë‚˜ ë¶„ì„:")
            print(f"   ëª¨ë¸ íƒ€ì…: {persona_summary.get('model_type', 'N/A')}")
            print(f"   í•™ìŠµ íŒ¨í„´: {persona_summary.get('learned_patterns', 0)}ê°œ")
            print()
        
        # 4. ğŸ†• ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        conversation_summary = self._analyze_conversations()
        if conversation_summary:
            print("ğŸ’¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸:")
            print(f"   ëŒ€í™” ì„¸ì…˜: {conversation_summary.get('session_count', 0)}ê°œ")
            print(f"   ì£¼ìš” ì£¼ì œ: {', '.join(conversation_summary.get('topics', [])[:3])}")
            print()
        
        # 5. ğŸ†• í™•ì¥ ì •ë³´ì´ë¡  ë©”íŠ¸ë¦­
        extended_metrics = self._calculate_extended_metrics()
        print("ğŸ“Š í™•ì¥ ë©”íŠ¸ë¦­:")
        print(f"   ìƒí˜¸ì •ë³´ëŸ‰: {extended_metrics['mutual_information']:.3f} bits")
        print(f"   ë³µì¡ë„ ì§€ìˆ˜: {extended_metrics['complexity_index']:.3f}")
        print()
        
        # 6. í•©(åˆ) - í†µí•© í†µì°°
        insights = self._generate_enhanced_insights(
            lua_summary, elo_summary, persona_summary, 
            conversation_summary, extended_metrics
        )
        print("ğŸ’¡ í•©(åˆ) - í†µí•© í†µì°°:")
        for insight in insights:
            priority = insight['priority'].upper()
            print(f"   [{priority}] {insight['message']}")
        print()
        
        # 7. ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­
        recommendations = self._generate_enhanced_recommendations(insights)
        print("âœ… ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        print()
        
        # ê²°ê³¼ ì·¨í•©
        result = {
            'synthesizer': 'lumen_enhanced',
            'version': '2.0',
            'persona': 'í•©(åˆ)',
            'role': 'ë‹¤ì°¨ì› í†µí•©',
            'philosophy': 'ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?',
            'timestamp': datetime.now().isoformat(),
            'sources': {
                'lua': str(self.lua_path),
                'elo': str(self.elo_path),
                'binoche_persona': bool(self.binoche_persona),
                'conversation_context': bool(self.conversation_context),
                'ensemble_metrics': bool(self.ensemble_metrics)
            },
            'synthesis': {
                'lua_summary': lua_summary,
                'elo_summary': elo_summary,
                'persona_summary': persona_summary,
                'conversation_summary': conversation_summary,
                'extended_metrics': extended_metrics,
                'insights': insights,
                'recommendations': recommendations
            },
            'dialectic': {
                'thesis': 'ì •(æ­£) - ê´€ì°°ëœ ì‹œìŠ¤í…œ ìƒíƒœ',
                'antithesis': 'ë°˜(å) - ê²€ì¦ëœ í’ˆì§ˆ ì´ìŠˆ',
                'synthesis': 'í•©(åˆ) - ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©í–¥',
                'extension': 'í™•ì¥(æ“´) - í˜ë¥´ì†Œë‚˜ì™€ ëŒ€í™” í†µí•©'
            }
        }
        
        return result
    
    def _summarize_lua(self) -> Dict[str, Any]:
        """ë£¨ì•„ ê´€ì°° ìš”ì•½"""
        quality_metrics = self.lua_data.get('quality_metrics') or {}
        latency_metrics = self.lua_data.get('latency_metrics') or {}
        
        return {
            'total_events': self.lua_data.get('events_in_window', 0),
            'event_types': len(self.lua_data.get('event_types', {})),
            'unique_tasks': self.lua_data.get('unique_tasks', 0),
            'quality_count': quality_metrics.get('count', 0),
            'quality_avg': quality_metrics.get('average', 0),
            'latency_count': latency_metrics.get('count', 0)
        }
    
    def _summarize_elo(self) -> Dict[str, Any]:
        """ì—˜ë¡œ ê²€ì¦ ìš”ì•½"""
        it = self.elo_data.get('information_theory', {})
        return {
            'entropy': it.get('entropy', {}).get('value', 0),
            'entropy_normalized': it.get('entropy', {}).get('normalized', 0),
            'information_density': it.get('information_density', {}).get('value', 0),
            'anomaly_count': len(self.elo_data.get('anomalies', [])),
            'consistency': self.elo_data.get('consistency', {}).get('overall', 'unknown'),
            'verdict': self.elo_data.get('verdict', '')
        }
    
    def _analyze_persona(self) -> Optional[Dict[str, Any]]:
        """í˜ë¥´ì†Œë‚˜ ëª¨ë¸ ë¶„ì„"""
        if not self.binoche_persona:
            return None
        
        patterns = self.binoche_persona.get('learned_patterns', [])
        
        return {
            'model_type': 'binoche_ensemble',
            'learned_patterns': len(patterns),
            'confidence_avg': sum(p.get('confidence', 0) for p in patterns) / len(patterns) if patterns else 0,
            'style_preference': self.binoche_persona.get('style_preference', {}),
            'last_updated': self.binoche_persona.get('last_updated', '')
        }
    
    def _analyze_conversations(self) -> Optional[Dict[str, Any]]:
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        if not self.conversation_context:
            return None
        
        # ëŒ€í™” ì„¸ì…˜ ìˆ˜ ì¶”ì •
        sessions = self.conversation_context.get('sessions', [])
        if not sessions and 'events' in self.conversation_context:
            # ì´ë²¤íŠ¸ì—ì„œ ì„¸ì…˜ ì¶”ì •
            sessions = [self.conversation_context]
        
        # ì£¼ì œ ì¶”ì¶œ
        topics = []
        if 'topics' in self.conversation_context:
            topics = self.conversation_context['topics']
        elif sessions:
            for session in sessions[:5]:  # ìµœê·¼ 5ê°œë§Œ
                if 'topic' in session:
                    topics.append(session['topic'])
        
        return {
            'session_count': len(sessions) if sessions else 1,
            'topics': topics[:5],  # ìƒìœ„ 5ê°œ
            'total_messages': sum(s.get('message_count', 0) for s in sessions) if sessions else 0
        }
    
    def _calculate_extended_metrics(self) -> Dict[str, Any]:
        """í™•ì¥ ì •ë³´ì´ë¡  ë©”íŠ¸ë¦­"""
        
        # 1. ìƒí˜¸ì •ë³´ëŸ‰ (Mutual Information)
        # I(X;Y) = H(X) + H(Y) - H(X,Y)
        entropy = self.elo_data.get('information_theory', {}).get('entropy', {}).get('value', 0)
        
        # ì´ë²¤íŠ¸ íƒ€ì…ê³¼ Task ê°„ì˜ ìƒí˜¸ì •ë³´ëŸ‰ ê·¼ì‚¬
        event_types = len(self.lua_data.get('event_types', {}))
        unique_tasks = self.lua_data.get('unique_tasks', 0)
        
        if event_types > 0 and unique_tasks > 0:
            # ê°„ë‹¨í•œ ê·¼ì‚¬: H(types) + H(tasks) - H(joint)
            h_types = math.log2(event_types)
            h_tasks = math.log2(unique_tasks)
            h_joint = entropy  # ê²°í•© ì—”íŠ¸ë¡œí”¼ ê·¼ì‚¬
            mutual_info = h_types + h_tasks - h_joint
        else:
            mutual_info = 0
        
        # 2. ë³µì¡ë„ ì§€ìˆ˜ (Complexity Index)
        # C = H * D * (1 - A)
        # H: ì—”íŠ¸ë¡œí”¼, D: ì •ë³´ë°€ë„, A: ì´ìƒì¹˜ ë¹„ìœ¨
        info_density = self.elo_data.get('information_theory', {}).get('information_density', {}).get('value', 0)
        anomaly_count = len(self.elo_data.get('anomalies', []))
        total_events = self.lua_data.get('events_in_window', 1)
        anomaly_ratio = anomaly_count / total_events if total_events > 0 else 0
        
        complexity_index = entropy * info_density * (1 - anomaly_ratio)
        
        # 3. í’ˆì§ˆ-ì—”íŠ¸ë¡œí”¼ ìƒê´€ê´€ê³„
        quality_metrics = self.lua_data.get('quality_metrics') or {}
        quality_avg = quality_metrics.get('average', 0)
        quality_entropy_correlation = quality_avg * entropy if quality_avg > 0 else 0
        
        return {
            'mutual_information': mutual_info,
            'complexity_index': complexity_index,
            'quality_entropy_correlation': quality_entropy_correlation,
            'anomaly_ratio': anomaly_ratio
        }
    
    def _generate_enhanced_insights(
        self, 
        lua: Dict, 
        elo: Dict, 
        persona: Optional[Dict],
        conversation: Optional[Dict],
        extended: Dict
    ) -> List[Dict[str, Any]]:
        """í™•ì¥ëœ í†µí•© í†µì°° ìƒì„±"""
        insights = []
        
        # ê¸°ë³¸ í†µì°° (ê¸°ì¡´)
        if elo['information_density'] < 0.3:
            insights.append({
                'priority': 'high',
                'category': 'data_quality',
                'message': f"ì •ë³´ ë°€ë„ê°€ ë‚®ìŒ ({elo['information_density']:.1%}). ë” ë§ì€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í•„ìš”",
                'source': 'elo',
                'actionable': True
            })
        
        # ğŸ†• í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ í†µì°°
        if persona and persona.get('learned_patterns', 0) > 0:
            confidence = persona.get('confidence_avg', 0)
            if confidence > 0.8:
                insights.append({
                    'priority': 'info',
                    'category': 'persona',
                    'message': f"í˜ë¥´ì†Œë‚˜ ëª¨ë¸ ì‹ ë¢°ë„ ë†’ìŒ ({confidence:.2f}). ììœ¨ ì˜ì‚¬ê²°ì • ê°€ëŠ¥",
                    'source': 'persona',
                    'actionable': False
                })
            elif confidence < 0.6:
                insights.append({
                    'priority': 'medium',
                    'category': 'persona',
                    'message': f"í˜ë¥´ì†Œë‚˜ ëª¨ë¸ ì‹ ë¢°ë„ ë‚®ìŒ ({confidence:.2f}). ì¶”ê°€ í•™ìŠµ í•„ìš”",
                    'source': 'persona',
                    'actionable': True
                })
        
        # ğŸ†• ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í†µì°°
        if conversation:
            session_count = conversation.get('session_count', 0)
            if session_count > 10:
                insights.append({
                    'priority': 'info',
                    'category': 'conversation',
                    'message': f"í’ë¶€í•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ({session_count}ê°œ ì„¸ì…˜). íŒ¨í„´ í•™ìŠµì— í™œìš© ê°€ëŠ¥",
                    'source': 'conversation',
                    'actionable': True
                })
        
        # ğŸ†• í™•ì¥ ë©”íŠ¸ë¦­ ê¸°ë°˜ í†µì°°
        if extended['complexity_index'] > 2.0:
            insights.append({
                'priority': 'low',
                'category': 'complexity',
                'message': f"ì‹œìŠ¤í…œ ë³µì¡ë„ ë†’ìŒ (CI={extended['complexity_index']:.2f}). ë‹¨ìˆœí™” ê³ ë ¤",
                'source': 'extended_metrics',
                'actionable': True
            })
        
        if extended['mutual_information'] < 1.0:
            insights.append({
                'priority': 'medium',
                'category': 'correlation',
                'message': f"ì´ë²¤íŠ¸-Task ìƒê´€ê´€ê³„ ì•½í•¨ (MI={extended['mutual_information']:.2f}). ì—°ê²°ì„± ê°•í™” í•„ìš”",
                'source': 'extended_metrics',
                'actionable': True
            })
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¶€ì¡±
        if lua['quality_count'] < lua['total_events'] * 0.5:
            coverage = lua['quality_count'] / lua['total_events'] if lua['total_events'] > 0 else 0
            insights.append({
                'priority': 'medium',
                'category': 'monitoring',
                'message': f"í’ˆì§ˆ ë©”íŠ¸ë¦­ ì»¤ë²„ë¦¬ì§€ ë‚®ìŒ ({coverage:.1%}). í‰ê°€ ê°•í™” í•„ìš”",
                'source': 'lua',
                'actionable': True
            })
        
        # ì´ìƒì¹˜ ë°œê²¬
        if elo['anomaly_count'] > 0:
            insights.append({
                'priority': 'medium',
                'category': 'anomaly',
                'message': f"{elo['anomaly_count']}ê±´ì˜ ì´ìƒì¹˜ íƒì§€. ìƒì„¸ ì¡°ì‚¬ í•„ìš”",
                'source': 'elo',
                'actionable': True
            })
        
        # ê¸ì •ì  ì‹ í˜¸
        if elo['consistency'] in ['consistent', 'mostly_consistent']:
            insights.append({
                'priority': 'info',
                'category': 'positive',
                'message': "ì‹œìŠ¤í…œ ì¼ê´€ì„± ì–‘í˜¸. ì•ˆì •ì  ìš´ì˜ ì¤‘",
                'source': 'elo',
                'actionable': False
            })
        
        return insights
    
    def _generate_enhanced_recommendations(self, insights: List[Dict]) -> List[str]:
        """í™•ì¥ëœ ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        for insight in insights:
            if not insight.get('actionable', True):
                continue
            
            if insight['category'] == 'data_quality':
                recommendations.append(
                    "ëª¨ë“  ì£¼ìš” ì´ë²¤íŠ¸ì— quality/latency ë©”íŠ¸ë¦­ ì¶”ê°€"
                )
            elif insight['category'] == 'monitoring':
                recommendations.append(
                    "í‰ê°€(eval) ì´ë²¤íŠ¸ ë¹ˆë„ ì¦ê°€ - í˜„ì¬ ëŒ€ë¹„ 2ë°°"
                )
            elif insight['category'] == 'anomaly':
                recommendations.append(
                    "ì´ìƒì¹˜ ì›ì¸ ë¶„ì„ ë° ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•"
                )
            elif insight['category'] == 'persona':
                if 'ì‹ ë¢°ë„ ë‚®ìŒ' in insight['message']:
                    recommendations.append(
                        "ë¹„ë…¸ìŠˆ í˜ë¥´ì†Œë‚˜ ëª¨ë¸ ì¬í•™ìŠµ - ìµœê·¼ 1ì£¼ì¼ ë°ì´í„° í™œìš©"
                    )
            elif insight['category'] == 'conversation':
                recommendations.append(
                    "ëŒ€í™” íŒ¨í„´ì„ í˜ë¥´ì†Œë‚˜ í•™ìŠµì— í†µí•© - ìë™ í”¼ë“œë°± ë£¨í”„ êµ¬ì¶•"
                )
            elif insight['category'] == 'complexity':
                recommendations.append(
                    "ì´ë²¤íŠ¸ íƒ€ì… í†µí•© ë° ì¤‘ë³µ ì œê±°ë¡œ ë³µì¡ë„ ê°ì†Œ"
                )
            elif insight['category'] == 'correlation':
                recommendations.append(
                    "Task-ì´ë²¤íŠ¸ ë§¤í•‘ ê°•í™” - ëª…í™•í•œ ì¸ê³¼ê´€ê³„ ì •ì˜"
                )
        
        # ì¤‘ë³µ ì œê±°
        recommendations = list(dict.fromkeys(recommendations))
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("í˜„ì¬ ìƒíƒœ ì–‘í˜¸. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ìœ ì§€")
        
        return recommendations


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ë£¨ë©˜ (í•©) ê°•í™”íŒ - í˜ë¥´ì†Œë‚˜ì™€ ëŒ€í™” í†µí•©"
    )
    parser.add_argument(
        '--lua-observation',
        default='outputs/lua_observation_latest.json',
        help='ë£¨ì•„ì˜ ê´€ì°° ë°ì´í„°'
    )
    parser.add_argument(
        '--elo-validation',
        default='outputs/elo_validation_latest.json',
        help='ì—˜ë¡œì˜ ê²€ì¦ ë°ì´í„°'
    )
    parser.add_argument(
        '--out-json',
        default='outputs/lumen_enhanced_synthesis_latest.json',
        help='í†µí•© ê²°ê³¼ JSON'
    )
    parser.add_argument(
        '--out-md',
        default='outputs/lumen_enhanced_synthesis_latest.md',
        help='í†µí•© ê²°ê³¼ Markdown'
    )
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ë³´ì •
    repo_root = Path(__file__).parent.parent.parent
    lua_path = repo_root / args.lua_observation if not Path(args.lua_observation).is_absolute() else Path(args.lua_observation)
    elo_path = repo_root / args.elo_validation if not Path(args.elo_validation).is_absolute() else Path(args.elo_validation)
    out_json = repo_root / args.out_json if not Path(args.out_json).is_absolute() else Path(args.out_json)
    out_md = repo_root / args.out_md if not Path(args.out_md).is_absolute() else Path(args.out_md)
    
    # í†µí•© ì‹¤í–‰
    synthesizer = LumenEnhancedSynthesizer(str(lua_path), str(elo_path))
    result = synthesizer.synthesize()
    
    # JSON ì €ì¥
    out_json.parent.mkdir(parents=True, exist_ok=True)
    with open(out_json, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ JSON saved: {out_json}")
    
    # Markdown ìƒì„±
    from lumen_synthesis_agent import generate_markdown
    md_content = generate_markdown(result)
    with open(out_md, 'w', encoding='utf-8') as f:
        f.write(md_content)
    print(f"ğŸ“„ Markdown saved: {out_md}")
    
    print()
    print("âœ… ë£¨ë©˜ (í•©) ê°•í™”íŒ í†µí•© ì™„ë£Œ")
    print("   ì •ë°˜í•©(æ­£ååˆ) + í™•ì¥(æ“´) ì‚¬ì´í´ ì™„ì„±!")


if __name__ == '__main__':
    main()
