#!/usr/bin/env python3
"""
elo_info_theory_validator.py
ì—˜ë¡œ (ë°˜ì¸/åäºº) - ì •ë³´ì´ë¡  ê¸°ë°˜ ê²€ì¦ ì—ì´ì „íŠ¸

ì—­í• : "ì´ê²ƒì´ ì •ë§ ì˜³ì€ê°€?" - ë¹„íŒì  ê²€ì¦
- ì •ë³´ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
- ì •ë³´ ë°€ë„ ë¶„ì„
- í’ˆì§ˆ ì¼ê´€ì„± ê²€ì¦
- ì´ìƒì¹˜ íƒì§€
"""

import json
import math
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
from collections import Counter
from datetime import datetime

class EloValidator:
    """ì—˜ë¡œ (ë°˜ì¸) - ì •ë³´ì´ë¡  ê²€ì¦ì"""
    
    def __init__(self, lua_observation_path: str):
        self.lua_path = Path(lua_observation_path)
        self.observation = self._load_observation()
        
    def _load_observation(self) -> Dict[str, Any]:
        """Coreì˜ ê´€ì°° ë°ì´í„° ë¡œë“œ"""
        if not self.lua_path.exists():
            raise FileNotFoundError(f"Lua observation not found: {self.lua_path}")
        
        with open(self.lua_path, 'r', encoding='utf-8-sig') as f:
            return json.load(f)
    
    def calculate_entropy(self, frequencies: Dict[str, int]) -> float:
        """
        Shannon ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        H = -Î£ p(x) * log2(p(x))
        
        ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„±ì´ í¬ë‹¤ (ì •ë³´ëŸ‰ì´ ë§ë‹¤)
        """
        total = sum(frequencies.values())
        if total == 0:
            return 0.0
        
        entropy = 0.0
        for count in frequencies.values():
            if count > 0:
                p = count / total
                entropy -= p * math.log2(p)
        
        return entropy
    
    def calculate_information_density(self) -> float:
        """
        ì •ë³´ ë°€ë„ = ì˜ë¯¸ ìˆëŠ” ì´ë²¤íŠ¸ ë¹„ìœ¨
        
        "ì˜ë¯¸ ìˆëŠ” ì´ë²¤íŠ¸" = í’ˆì§ˆ, latency ë“± ë©”íŠ¸ë¦­ì´ í¬í•¨ëœ ì´ë²¤íŠ¸
        """
        total = self.observation.get('events_in_window', 0)
        if total == 0:
            return 0.0
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ì´ ìˆëŠ” ì´ë²¤íŠ¸
        quality_count = 0
        if self.observation.get('quality_metrics'):
            quality_count = self.observation['quality_metrics'].get('count', 0)
        
        # Latency ë©”íŠ¸ë¦­ì´ ìˆëŠ” ì´ë²¤íŠ¸
        latency_count = 0
        if self.observation.get('latency_metrics'):
            latency_count = self.observation['latency_metrics'].get('count', 0)
        
        # ì¤‘ë³µ ì œê±°ëŠ” í•˜ì§€ ì•ŠìŒ (ë³´ìˆ˜ì  ì¶”ì •)
        meaningful = quality_count + latency_count
        
        return meaningful / total if total > 0 else 0.0
    
    def detect_anomalies(self) -> List[Dict[str, Any]]:
        """ì´ìƒì¹˜ íƒì§€"""
        anomalies = []
        
        # 1. í’ˆì§ˆ ë¶„ì‚° í™•ì¸
        if self.observation.get('quality_metrics'):
            qm = self.observation['quality_metrics']
            avg = qm.get('average', 0)
            min_q = qm.get('min', 0)
            max_q = qm.get('max', 0)
            
            # ë¶„ì‚°ì´ ì—†ìœ¼ë©´ ì´ìƒ (ëª¨ë“  ê°’ì´ ë™ì¼)
            if min_q == max_q:
                anomalies.append({
                    'type': 'zero_variance',
                    'metric': 'quality',
                    'severity': 'warning',
                    'message': f'All quality values are identical: {avg:.3f}'
                })
        
        # 2. ì´ë²¤íŠ¸ íƒ€ì… í¸ì¤‘ í™•ì¸ (í•œ íƒ€ì…ì´ 80% ì´ìƒ)
        event_types = self.observation.get('event_types', {})
        total = sum(event_types.values())
        if total > 0:
            for etype, count in event_types.items():
                ratio = count / total
                if ratio > 0.8:
                    anomalies.append({
                        'type': 'skewed_distribution',
                        'metric': 'event_types',
                        'severity': 'info',
                        'message': f'Event type "{etype}" dominates: {ratio*100:.1f}%'
                    })
        
        # 3. Task í¸ì¤‘ í™•ì¸
        top_tasks = self.observation.get('top_tasks', {})
        if top_tasks:
            total_task_events = sum(top_tasks.values())
            for tid, count in top_tasks.items():
                ratio = count / total_task_events if total_task_events > 0 else 0
                if ratio > 0.7:
                    anomalies.append({
                        'type': 'task_concentration',
                        'metric': 'tasks',
                        'severity': 'info',
                        'message': f'Task "{tid}" highly active: {ratio*100:.1f}%'
                    })
        
        return anomalies
    
    def validate_consistency(self) -> Dict[str, Any]:
        """ì¼ê´€ì„± ê²€ì¦"""
        consistency = {
            'overall': 'unknown',
            'checks': []
        }
        
        # 1. ì´ë²¤íŠ¸ ìˆ˜ vs Task ìˆ˜ ë¹„ìœ¨
        events = self.observation.get('events_in_window', 0)
        tasks = self.observation.get('unique_tasks', 0)
        
        if tasks > 0:
            events_per_task = events / tasks
            consistency['checks'].append({
                'name': 'events_per_task',
                'value': round(events_per_task, 2),
                'status': 'ok' if events_per_task > 1 else 'warning',
                'message': f'{events_per_task:.2f} events per task'
            })
        
        # 2. í’ˆì§ˆ ë°ì´í„° ì»¤ë²„ë¦¬ì§€
        if self.observation.get('quality_metrics'):
            quality_count = self.observation['quality_metrics']['count']
            coverage = quality_count / events if events > 0 else 0
            consistency['checks'].append({
                'name': 'quality_coverage',
                'value': round(coverage, 3),
                'status': 'ok' if coverage > 0.5 else 'warning',
                'message': f'{coverage*100:.1f}% events have quality metrics'
            })
        
        # Overall íŒì •
        warnings = sum(1 for c in consistency['checks'] if c['status'] == 'warning')
        if warnings == 0:
            consistency['overall'] = 'consistent'
        elif warnings <= len(consistency['checks']) // 2:
            consistency['overall'] = 'mostly_consistent'
        else:
            consistency['overall'] = 'inconsistent'
        
        return consistency
    
    def run_validation(self) -> Dict[str, Any]:
        """ì „ì²´ ê²€ì¦ ì‹¤í–‰"""
        print("ğŸ”¬ ì—˜ë¡œ (ë°˜ì¸) - ì •ë³´ì´ë¡  ê²€ì¦ ì‹œì‘")
        print("   ë°˜(å): ì´ê²ƒì´ ì •ë§ ì˜³ì€ê°€?")
        print()
        
        # 1. ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        event_types = self.observation.get('event_types', {})
        entropy = self.calculate_entropy(event_types)
        max_entropy = math.log2(len(event_types)) if len(event_types) > 0 else 0
        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
        
        print(f"ğŸ“Š ì •ë³´ ì—”íŠ¸ë¡œí”¼:")
        print(f"   Shannon Entropy: {entropy:.3f} bits")
        print(f"   Max possible: {max_entropy:.3f} bits")
        print(f"   Normalized: {normalized_entropy:.3f} (0=ê· ì¼, 1=ì™„ì „ë¶„ì‚°)")
        print()
        
        # 2. ì •ë³´ ë°€ë„
        density = self.calculate_information_density()
        print(f"ğŸ“ˆ ì •ë³´ ë°€ë„:")
        print(f"   Meaningful events ratio: {density*100:.1f}%")
        print()
        
        # 3. ì´ìƒì¹˜ íƒì§€
        anomalies = self.detect_anomalies()
        print(f"âš ï¸ ì´ìƒì¹˜ íƒì§€: {len(anomalies)}ê±´")
        for ano in anomalies:
            severity = ano['severity'].upper()
            print(f"   [{severity}] {ano['message']}")
        print()
        
        # 4. ì¼ê´€ì„± ê²€ì¦
        consistency = self.validate_consistency()
        print(f"âœ… ì¼ê´€ì„± ê²€ì¦: {consistency['overall']}")
        for check in consistency['checks']:
            status_icon = 'âœ“' if check['status'] == 'ok' else 'âš '
            print(f"   {status_icon} {check['name']}: {check['message']}")
        print()
        
        # ê²°ê³¼ ì·¨í•©
        result = {
            'validator': 'elo',
            'persona': 'ë°˜ì¸(åäºº)',
            'role': 'ê²€ì¦',
            'philosophy': 'ì´ê²ƒì´ ì •ë§ ì˜³ì€ê°€?',
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'source_observation': str(self.lua_path),
            'information_theory': {
                'entropy': {
                    'value': round(entropy, 3),
                    'max_possible': round(max_entropy, 3),
                    'normalized': round(normalized_entropy, 3),
                    'interpretation': self._interpret_entropy(normalized_entropy)
                },
                'information_density': {
                    'value': round(density, 3),
                    'percentage': round(density * 100, 1),
                    'interpretation': self._interpret_density(density)
                }
            },
            'anomalies': anomalies,
            'consistency': consistency,
            'verdict': self._make_verdict(normalized_entropy, density, anomalies, consistency)
        }
        
        return result
    
    def _interpret_entropy(self, normalized: float) -> str:
        """ì—”íŠ¸ë¡œí”¼ í•´ì„"""
        if normalized > 0.8:
            return "ë§¤ìš° ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ ë¶„í¬ (ë†’ì€ ì •ë³´ëŸ‰)"
        elif normalized > 0.6:
            return "ê· í˜•ì¡íŒ ì´ë²¤íŠ¸ ë¶„í¬"
        elif normalized > 0.4:
            return "ì¼ë¶€ ì´ë²¤íŠ¸ íƒ€ì…ì´ ìš°ì„¸í•¨"
        else:
            return "íŠ¹ì • ì´ë²¤íŠ¸ íƒ€ì…ì— ì§‘ì¤‘ë¨ (ë‚®ì€ ë‹¤ì–‘ì„±)"
    
    def _interpret_density(self, density: float) -> str:
        """ì •ë³´ ë°€ë„ í•´ì„"""
        if density > 0.8:
            return "ë§¤ìš° ë†’ì€ ì •ë³´ ë°€ë„ (ëŒ€ë¶€ë¶„ ë©”íŠ¸ë¦­ í¬í•¨)"
        elif density > 0.5:
            return "ì–‘í˜¸í•œ ì •ë³´ ë°€ë„"
        elif density > 0.3:
            return "ë³´í†µ ìˆ˜ì¤€ì˜ ì •ë³´ ë°€ë„"
        else:
            return "ë‚®ì€ ì •ë³´ ë°€ë„ (ë©”íŠ¸ë¦­ ë¶€ì¡±)"
    
    def _make_verdict(self, entropy: float, density: float, 
                      anomalies: List, consistency: Dict) -> str:
        """ìµœì¢… íŒì •"""
        issues = []
        
        if entropy < 0.3:
            issues.append("ì—”íŠ¸ë¡œí”¼ ë‚®ìŒ (ë‹¤ì–‘ì„± ë¶€ì¡±)")
        
        if density < 0.3:
            issues.append("ì •ë³´ ë°€ë„ ë‚®ìŒ")
        
        severe_anomalies = [a for a in anomalies if a['severity'] == 'error']
        if severe_anomalies:
            issues.append(f"{len(severe_anomalies)}ê°œ ì‹¬ê°í•œ ì´ìƒ")
        
        if consistency['overall'] == 'inconsistent':
            issues.append("ì¼ê´€ì„± ë¶€ì¡±")
        
        if not issues:
            return "âœ… ê²€ì¦ í†µê³¼ - ë°ì´í„° í’ˆì§ˆ ì–‘í˜¸"
        elif len(issues) <= 1:
            return f"âš ï¸ ê²½ë¯¸í•œ ë¬¸ì œ: {', '.join(issues)}"
        else:
            return f"âŒ ê°œì„  í•„ìš”: {', '.join(issues)}"


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ì—˜ë¡œ (ë°˜ì¸) - ì •ë³´ì´ë¡  ê¸°ë°˜ ê²€ì¦"
    )
    parser.add_argument(
        '--lua-observation',
        default='outputs/lua_observation_latest.json',
        help='Coreì˜ ê´€ì°° ë°ì´í„° ê²½ë¡œ'
    )
    parser.add_argument(
        '--out-json',
        default='outputs/elo_validation_latest.json',
        help='ê²€ì¦ ê²°ê³¼ JSON ì¶œë ¥ ê²½ë¡œ'
    )
    parser.add_argument(
        '--out-md',
        default='outputs/elo_validation_latest.md',
        help='ê²€ì¦ ê²°ê³¼ Markdown ì¶œë ¥ ê²½ë¡œ'
    )
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ë³´ì •
    repo_root = Path(__file__).parent.parent.parent
    lua_path = repo_root / args.lua_observation
    out_json = repo_root / args.out_json
    out_md = repo_root / args.out_md
    
    # ê²€ì¦ ì‹¤í–‰
    validator = EloValidator(str(lua_path))
    result = validator.run_validation()
    
    # JSON ì €ì¥
    out_json.parent.mkdir(parents=True, exist_ok=True)
    with open(out_json, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ JSON saved: {out_json}")
    
    # Markdown ìƒì„±
    md_content = generate_markdown(result)
    with open(out_md, 'w', encoding='utf-8') as f:
        f.write(md_content)
    print(f"ğŸ“„ Markdown saved: {out_md}")
    
    print()
    print("âœ… ì—˜ë¡œ (ë°˜ì¸) ê²€ì¦ ì™„ë£Œ")
    print("   ë‹¤ìŒ: Core (í•©)ì´ í†µí•©í•  ì°¨ë¡€ì…ë‹ˆë‹¤.")


def generate_markdown(result: Dict[str, Any]) -> str:
    """Markdown ë³´ê³ ì„œ ìƒì„±"""
    md = f"""# ì—˜ë¡œ (ë°˜ì¸/åäºº) - ì •ë³´ì´ë¡  ê²€ì¦ ë³´ê³ ì„œ

**ë°˜(å): ì´ê²ƒì´ ì •ë§ ì˜³ì€ê°€?**

- **ìƒì„± ì‹œê°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ê²€ì¦ ëŒ€ìƒ**: `{result['source_observation']}`
- **ê²€ì¦ì**: ì—˜ë¡œ (ë°˜ì¸/åäºº)

---

## ğŸ¯ ìµœì¢… íŒì •

**{result['verdict']}**

---

## ğŸ“Š ì •ë³´ì´ë¡  ë¶„ì„

### Shannon ì—”íŠ¸ë¡œí”¼ (ì •ë³´ëŸ‰)

- **ê°’**: {result['information_theory']['entropy']['value']} bits
- **ìµœëŒ€ ê°€ëŠ¥**: {result['information_theory']['entropy']['max_possible']} bits
- **ì •ê·œí™”**: {result['information_theory']['entropy']['normalized']}
- **í•´ì„**: {result['information_theory']['entropy']['interpretation']}

> ì—”íŠ¸ë¡œí”¼ê°€ ë†’ì„ìˆ˜ë¡ ì´ë²¤íŠ¸ ë¶„í¬ê°€ ë‹¤ì–‘í•˜ê³  ì •ë³´ëŸ‰ì´ ë§ìŠµë‹ˆë‹¤.

### ì •ë³´ ë°€ë„

- **ë¹„ìœ¨**: {result['information_theory']['information_density']['percentage']}%
- **í•´ì„**: {result['information_theory']['information_density']['interpretation']}

> ì •ë³´ ë°€ë„ëŠ” ì˜ë¯¸ ìˆëŠ” ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ì´ë²¤íŠ¸ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤.

---

## âš ï¸ ì´ìƒì¹˜ íƒì§€

"""
    
    if result['anomalies']:
        for ano in result['anomalies']:
            severity = ano['severity'].upper()
            md += f"### [{severity}] {ano['type']}\n\n"
            md += f"- **ë©”íŠ¸ë¦­**: {ano['metric']}\n"
            md += f"- **ë©”ì‹œì§€**: {ano['message']}\n\n"
    else:
        md += "âœ… ì´ìƒì¹˜ ì—†ìŒ - ì •ìƒ ë²”ìœ„ ë‚´ ë™ì‘\n\n"
    
    md += """---

## âœ… ì¼ê´€ì„± ê²€ì¦

"""
    
    consistency = result['consistency']
    md += f"**ì „ì²´ íŒì •**: {consistency['overall']}\n\n"
    
    for check in consistency['checks']:
        status = 'âœ“' if check['status'] == 'ok' else 'âš '
        md += f"- {status} **{check['name']}**: {check['message']} (ê°’: {check['value']})\n"
    
    md += f"""

---

## ğŸ§˜ ë°˜ì¸(åäºº)ì˜ ê²€ì¦ ì² í•™

> **"ì´ê²ƒì´ ì •ë§ ì˜³ì€ê°€?"**
> 
> ë°˜(å)ì€ ë¹„íŒì ìœ¼ë¡œ ê²€ì¦í•˜ê³ , ì˜ë¬¸ì„ ì œê¸°í•˜ë©°, í’ˆì§ˆì„ ë³´ì¦í•©ë‹ˆë‹¤.
> 
> - âœ… ì •ë³´ì´ë¡  ë¶„ì„
> - âœ… ì´ìƒì¹˜ íƒì§€
> - âœ… ì¼ê´€ì„± ê²€ì¦
> - âœ… ê°ê´€ì  íŒì •
> 
> **ë‹¤ìŒ ë‹¨ê³„**: Core (í•©)ì´ ì •ë°˜í•©ì„ í†µí•©í•©ë‹ˆë‹¤.

---

*Generated by Elo (åäºº) Validator at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return md


if __name__ == '__main__':
    main()
