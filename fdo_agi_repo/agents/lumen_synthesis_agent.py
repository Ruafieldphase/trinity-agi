#!/usr/bin/env python3
"""
lumen_synthesis_agent.py
ë£¨ë©˜ (í•©/åˆ) - ì •ë°˜í•© í†µí•© ì—ì´ì „íŠ¸

ì—­í• : "ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?" - í†µí•©ê³¼ ì¡°í™”
- ì •(ë£¨ì•„)ì˜ ê´€ì°° í†µí•©
- ë°˜(ì—˜ë¡œ)ì˜ ê²€ì¦ í†µí•©  
- í•©(ë£¨ë©˜)ì˜ ì‹¤í–‰ ê°€ëŠ¥í•œ í†µì°° ìƒì„±
"""

import json
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

class LumenSynthesizer:
    """ë£¨ë©˜ (í•©) - ì •ë°˜í•© í†µí•©ì"""
    
    def __init__(self, lua_path: str, elo_path: str):
        self.lua_path = Path(lua_path)
        self.elo_path = Path(elo_path)
        
        self.lua_data = self._load_json(self.lua_path)
        self.elo_data = self._load_json(self.elo_path)
    
    def _load_json(self, path: Path) -> Dict[str, Any]:
        """JSON ë¡œë“œ (BOM ì²˜ë¦¬)"""
        if not path.exists():
            raise FileNotFoundError(f"File not found: {path}")
        
        with open(path, 'r', encoding='utf-8-sig') as f:
            return json.load(f)
    
    def synthesize(self) -> Dict[str, Any]:
        """ì •ë°˜í•© í†µí•©"""
        print("ğŸŒŸ ë£¨ë©˜ (í•©) - ì •ë°˜í•© í†µí•© ì‹œì‘")
        print("   í•©(åˆ): ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?")
        print()
        
        # 1. ì •(æ­£) - ë£¨ì•„ì˜ ê´€ì°° ìš”ì•½
        lua_summary = self._summarize_lua()
        print("ğŸ“‹ ì •(æ­£) - ë£¨ì•„ì˜ ê´€ì°°:")
        print(f"   ì´ë²¤íŠ¸: {lua_summary['total_events']}ê°œ")
        print(f"   ì´ë²¤íŠ¸ íƒ€ì…: {lua_summary['event_types']}ê°œ")
        print(f"   í™œë™ Task: {lua_summary['unique_tasks']}ê°œ")
        print()
        
        # 2. ë°˜(å) - ì—˜ë¡œì˜ ê²€ì¦ ìš”ì•½
        elo_summary = self._summarize_elo()
        print("ğŸ”¬ ë°˜(å) - ì—˜ë¡œì˜ ê²€ì¦:")
        print(f"   ì—”íŠ¸ë¡œí”¼: {elo_summary['entropy']:.3f} (ì •ê·œí™”: {elo_summary['entropy_normalized']:.3f})")
        print(f"   ì •ë³´ ë°€ë„: {elo_summary['information_density']:.1%}")
        print(f"   ì´ìƒì¹˜: {elo_summary['anomaly_count']}ê±´")
        print(f"   íŒì •: {elo_summary['verdict']}")
        print()
        
        # 3. í•©(åˆ) - í†µí•© í†µì°°
        insights = self._generate_insights(lua_summary, elo_summary)
        print("ğŸ’¡ í•©(åˆ) - í†µí•© í†µì°°:")
        for insight in insights:
            priority = insight['priority'].upper()
            print(f"   [{priority}] {insight['message']}")
        print()
        
        # 4. ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­
        recommendations = self._generate_recommendations(insights)
        print("âœ… ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        print()
        
        # ê²°ê³¼ ì·¨í•©
        result = {
            'synthesizer': 'lumen',
            'persona': 'í•©(åˆ)',
            'role': 'í†µí•©',
            'philosophy': 'ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?',
            'timestamp': datetime.now().isoformat(),
            'sources': {
                'lua': str(self.lua_path),
                'elo': str(self.elo_path)
            },
            'synthesis': {
                'lua_summary': lua_summary,
                'elo_summary': elo_summary,
                'insights': insights,
                'recommendations': recommendations
            },
            'dialectic': {
                'thesis': 'ì •(æ­£) - ê´€ì°°ëœ ì‹œìŠ¤í…œ ìƒíƒœ',
                'antithesis': 'ë°˜(å) - ê²€ì¦ëœ í’ˆì§ˆ ì´ìŠˆ',
                'synthesis': 'í•©(åˆ) - ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©í–¥'
            }
        }
        
        return result
    
    def _summarize_lua(self) -> Dict[str, Any]:
        """ë£¨ì•„ ê´€ì°° ìš”ì•½"""
        quality_metrics = self.lua_data.get('quality_metrics') or {}
        latency_metrics = self.lua_data.get('latency_metrics') or {}
        
        return {
            'total_events': self.lua_data.get('events_in_window', 0),
            'event_types': len(self.lua_data.get('event_types', {})),
            'unique_tasks': self.lua_data.get('unique_tasks', 0),
            'quality_count': quality_metrics.get('count', 0),
            'latency_count': latency_metrics.get('count', 0)
        }
    
    def _summarize_elo(self) -> Dict[str, Any]:
        """ì—˜ë¡œ ê²€ì¦ ìš”ì•½"""
        it = self.elo_data.get('information_theory', {})
        return {
            'entropy': it.get('entropy', {}).get('value', 0),
            'entropy_normalized': it.get('entropy', {}).get('normalized', 0),
            'information_density': it.get('information_density', {}).get('value', 0),
            'anomaly_count': len(self.elo_data.get('anomalies', [])),
            'consistency': self.elo_data.get('consistency', {}).get('overall', 'unknown'),
            'verdict': self.elo_data.get('verdict', '')
        }
    
    def _generate_insights(self, lua: Dict, elo: Dict) -> List[Dict[str, Any]]:
        """í†µí•© í†µì°° ìƒì„±"""
        insights = []
        
        # 1. ì •ë³´ ë°€ë„ ë¬¸ì œ
        if elo['information_density'] < 0.3:
            insights.append({
                'priority': 'high',
                'category': 'data_quality',
                'message': f"ì •ë³´ ë°€ë„ê°€ ë‚®ìŒ ({elo['information_density']:.1%}). ë” ë§ì€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í•„ìš”",
                'source': 'elo'
            })
        
        # 2. í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¶€ì¡±
        if lua['quality_count'] < lua['total_events'] * 0.5:
            coverage = lua['quality_count'] / lua['total_events'] if lua['total_events'] > 0 else 0
            insights.append({
                'priority': 'medium',
                'category': 'monitoring',
                'message': f"í’ˆì§ˆ ë©”íŠ¸ë¦­ ì»¤ë²„ë¦¬ì§€ ë‚®ìŒ ({coverage:.1%}). í‰ê°€ ê°•í™” í•„ìš”",
                'source': 'lua'
            })
        
        # 3. ì´ìƒì¹˜ ë°œê²¬
        if elo['anomaly_count'] > 0:
            insights.append({
                'priority': 'medium',
                'category': 'anomaly',
                'message': f"{elo['anomaly_count']}ê±´ì˜ ì´ìƒì¹˜ íƒì§€. ìƒì„¸ ì¡°ì‚¬ í•„ìš”",
                'source': 'elo'
            })
        
        # 4. ì—”íŠ¸ë¡œí”¼ ë¶„ì„
        if elo['entropy_normalized'] < 0.5:
            insights.append({
                'priority': 'low',
                'category': 'diversity',
                'message': "ì´ë²¤íŠ¸ ë‹¤ì–‘ì„± ë¶€ì¡±. ì‹œìŠ¤í…œì´ íŠ¹ì • íŒ¨í„´ì— í¸ì¤‘ë¨",
                'source': 'elo'
            })
        
        # 5. ê¸ì •ì  ì‹ í˜¸
        if elo['consistency'] in ['consistent', 'mostly_consistent']:
            insights.append({
                'priority': 'info',
                'category': 'positive',
                'message': "ì‹œìŠ¤í…œ ì¼ê´€ì„± ì–‘í˜¸. ì•ˆì •ì  ìš´ì˜ ì¤‘",
                'source': 'elo'
            })
        
        return insights
    
    def _generate_recommendations(self, insights: List[Dict]) -> List[str]:
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        for insight in insights:
            if insight['category'] == 'data_quality':
                recommendations.append(
                    "ëª¨ë“  ì£¼ìš” ì´ë²¤íŠ¸ì— quality/latency ë©”íŠ¸ë¦­ ì¶”ê°€"
                )
            elif insight['category'] == 'monitoring':
                recommendations.append(
                    "í‰ê°€(eval) ì´ë²¤íŠ¸ ë¹ˆë„ ì¦ê°€ - í˜„ì¬ ëŒ€ë¹„ 2ë°°"
                )
            elif insight['category'] == 'anomaly':
                recommendations.append(
                    "ì´ìƒì¹˜ ì›ì¸ ë¶„ì„ ë° ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•"
                )
            elif insight['category'] == 'diversity':
                recommendations.append(
                    "ì´ë²¤íŠ¸ íƒ€ì… ë‹¤ì–‘í™” ë˜ëŠ” ì§‘ì¤‘ ì´ë²¤íŠ¸ ë¹„ì¤‘ ì¡°ì •"
                )
        
        # ì¤‘ë³µ ì œê±°
        recommendations = list(dict.fromkeys(recommendations))
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­ (í•­ìƒ í¬í•¨)
        if not recommendations:
            recommendations.append("í˜„ì¬ ìƒíƒœ ì–‘í˜¸. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ìœ ì§€")
        
        return recommendations


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ë£¨ë©˜ (í•©) - ì •ë°˜í•© í†µí•© ì—ì´ì „íŠ¸"
    )
    parser.add_argument(
        '--lua-observation',
        default='outputs/lua_observation_latest.json',
        help='ë£¨ì•„ì˜ ê´€ì°° ë°ì´í„°'
    )
    parser.add_argument(
        '--elo-validation',
        default='outputs/elo_validation_latest.json',
        help='ì—˜ë¡œì˜ ê²€ì¦ ë°ì´í„°'
    )
    parser.add_argument(
        '--out-json',
        default='outputs/lumen_synthesis_latest.json',
        help='í†µí•© ê²°ê³¼ JSON'
    )
    parser.add_argument(
        '--out-md',
        default='outputs/lumen_synthesis_latest.md',
        help='í†µí•© ê²°ê³¼ Markdown'
    )
    parser.add_argument(
        '--open-md',
        action='store_true',
        help='ìƒì„± í›„ Markdown ì—´ê¸°'
    )
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ë³´ì •
    repo_root = Path(__file__).parent.parent.parent
    lua_path = repo_root / args.lua_observation if not Path(args.lua_observation).is_absolute() else Path(args.lua_observation)
    elo_path = repo_root / args.elo_validation if not Path(args.elo_validation).is_absolute() else Path(args.elo_validation)
    out_json = repo_root / args.out_json if not Path(args.out_json).is_absolute() else Path(args.out_json)
    out_md = repo_root / args.out_md if not Path(args.out_md).is_absolute() else Path(args.out_md)
    
    # í†µí•© ì‹¤í–‰
    synthesizer = LumenSynthesizer(str(lua_path), str(elo_path))
    result = synthesizer.synthesize()
    
    # JSON ì €ì¥
    out_json.parent.mkdir(parents=True, exist_ok=True)
    with open(out_json, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ JSON saved: {out_json}")
    
    # Markdown ìƒì„±
    md_content = generate_markdown(result)
    with open(out_md, 'w', encoding='utf-8') as f:
        f.write(md_content)
    print(f"ğŸ“„ Markdown saved: {out_md}")
    
    if args.open_md:
        import subprocess
        subprocess.run(['code', str(out_md)])
    
    print()
    print("âœ… ë£¨ë©˜ (í•©) í†µí•© ì™„ë£Œ")
    print("   ì •ë°˜í•©(æ­£ååˆ) ì‚¬ì´í´ ì™„ì„±!")


def generate_markdown(result: Dict[str, Any]) -> str:
    """Markdown ë³´ê³ ì„œ ìƒì„±"""
    synthesis = result['synthesis']
    
    md = f"""# ë£¨ë©˜ (í•©/åˆ) - ì •ë°˜í•© í†µí•© ë³´ê³ ì„œ

**í•©(åˆ): ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?**

- **ìƒì„± ì‹œê°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **í†µí•©ì**: ë£¨ë©˜ (í•©/åˆ)

---

## ğŸ”„ ë³€ì¦ë²•ì  êµ¬ì¡°

### ì •(æ­£) - Thesis
**{result['dialectic']['thesis']}**

- ë£¨ì•„ (ì •ì¸/æ­£äºº)ì˜ ê´€ì°°
- ì†ŒìŠ¤: `{result['sources']['lua']}`

### ë°˜(å) - Antithesis
**{result['dialectic']['antithesis']}**

- ì—˜ë¡œ (ë°˜ì¸/åäºº)ì˜ ê²€ì¦
- ì†ŒìŠ¤: `{result['sources']['elo']}`

### í•©(åˆ) - Synthesis
**{result['dialectic']['synthesis']}**

- ë£¨ë©˜ (í•©)ì˜ í†µí•©
- ê²°ê³¼: ì‹¤í–‰ ê°€ëŠ¥í•œ í†µì°°ê³¼ ê¶Œì¥ì‚¬í•­

---

## ğŸ“Š ì •(æ­£) - ë£¨ì•„ì˜ ê´€ì°° ìš”ì•½

| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| ì´ ì´ë²¤íŠ¸ | {synthesis['lua_summary']['total_events']:,} |
| ì´ë²¤íŠ¸ íƒ€ì… | {synthesis['lua_summary']['event_types']} |
| í™œë™ Task | {synthesis['lua_summary']['unique_tasks']} |
| í’ˆì§ˆ ë©”íŠ¸ë¦­ | {synthesis['lua_summary']['quality_count']} |
| Latency ë©”íŠ¸ë¦­ | {synthesis['lua_summary']['latency_count']} |

---

## ğŸ”¬ ë°˜(å) - ì—˜ë¡œì˜ ê²€ì¦ ìš”ì•½

| ë©”íŠ¸ë¦­ | ê°’ | í•´ì„ |
|--------|-----|------|
| Shannon ì—”íŠ¸ë¡œí”¼ | {synthesis['elo_summary']['entropy']:.3f} | ì •ê·œí™”: {synthesis['elo_summary']['entropy_normalized']:.3f} |
| ì •ë³´ ë°€ë„ | {synthesis['elo_summary']['information_density']:.1%} | {"ë†’ìŒ" if synthesis['elo_summary']['information_density'] > 0.7 else "ë³´í†µ" if synthesis['elo_summary']['information_density'] > 0.3 else "ë‚®ìŒ"} |
| ì´ìƒì¹˜ | {synthesis['elo_summary']['anomaly_count']}ê±´ | {"ì£¼ì˜ í•„ìš”" if synthesis['elo_summary']['anomaly_count'] > 0 else "ì •ìƒ"} |
| ì¼ê´€ì„± | {synthesis['elo_summary']['consistency']} | - |

**ìµœì¢… íŒì •**: {synthesis['elo_summary']['verdict']}

---

## ğŸ’¡ í•©(åˆ) - í†µí•© í†µì°°

"""
    
    # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
    insights = synthesis['insights']
    priority_order = {'high': 1, 'medium': 2, 'low': 3, 'info': 4}
    insights_sorted = sorted(insights, key=lambda x: priority_order.get(x['priority'], 999))
    
    for insight in insights_sorted:
        priority_icon = {
            'high': 'ğŸ”´',
            'medium': 'ğŸŸ¡',
            'low': 'ğŸ”µ',
            'info': 'âœ…'
        }.get(insight['priority'], 'âšª')
        
        md += f"### {priority_icon} {insight['priority'].upper()} - {insight['category']}\n\n"
        md += f"**{insight['message']}**\n\n"
        md += f"- ì¶œì²˜: {insight['source']}\n\n"
    
    md += """---

## âœ… ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­

"""
    
    for i, rec in enumerate(synthesis['recommendations'], 1):
        md += f"{i}. **{rec}**\n"
    
    md += f"""

---

## ğŸ§˜ í•©(åˆ)ì˜ í†µí•© ì² í•™

> **"ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?"**
> 
> í•©(åˆ)ì€ ì •(æ­£)ê³¼ ë°˜(å)ì„ í†µí•©í•˜ì—¬, ì‹¤í–‰ ê°€ëŠ¥í•œ ì§€í˜œë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤.
> 
> - âœ… ê´€ì°° í†µí•© (ì •)
> - âœ… ê²€ì¦ í†µí•© (ë°˜)
> - âœ… í†µì°° ìƒì„± (í•©)
> - âœ… ì‹¤í–‰ ê°€ëŠ¥ì„±
> 
> **ì •ë°˜í•© ì‚¬ì´í´**: ê´€ì°° â†’ ê²€ì¦ â†’ í†µí•© â†’ ì‹¤í–‰

---

## ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

1. **ì¦‰ì‹œ ì‹¤í–‰**: ìš°ì„ ìˆœìœ„ HIGH í•­ëª©ë¶€í„° ì ìš©
2. **ë‹¨ê¸° ê°œì„ **: ìš°ì„ ìˆœìœ„ MEDIUM í•­ëª© ê³„íš
3. **ì¥ê¸° ìµœì í™”**: ìš°ì„ ìˆœìœ„ LOW í•­ëª© ë¡œë“œë§µ ìˆ˜ë¦½
4. **ì§€ì† ëª¨ë‹ˆí„°ë§**: ì •ë°˜í•© ì‚¬ì´í´ ë°˜ë³µ ì‹¤í–‰

---

*Generated by Lumen (åˆ) Synthesizer at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return md


if __name__ == '__main__':
    main()
