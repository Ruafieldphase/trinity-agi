"""
í†µí•© ëª¨ë‹ˆí„°ë§ ë°ëª¬
- Task Queue Serverì™€ í†µí•©í•˜ì—¬ ì‹¤ì œ RPA ì‘ì—… ëª¨ë‹ˆí„°ë§
- ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ + ìë™ ì•Œë¦¼
"""
import time
import argparse
import sys
from pathlib import Path
from typing import Optional
import psutil
import requests
from datetime import datetime

# ìƒëŒ€ ê²½ë¡œë¡œ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent))
from metrics_collector import MetricsCollector, DashboardRenderer
from alert_manager import AlertManager, DEFAULT_THRESHOLDS


class RPAMonitoringDaemon:
    """RPA ëª¨ë‹ˆí„°ë§ ë°ëª¬"""
    
    def __init__(
        self,
        server_url: str = "http://127.0.0.1:8091",
        interval_seconds: float = 5.0,
        output_dir: Optional[Path] = None,
    ):
        """
        Args:
            server_url: Task Queue Server URL
            interval_seconds: ëª¨ë‹ˆí„°ë§ ì£¼ê¸° (ì´ˆ)
            output_dir: ë©”íŠ¸ë¦­/ì•Œë¦¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.server_url = server_url
        self.interval_seconds = interval_seconds
        self.output_dir = output_dir or Path(__file__).parent.parent / "outputs"
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
        self.collector = MetricsCollector(
            history_size=100,
            persist_path=self.output_dir / "rpa_monitoring_metrics.jsonl"
        )
        
        # ì•Œë¦¼ ê´€ë¦¬ì
        self.alert_manager = AlertManager(
            alert_log_path=self.output_dir / "rpa_monitoring_alerts.jsonl"
        )
        
        # ê¸°ë³¸ ì„ê³„ê°’ ì¶”ê°€
        for threshold in DEFAULT_THRESHOLDS:
            self.alert_manager.add_threshold(threshold)
        
        # ë§ˆì§€ë§‰ ì¡°íšŒ ìƒíƒœ
        self._last_total_tasks = 0
        self._last_successful_tasks = 0
        self._last_failed_tasks = 0
    
    def fetch_queue_stats(self) -> Optional[dict]:
        """Task Queue Serverì—ì„œ í†µê³„ ì¡°íšŒ"""
        try:
            response = requests.get(
                f"{self.server_url}/api/stats",
                timeout=2.0
            )
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            print(f"Failed to fetch queue stats: {e}")
            return None
    
    def fetch_results(self, count: int = 10) -> Optional[list]:
        """ìµœê·¼ ì‘ì—… ê²°ê³¼ ì¡°íšŒ"""
        try:
            response = requests.get(
                f"{self.server_url}/api/results?count={count}",
                timeout=2.0
            )
            response.raise_for_status()
            data = response.json()
            return data.get("results", [])
        except requests.RequestException as e:
            print(f"Failed to fetch results: {e}")
            return None
    
    def update_metrics_from_queue(self):
        """Queue í†µê³„ë¡œë¶€í„° ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        stats = self.fetch_queue_stats()
        if not stats:
            return
        
        # Task Queue ìƒíƒœ
        self.collector.update_queue_size(stats.get("pending", 0))
        self.collector.update_worker_count(stats.get("workers", 0))
        
        # ì‘ì—… ê²°ê³¼ ë¶„ì„
        results = self.fetch_results(count=20)
        if results:
            for result in results:
                # ìƒˆë¡œìš´ ê²°ê³¼ë§Œ ì²˜ë¦¬ (ì¤‘ë³µ ë°©ì§€)
                task_id = result.get("task_id", "")
                success = result.get("status") == "success"
                duration_ms = result.get("duration_ms", 0)
                
                # ê°„ë‹¨í•œ ì¤‘ë³µ ë°©ì§€: ë§ˆì§€ë§‰ ì¡°íšŒ ì´í›„ ì¦ê°€ë¶„ë§Œ ê¸°ë¡
                # (ì‹¤ì œë¡œëŠ” task_id ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ í•„ìš”)
                pass
        
        # ì „ì²´ í†µê³„ë¡œë¶€í„° ì‘ì—… ê¸°ë¡ (ì¦ê°€ë¶„ë§Œ)
        total = stats.get("completed", 0)
        successful = stats.get("successful", 0)
        failed = stats.get("failed", 0)
        
        new_total = total - self._last_total_tasks
        new_successful = successful - self._last_successful_tasks
        new_failed = failed - self._last_failed_tasks
        
        # ìƒˆë¡œìš´ ì‘ì—…ì´ ìˆìœ¼ë©´ ê¸°ë¡
        if new_total > 0:
            avg_response_time = stats.get("avg_duration_ms", 0)
            
            # ì„±ê³µ/ì‹¤íŒ¨ ê°ê° ê¸°ë¡
            for _ in range(new_successful):
                self.collector.record_task(success=True, response_time_ms=avg_response_time)
            
            for _ in range(new_failed):
                self.collector.record_task(success=False, response_time_ms=avg_response_time)
        
        # ë§ˆì§€ë§‰ ìƒíƒœ ì—…ë°ì´íŠ¸
        self._last_total_tasks = total
        self._last_successful_tasks = successful
        self._last_failed_tasks = failed
    
    def run(self, duration_minutes: Optional[float] = None):
        """ëª¨ë‹ˆí„°ë§ ë°ëª¬ ì‹¤í–‰
        
        Args:
            duration_minutes: ì‹¤í–‰ ì‹œê°„ (ë¶„), Noneì´ë©´ ë¬´í•œ ì‹¤í–‰
        """
        print("ğŸ” RPA Monitoring Daemon Started")
        print(f"  Server: {self.server_url}")
        print(f"  Interval: {self.interval_seconds}s")
        print(f"  Output: {self.output_dir}")
        print()
        
        start_time = time.time()
        iteration = 0
        
        try:
            while True:
                # ê²½ê³¼ ì‹œê°„ í™•ì¸
                if duration_minutes:
                    elapsed_minutes = (time.time() - start_time) / 60
                    if elapsed_minutes >= duration_minutes:
                        print(f"\nâ±ï¸  Duration limit reached ({duration_minutes} minutes)")
                        break
                
                iteration += 1
                
                # Queueì—ì„œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                self.update_metrics_from_queue()
                
                # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¸¡ì •
                memory_mb = psutil.Process().memory_info().rss / (1024 * 1024)
                cpu_percent = psutil.cpu_percent(interval=0.1)
                
                # ìŠ¤ëƒ…ìƒ· ìƒì„±
                snapshot = self.collector.take_snapshot(memory_mb, cpu_percent)
                
                # í†µê³„ ì¡°íšŒ
                stats = self.collector.get_statistics(window_seconds=60)
                
                # ì•Œë¦¼ í™•ì¸
                metrics = {
                    "error_rate": snapshot.error_rate,
                    "success_rate": snapshot.success_rate,
                    "avg_response_time_ms": snapshot.avg_response_time_ms,
                    "active_workers": snapshot.active_workers,
                    "queue_size": snapshot.queue_size,
                }
                self.alert_manager.check_metrics(metrics)
                
                # ëŒ€ì‹œë³´ë“œ ë Œë”ë§ (10ì´ˆë§ˆë‹¤ ìƒì„¸, ë‚˜ë¨¸ì§€ëŠ” ì»´íŒ©íŠ¸)
                if iteration % 2 == 0:
                    print(DashboardRenderer.render(snapshot, stats))
                else:
                    print(DashboardRenderer.render_compact(snapshot))
                
                # ëŒ€ê¸°
                time.sleep(self.interval_seconds)
                
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  Monitoring stopped by user")
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            raise
        finally:
            self._print_summary()
    
    def _print_summary(self):
        """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print("ğŸ“Š Monitoring Summary")
        print("=" * 70)
        
        # ë©”íŠ¸ë¦­ í†µê³„
        stats = self.collector.get_statistics()
        print("\nMetrics:")
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")
        
        # ì•Œë¦¼ í†µê³„
        all_alerts = self.alert_manager.get_recent_alerts(1000)
        print(f"\nAlerts:")
        print(f"  Total: {len(all_alerts)}")
        for severity in ["critical", "warning", "info"]:
            count = len(self.alert_manager.get_alerts_by_severity(severity))
            icon = AlertManager.SEVERITY_ICONS[severity]
            print(f"  {icon} {severity.capitalize()}: {count}")
        
        print(f"\nOutput Files:")
        print(f"  Metrics: {self.collector.persist_path}")
        print(f"  Alerts: {self.alert_manager.alert_log_path}")


def main():
    parser = argparse.ArgumentParser(description="RPA Monitoring Daemon")
    parser.add_argument(
        "--server",
        default="http://127.0.0.1:8091",
        help="Task Queue Server URL"
    )
    parser.add_argument(
        "--interval",
        type=float,
        default=5.0,
        help="Monitoring interval in seconds"
    )
    parser.add_argument(
        "--duration",
        type=float,
        default=None,
        help="Run duration in minutes (default: infinite)"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=None,
        help="Output directory for metrics and alerts"
    )
    
    args = parser.parse_args()
    
    daemon = RPAMonitoringDaemon(
        server_url=args.server,
        interval_seconds=args.interval,
        output_dir=args.output_dir,
    )
    
    daemon.run(duration_minutes=args.duration)


if __name__ == "__main__":
    main()
