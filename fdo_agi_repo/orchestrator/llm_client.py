from __future__ import annotations
from typing import Optional, Dict, Any
import json

try:
    import requests  # type: ignore
except Exception:  # pragma: no cover
    requests = None  # ëŸ°íƒ€ì„ì— ë¯¸ì„¤ì¹˜ì¼ ìˆ˜ ìˆìŒ (llm.enabled=falseì¼ ë•ŒëŠ” ì˜í–¥ ì—†ìŒ)

from agi_core.rhythm_boundaries import RhythmBoundaryManager
from pathlib import Path
import os
import warnings


class LLMClient:
    """
    ìµœì†Œ í† ëŒ€ê°€ ë˜ëŠ” LLM í´ë¼ì´ì–¸íŠ¸ ì¸í„°í˜ì´ìŠ¤.
    í˜„ì¬ëŠ” ì‹¤ì œ LLM í˜¸ì¶œ ì—†ì´, ì—°ê²°ì´ êº¼ì ¸ ìˆìœ¼ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì¶”í›„ provider/openai, anthropic, vertex, local-vllm ë“±ì„ ì—°ê²°í•©ë‹ˆë‹¤.
    """

    def __init__(self, provider: Optional[str] = None, model: Optional[str] = None, endpoint: Optional[str] = None, **kwargs: Any) -> None:
        self.provider = provider
        self.model = model
        self.endpoint = endpoint
        self.kwargs = kwargs

    def generate(self, system_prompt: str, user_prompt: str, **kwargs: Any) -> Optional[str]:
        # ğŸ§¬ Rhythm-Aware Parameters
        workspace_root = Path(__file__).parent.parent.parent # fdo_agi_repo/orchestrator -> workspace root
        boundary_manager = RhythmBoundaryManager(workspace_root)
        rhythm_state = boundary_manager.get_rhythm_state()
        
        # ë¦¬ë“¬ì— ë”°ë¥¸ ì˜¨ë„(Temperature) ì¡°ì ˆ
        # í™•ì¥: ì°½ì˜ì (High), ìˆ˜ì¶•: ì •ë°€(Low)
        base_temp = kwargs.get("temperature", self.kwargs.get("temperature", 0.7))
        if rhythm_state["phase"] == "EXPANSION":
            kwargs["temperature"] = min(1.0, base_temp * 1.2)
        else:
            kwargs["temperature"] = max(0.1, base_temp * 0.7)
            
        # ë¦¬ë“¬ì— ë”°ë¥¸ íƒ€ì„ì•„ì›ƒ(Timeout) ì¡°ì ˆ
        # í™•ì¥: ì¸ë‚´ì‹¬(Long), ìˆ˜ì¶•: ê¸°ë¯¼í•¨(Short)
        base_timeout = kwargs.get("timeout", self.kwargs.get("timeout", 30))
        kwargs["timeout"] = boundary_manager.adjust_threshold("timeout_seconds", base_timeout, rhythm_state)

        # provider ë¶„ê¸°
        if self.provider in (None, "", "disabled"):
            return None
        if self.provider == "local_proxy":
            return self._generate_via_local_proxy(system_prompt, user_prompt, **kwargs)
        if self.provider in ("google", "genai", "google_ai_studio"):
            return self._generate_via_google(system_prompt, user_prompt, **kwargs)
        if self.provider in ("auto", "model_selector"):
            return self._generate_via_model_selector(system_prompt, user_prompt, **kwargs)
        # TODO: openai, anthropic, vertex ë“± ì¶”ê°€ êµ¬í˜„
        return None

    def _generate_via_model_selector(self, system_prompt: str, user_prompt: str, **kwargs: Any) -> Optional[str]:
        """
        Workspaceì˜ ModelSelector(GenAI/Vertex ìë™ ì„ íƒ)ë¥¼ ì‚¬ìš©í•œë‹¤.
        - ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë°–ìœ¼ë¡œ ë˜ì§€ì§€ ì•Šê³  Noneì„ ë°˜í™˜(ìƒìœ„ ë£¨í”„ ì•ˆì •ì„± ìœ ì§€).
        - í‚¤/ìê²©ì¦ëª… ê°’ì€ ë¡œê·¸/íŒŒì¼ì— ë‚¨ê¸°ì§€ ì•ŠëŠ”ë‹¤.
        """
        try:
            from services.model_selector import ModelSelector  # type: ignore

            selector = ModelSelector()
            if not getattr(selector, "available", False):
                return None

            # ModelSelectorëŠ” contentë¥¼ ê·¸ëŒ€ë¡œ ë°›ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¨ìˆœ í•©ì„±.
            prompt = f"{system_prompt}\n\n{user_prompt}".strip()
            response, _model_used = selector.try_generate_content(
                prompt,
                intent="CHAT",
                text_length=len(prompt),
                urgency=False,
                high_precision=False,
                generation_config={"temperature": float(kwargs.get("temperature", 0.7))},
                timeout=int(kwargs.get("timeout", 30)),
            )
            if response is None:
                return None
            # GenAI/Vertex ì‘ë‹µ ê°ì²´ëŠ” .textê°€ ìˆì„ ìˆ˜ ìˆë‹¤.
            txt = getattr(response, "text", None)
            if isinstance(txt, str) and txt.strip():
                return txt.strip()
            # Fallback: stringify.
            s = str(response)
            return s.strip() if s.strip() else None
        except Exception:
            return None

    def _generate_via_google(self, system_prompt: str, user_prompt: str, **kwargs: Any) -> Optional[str]:
        """Google Gemini APIë¥¼ í†µí•œ ìƒì„±"""
        try:
            # google.generativeaiëŠ” í™˜ê²½ì— ë”°ë¼ FutureWarningì´ ë…¸ì´ì¦ˆë¡œ ë³´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìˆ¨ê¸´ë‹¤.
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", category=FutureWarning)
                import google.generativeai as genai
            
            api_key = (
                self.kwargs.get("api_key")
                or os.environ.get("GOOGLE_API_KEY")
                or os.environ.get("GEMINI_API_KEY")
            )
            if not api_key:
                return None
                
            genai.configure(api_key=api_key)
            # Prefer Gemini 3, then 2.5, then fall back.
            candidates = [
                (self.model or "").strip(),
                "gemini-3-flash",
                "gemini-3-pro",
                "gemini-2.5-flash",
                "gemini-2.5-flash-lite",
                "gemini-1.5-flash",
                "gemini-1.5-pro",
            ]
            candidates = [c for c in candidates if c]
            
            # Combine prompts if necessary or use chat interface
            contents = [
                {"role": "user", "parts": [f"System Instructions: {system_prompt}\n\nUser Request: {user_prompt}"]}
            ]
            
            generation_config = {
                "temperature": kwargs.get("temperature", 0.7),
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 4096,
            }
            for model_name in candidates:
                try:
                    model = genai.GenerativeModel(model_name)
                    response = model.generate_content(contents, generation_config=generation_config)
                    txt = getattr(response, "text", None)
                    if isinstance(txt, str) and txt.strip():
                        return txt.strip()
                except Exception:
                    continue
            return None
        except Exception:
            return None

    def _generate_via_local_proxy(self, system_prompt: str, user_prompt: str, **kwargs: Any) -> Optional[str]:
        if not self.endpoint or not requests:
            return None
            
        payload = {
            "model": self.model or "yanolja_-_eeve-korean-instruct-10.8b-v1.0",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "temperature": 0.3,
            "n": 1,
            "stream": False,
        }
        
        # allow override of timeout via kwargs or ctor kwargs
        timeout_val = kwargs.get("timeout") if kwargs else None
        if timeout_val is None:
            timeout_val = (self.kwargs or {}).get("timeout", 30)
        try:
            resp = requests.post(self.endpoint, json=payload, timeout=timeout_val)
            
            if resp.status_code == 200:
                data = resp.json()
                choices = data.get("choices", [])
                if choices:
                    content = choices[0].get("message", {}).get("content")
                    return content
        except Exception:
            return None
        return None


def get_llm_client_for_persona(persona: str, overrides: Optional[Dict[str, Any]] = None) -> LLMClient:
    overrides = overrides or {}
    # í–¥í›„ overridesì—ì„œ provider/modelì„ ì£¼ì…
    provider = overrides.get("provider")
    model = overrides.get("model")
    endpoint = overrides.get("endpoint")
    return LLMClient(provider=provider, model=model, endpoint=endpoint)
