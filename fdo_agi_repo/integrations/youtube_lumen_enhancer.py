#!/usr/bin/env python3
"""
YouTube Learning + Lumen Enhancement
YouTube ì˜ìƒ ë¶„ì„ ê²°ê³¼ì— ë£¨ë©˜ í˜ë¥´ì†Œë‚˜ ì¸ì‚¬ì´íŠ¸ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
"""

import json
import requests
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# ì„¤ì •
WORKSPACE_ROOT = Path(__file__).parent.parent.parent
YOUTUBE_OUTPUT_DIR = WORKSPACE_ROOT / "fdo_agi_repo" / "outputs" / "youtube_learner"
LUMEN_GATEWAY = "https://lumen-gateway-x4qvsargwa-uc.a.run.app/chat"
OUTPUT_DIR = WORKSPACE_ROOT / "outputs"

def find_latest_youtube_analysis() -> Path:
    """ìµœì‹  YouTube ë¶„ì„ ê²°ê³¼ ì°¾ê¸°"""
    if not YOUTUBE_OUTPUT_DIR.exists():
        raise FileNotFoundError(f"YouTube output directory not found: {YOUTUBE_OUTPUT_DIR}")
    
    json_files = list(YOUTUBE_OUTPUT_DIR.glob("*_analysis.json"))
    if not json_files:
        raise FileNotFoundError("No YouTube analysis files found")
    
    latest = max(json_files, key=lambda p: p.stat().st_mtime)
    return latest

def load_youtube_analysis(file_path: Path) -> Dict[str, Any]:
    """YouTube ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def generate_analysis_summary(data: Dict[str, Any]) -> str:
    """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    title = data.get('title', 'Unknown')
    duration = data.get('duration', 0)
    duration_min = int(duration / 60)
    
    # ìë§‰ ìƒ˜í”Œ ì¶”ì¶œ (ì²˜ìŒ 10ê°œ)
    subtitles = data.get('subtitles', [])[:10]
    subtitle_text = " ".join([s.get('text', '') for s in subtitles])
    
    summary = f"""
YouTube ì˜ìƒ ë¶„ì„ ê²°ê³¼:

ì œëª©: {title}
ê¸¸ì´: {duration_min}ë¶„
ìë§‰ ìƒ˜í”Œ: {subtitle_text[:300]}...

ì´ ì˜ìƒì˜ í•µì‹¬ ì£¼ì œì™€ í•™ìŠµ ê°€ì¹˜ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""
    
    return summary.strip()

def query_persona(persona_name: str, analysis_summary: str) -> str:
    """í˜ë¥´ì†Œë‚˜ì—ê²Œ ë¶„ì„ ìš”ì²­"""
    prompt = f"{persona_name}, {analysis_summary}"
    payload = {"message": prompt}
    
    try:
        response = requests.post(
            LUMEN_GATEWAY,
            json=payload,
            headers={'Content-Type': 'application/json; charset=utf-8'},
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        return result.get('response', 'No response')
    except Exception as e:
        return f"Error: {str(e)}"

def enhance_with_personas(data: Dict[str, Any]) -> Dict[str, Any]:
    """í˜ë¥´ì†Œë‚˜ ì¸ì‚¬ì´íŠ¸ë¡œ ë¶„ì„ ê°•í™”"""
    print("\n1ï¸âƒ£ ë¶„ì„ ìš”ì•½ ìƒì„±...")
    summary = generate_analysis_summary(data)
    
    print("\n2ï¸âƒ£ ì„¸ë‚˜ (âœ’ï¸)ì—ê²Œ í•™ìŠµ ê°€ì¹˜ ë¶„ì„ ìš”ì²­...")
    sena_insight = query_persona("ì„¸ë‚˜", summary)
    print(f"   ì‘ë‹µ: {sena_insight[:100]}...")
    
    print("\n3ï¸âƒ£ ë£¨ë¹— (ğŸª¨)ì—ê²Œ ì‹¤ìš©ì  ì ìš© ë°©ë²• ìš”ì²­...")
    rubit_prompt = f"{summary}\n\nì´ ì§€ì‹ì„ ì‹¤ì œë¡œ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ”ì§€ 2ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
    rubit_insight = query_persona("ë£¨ë¹—", rubit_prompt)
    print(f"   ì‘ë‹µ: {rubit_insight[:100]}...")
    
    # ê°•í™”ëœ ë°ì´í„° ìƒì„±
    enhanced = {
        "original_data": {
            "video_id": data.get('video_id'),
            "title": data.get('title'),
            "duration": data.get('duration'),
            "subtitles_count": data.get('subtitles_count', 0)
        },
        "timestamp": datetime.now().isoformat(),
        "lumen_insights": {
            "sena_learning_value": sena_insight,
            "rubit_practical_application": rubit_insight
        }
    }
    
    return enhanced

def save_enhanced_result(enhanced: Dict[str, Any], video_id: str):
    """ê°•í™”ëœ ê²°ê³¼ ì €ì¥"""
    # JSON ì €ì¥
    json_file = OUTPUT_DIR / f"youtube_enhanced_{video_id}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(enhanced, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… JSON ì €ì¥: {json_file}")
    
    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
    md_file = OUTPUT_DIR / f"youtube_enhanced_{video_id}.md"
    
    with open(md_file, 'w', encoding='utf-8') as f:
        original = enhanced['original_data']
        insights = enhanced['lumen_insights']
        duration_min = int(original['duration'] / 60)
        
        f.write(f"""# YouTube í•™ìŠµ ê°•í™” ë¦¬í¬íŠ¸

**ìƒì„± ì‹œê°**: {enhanced['timestamp']}

## ğŸ“º ì˜ìƒ ì •ë³´

- **ì œëª©**: {original['title']}
- **ê¸¸ì´**: {duration_min}ë¶„
- **ìë§‰ ìˆ˜**: {original['subtitles_count']}ê°œ

---

## ğŸ’¡ ì„¸ë‚˜ (âœ’ï¸) - í•™ìŠµ ê°€ì¹˜ ë¶„ì„

{insights['sena_learning_value']}

---

## ğŸ”§ ë£¨ë¹— (ğŸª¨) - ì‹¤ìš©ì  ì ìš© ë°©ë²•

{insights['rubit_practical_application']}

---

*ì´ ë¦¬í¬íŠ¸ëŠ” YouTube + ë£¨ë©˜ í†µí•© ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
""")
    
    print(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥: {md_file}")

def main():
    print("\nğŸ¬ YouTube í•™ìŠµ + ë£¨ë©˜ ê°•í™”\n")
    print("=" * 60)
    
    # 1. ìµœì‹  ë¶„ì„ íŒŒì¼ ì°¾ê¸°
    print("\n1ï¸âƒ£ ìµœì‹  YouTube ë¶„ì„ ì°¾ê¸°...")
    try:
        latest_file = find_latest_youtube_analysis()
        print(f"   íŒŒì¼: {latest_file.name}")
    except FileNotFoundError as e:
        print(f"   âš ï¸ {e}")
        return
    
    # 2. ë¶„ì„ ë°ì´í„° ë¡œë“œ
    print("\n2ï¸âƒ£ ë¶„ì„ ë°ì´í„° ë¡œë“œ...")
    data = load_youtube_analysis(latest_file)
    print(f"   ì œëª©: {data.get('title', 'Unknown')[:60]}...")
    
    # 3. í˜ë¥´ì†Œë‚˜ ì¸ì‚¬ì´íŠ¸ë¡œ ê°•í™”
    print("\n3ï¸âƒ£ í˜ë¥´ì†Œë‚˜ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ...")
    enhanced = enhance_with_personas(data)
    
    # 4. ê²°ê³¼ ì €ì¥
    print("\n4ï¸âƒ£ ê°•í™”ëœ ê²°ê³¼ ì €ì¥...")
    video_id = data.get('video_id', 'unknown')
    save_enhanced_result(enhanced, video_id)
    
    print("\n" + "=" * 60)
    print("ğŸŠ YouTube í•™ìŠµ ê°•í™” ì™„ë£Œ!\n")
    print(f"ğŸ“‹ ë¦¬í¬íŠ¸: {OUTPUT_DIR / f'youtube_enhanced_{video_id}.md'}")
    print(f"ğŸ“ JSON: {OUTPUT_DIR / f'youtube_enhanced_{video_id}.json'}")

if __name__ == "__main__":
    main()
