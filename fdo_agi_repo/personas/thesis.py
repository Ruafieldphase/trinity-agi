from __future__ import annotations
from typing import Dict, Any
import os
try:
    from fdo_agi_repo.orchestrator.contracts import TaskSpec, PersonaOutput
    from fdo_agi_repo.orchestrator.memory_bus import append_ledger
except ModuleNotFoundError:  # script-run fallback
    from orchestrator.contracts import TaskSpec, PersonaOutput  # type: ignore
    from orchestrator.memory_bus import append_ledger  # type: ignore
import time
import google.generativeai as genai

# Configure Google AI Studio API (no hardcoded default key)
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)  # type: ignore[attr-defined]

# Placeholder fallback function
def _draft_thesis(goal: str, cites_count: int, cite_paths: list[str]) -> str:
    lines = []
    lines.append(f"[THESIS] ëª©í‘œ: {goal}")
    lines.append("- ì ‘ê·¼: 3ë‹¨ê³„ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ (ìš”êµ¬ ë¶„ì„ â†’ ê·¼ê±° í™•ë³´ â†’ ì‚°ì¶œë¬¼ ì‘ì„±)")
    lines.append(f"- ê·¼ê±°: ë¡œì»¬ ì°¸ì¡° {cites_count}ê°œ í™œìš© ({', '.join(cite_paths) if cite_paths else 'N/A'})")
    lines.append("- ì¶œë ¥: sandbox/docs/result.mdì— ì´ˆì•ˆ ì €ì¥ ë° ì¦ê±° ë³´ê°• ê³„íš í¬í•¨")
    return "\n".join(lines)

def run_thesis(task: TaskSpec, plan: Dict[str, Any], tools, conversation_context: str = "") -> PersonaOutput:
    # 1. ë¡œì»¬ RAG ì°¸ì¡° í™•ë³´
    cites = []
    rag_results_text = ""
    try:
        res = tools.call("rag", {"query": task.goal, "top_k": 3})
        rag_hits = res.get("hits", [])
        for it in rag_hits[:3]:
            cites.append({"source": it.get("source", "local"), "pointer": it.get("id", "")})
        rag_results_text = "\n".join([f"- {hit.get('id', '')}: {hit.get('text', '')[:150]}..." for hit in rag_hits])
    except Exception as e:
        append_ledger({"event": "rag_call_failed", "task_id": task.task_id, "persona": "thesis", "error": str(e)})

    # 2. í•™ìŠµ ì»¨í…ìŠ¤íŠ¸(Few-shot) í™•ì¸
    learning_context = plan.get("learning_context", "")

    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ì¦ê±° ê¸°ë°˜ ì¶œë ¥ ê°•í™”
    prompt_parts = [
        "ë‹¹ì‹ ì€ ì°½ì˜ì  ë°œì‚°í˜• í˜ë¥´ì†Œë‚˜(Thesis)ì…ë‹ˆë‹¤. ë¶„ì„ê°€ë¡œì„œ, ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ 3ë‹¨ê³„ ì‹¤í–‰ ê³„íšì„ ì œì•ˆí•˜ê³ , ë¡œì»¬ ê·¼ê±° í™œìš©ì„ ìš°ì„ í•©ë‹ˆë‹¤.",
        "\n\nâš ï¸ **í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ (CRITICAL)**: ",
        "\n1. **ëª¨ë“  ì£¼ì¥ì—ëŠ” êµ¬ì²´ì ì¸ ê·¼ê±° í•„ìˆ˜**: RAG ê²€ìƒ‰ ê²°ê³¼, ì°¸ê³  ë¬¸í—Œ, ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë°˜ë“œì‹œ ì¸ìš©",
        "\n2. **ê·¼ê±° ì—†ëŠ” ì¼ë°˜ë¡  ê¸ˆì§€**: \"ì¼ë°˜ì ìœ¼ë¡œ\", \"ë³´í†µ\" ê°™ì€ ì¶”ìƒì  í‘œí˜„ ëŒ€ì‹  êµ¬ì²´ì  ì¶œì²˜ ëª…ì‹œ",
        "\n3. **ì¸ìš© í˜•ì‹**: [ì¶œì²˜: {íŒŒì¼/ë¬¸ì„œëª…}] ë˜ëŠ” \"<ì°¸ê³ : {ê²€ìƒ‰ê²°ê³¼ID}>\" í˜•íƒœë¡œ ëª…ì‹œì  í‘œê¸°",
        "\n4. **ì¦ê±° ë¶€ì¡± ì‹œ**: RAG ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ëª…ì‹œì ìœ¼ë¡œ \"ì¶”ê°€ ì¡°ì‚¬ í•„ìš”\" ë˜ëŠ” \"ì›¹ ê²€ìƒ‰ ê¶Œì¥\" í‘œê¸°"
    ]
    
    # ëŒ€í™” ë§¥ë½ ì£¼ì… (Phase 2: Persona Context Propagation)
    if conversation_context:
        prompt_parts.append(f"\n\n{conversation_context}")
        prompt_parts.append("\nâš ï¸ **ë§¥ë½ í™œìš© í•„ìˆ˜**: ìœ„ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ê³ , ì¼ê´€ì„± ìˆëŠ” ê³„íšì„ ìˆ˜ë¦½í•˜ì‹­ì‹œì˜¤.")
    
    # RAG ê²€ìƒ‰ ê²°ê³¼ í‰ê°€
    rag_quality_msg = ""
    if rag_results_text:
        rag_quality_msg = f"\nâœ… **ê²€ìƒ‰ ê²°ê³¼ {len(rag_hits)}ê±´ í™•ë³´ë¨** - ì´ë¥¼ ë°˜ë“œì‹œ í™œìš©í•˜ì—¬ ê·¼ê±° ê¸°ë°˜ ê³„íš ìˆ˜ë¦½"
    else:
        rag_quality_msg = "\nâš ï¸ **ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ** - ì¼ë°˜ì  ì§€ì‹ë§Œìœ¼ë¡œ ì‘ì—… ì‹œ ë‚®ì€ í’ˆì§ˆ ì˜ˆìƒ. ì›¹ ê²€ìƒ‰ ë˜ëŠ” ì™¸ë¶€ ì°¸ì¡° í•„ìš”ì„± ëª…ì‹œ ê¶Œì¥"
    prompt_parts.append(rag_quality_msg)
    
    if learning_context:
        prompt_parts.append(f"\n\n--- ê³¼ê±° í•™ìŠµëœ ì„±ê³µ ì‚¬ë¡€ ---\n{learning_context}")
    
    prompt_parts.append(f"\n\n--- í˜„ì¬ ì‘ì—… ---\nëª©í‘œ: {task.goal}")
    prompt_parts.append(f"\n\n--- ì°¸ê³  ê°€ëŠ¥í•œ RAG ê²€ìƒ‰ ê²°ê³¼ ---\n{rag_results_text if rag_results_text else '(ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ì™¸ë¶€ ì¡°ì‚¬ í•„ìš”)'}")
    prompt_parts.append(
        "\n\n=== âš ï¸ CRITICAL: ì¦ê±° ê¸°ë°˜ ê³„íš í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ ===\n"
        "ë‹¤ìŒ 4ë‹¨ê³„ë¥¼ **ë°˜ë“œì‹œ** ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤:\n\n"
        
        "**1ë‹¨ê³„: RAG ê²€ìƒ‰ ê²°ê³¼ ê²€í† **\n"
        "   - ì œê³µëœ RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¨¼ì € ì½ê³  í‰ê°€\n"
        "   - ê° ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„±ê³¼ ì‹ ë¢°ë„ íŒë‹¨\n"
        "   - ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ \"ì¶”ê°€ ì›¹ ê²€ìƒ‰ í•„ìš”: [êµ¬ì²´ì  í‚¤ì›Œë“œ]\" ëª…ì‹œ\n\n"
        
        "**2ë‹¨ê³„: í•µì‹¬ ê·¼ê±° ì„ íƒ (ìµœì†Œ 3ê°œ)**\n"
        "   - RAG ê²°ê³¼ì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ 3ê°œ ì´ìƒì˜ êµ¬ì²´ì  ì¦ê±° ì„ íƒ\n"
        "   - ê° ì¦ê±°ì— ëŒ€í•´ ì¶œì²˜ë¥¼ ëª…í™•íˆ ê¸°ë¡ (íŒŒì¼ëª…, í•¨ìˆ˜ëª…, ì¤„ ë²ˆí˜¸ ë“±)\n"
        "   - ì˜ˆì‹œ: [ì¶œì²˜: pipeline.pyì˜ EvidenceStage í´ë˜ìŠ¤], [ì°¸ê³ : ê²€ìƒ‰ê²°ê³¼ #2ì˜ ì„¤ì • ì˜ˆì‹œ]\n\n"
        
        "**3ë‹¨ê³„: ì¦ê±° ê¸°ë°˜ ì‹¤í–‰ ê³„íš ì‘ì„±**\n"
        "   - ê° ì‘ì—… ë‹¨ê³„ë§ˆë‹¤ **ë°˜ë“œì‹œ** 1ê°œ ì´ìƒì˜ ì¶œì²˜/ê·¼ê±°ë¥¼ ëª…ì‹œ\n"
        "   - ê·¼ê±° ì—†ëŠ” ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ë¡ ì€ \"[ê°€ì •: ì¶”ê°€ ê²€ì¦ í•„ìš”]\"ë¡œ í‘œì‹œ\n"
        "   - ê³„íšì˜ ëª¨ë“  í•µì‹¬ ì£¼ì¥ì— ì¸ìš© í¬í•¨\n\n"
        
        "**4ë‹¨ê³„: í’ˆì§ˆ ìê°€ ê²€ì¦**\n"
        "   - ì‘ì„±í•œ ê³„íšì— 3ê°œ ì´ìƒì˜ êµ¬ì²´ì  ì¶œì²˜ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸\n"
        "   - ê° ì¶œì²˜ê°€ ì‹¤ì œ ì œê³µëœ RAG ê²°ê³¼ì—ì„œ ë‚˜ì™”ëŠ”ì§€ ê²€ì¦\n"
        "   - ë¶€ì¡±í•˜ë©´ 2ë‹¨ê³„ë¡œ ëŒì•„ê°€ ê·¼ê±° ì¶”ê°€\n\n"
        
        "ğŸ“‹ **ì¶œë ¥ í¬ë§· ì˜ˆì‹œ**:\n"
        "```\n"
        "## ì‘ì—… ê³„íš\n\n"
        "### 1. [ì‘ì—…ëª…]\n"
        "[ì¶œì²˜: config.md Line 15-20] ì„¤ì • íŒŒì¼ì— ë”°ë¥´ë©´...\n"
        "[ì°¸ê³ : ê²€ìƒ‰ê²°ê³¼ #3] ìœ ì‚¬ ì‚¬ë¡€ì—ì„œëŠ”...\n\n"
        "### 2. [ì‘ì—…ëª…]\n"
        "[ì¶œì²˜: pipeline.pyì˜ run() ë©”ì„œë“œ] íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ìƒ...\n"
        "```\n\n"
        
        "âš ï¸ **ê²½ê³ **: ì¶œì²˜ ì—†ëŠ” ê³„íšì€ ìë™ìœ¼ë¡œ í’ˆì§ˆ 0.4 ì´í•˜ë¡œ í‰ê°€ë˜ì–´ ì¬ì‘ì—… ìš”êµ¬ë©ë‹ˆë‹¤.\n"
        "           ë°˜ë“œì‹œ ìœ„ 4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¥¼ ë”°ë¼ **ì¦ê±° ê¸°ë°˜ ê³„íš**ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤."
    )
    
    prompt = "".join(prompt_parts)

    # 4. Gemini LLM í˜¸ì¶œ (Google AI Studio API)
    summary = ""
    err_text = None
    t_llm0 = time.perf_counter()
    try:
        model = genai.GenerativeModel("gemini-2.0-flash")  # type: ignore[attr-defined]
        response = model.generate_content(prompt)
        summary = response.text
    except Exception as e:
        err_text = f"{type(e).__name__}: {e}"
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, ê¸°ì¡´ placeholder ë¡œì§ìœ¼ë¡œ í´ë°±
        summary = _draft_thesis(task.goal, len(cites), [c.get("pointer", "") for c in cites])
    
    t_llm1 = time.perf_counter()
    append_ledger({
        "event": "persona_llm_run",
        "task_id": task.task_id,
        "persona": "thesis",
        "provider": "google-ai-studio",
        "model": "gemini-2.0-flash",
        "duration_sec": float(t_llm1 - t_llm0),
        "ok": bool(bool(summary) and not err_text),
        "error": err_text,
        "prompt_chars": len(prompt)
    })

    # 5. PersonaOutput ë°˜í™˜
    return PersonaOutput(
        task_id=task.task_id,
        persona="thesis",
        summary=summary,
        citations=cites,
        actions=[]
    )