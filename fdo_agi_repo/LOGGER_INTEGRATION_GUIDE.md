# Performance Logger í†µí•© ê°€ì´ë“œ

**íŒŒì¼**: `hey_sena_v4.1_cached.py`
**ëª©ì **: Phase 7 ì„±ëŠ¥ ë¡œê¹… í†µí•©
**ë‚ ì§œ**: 2025-10-28

---

## ğŸ“ ìˆ˜ì • ì‚¬í•­

### 1. Import ì¶”ê°€ (ë¼ì¸ 45-50)

**ê¸°ì¡´**:
```python
from orchestrator.tool_registry import ToolRegistry
from response_cache import get_cache

# Initialize performance cache (v4.1 feature)
cache = get_cache()
```

**ìˆ˜ì • í›„**:
```python
from orchestrator.tool_registry import ToolRegistry
from response_cache import get_cache
from tools.performance_logger import get_logger  # NEW

# Initialize performance cache (v4.1 feature)
cache = get_cache()

# Initialize performance logger (Phase 7) # NEW
logger = get_logger()  # NEW
```

---

### 2. conversation_mode_multiturn() í•¨ìˆ˜ ìˆ˜ì •

**ìœ„ì¹˜**: ë¼ì¸ 387-476

#### 2.1 í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ì— ì„¸ì…˜ ì‹œì‘ ì¶”ê°€

**ê¸°ì¡´ (ë¼ì¸ 406)**:
```python
    conversation_history = []
    turn_count = 0
    max_silence_checks = 2
```

**ìˆ˜ì • í›„**:
```python
    conversation_history = []
    turn_count = 0
    max_silence_checks = 2

    # Phase 7: Start performance logging session
    session_id = logger.start_session(metadata={
        "version": "v4.1",
        "llm_enabled": use_llm
    })
    session_start_time = time.time()
```

#### 2.2 ê° turnì—ì„œ ì‘ë‹µ ì‹œê°„ ì¸¡ì • ë° ë¡œê¹…

**ê¸°ì¡´ (ë¼ì¸ 459-472)**:
```python
        # Generate response with LLM!
        response_text = generate_response_with_context(text, conversation_history, use_llm=use_llm)
        print(f"\n[SENA] {response_text}")

        # Save to history
        conversation_history.append({
            "turn": turn_count,
            "user": text,
            "assistant": response_text
        })

        # TTS and play
        print("[TTS] Generating speech...")
        tts_and_play(registry, response_text)
```

**ìˆ˜ì • í›„**:
```python
        # Phase 7: Start turn timer
        turn_start_time = time.time()

        # Generate response with LLM!
        response_text = generate_response_with_context(text, conversation_history, use_llm=use_llm)
        print(f"\n[SENA] {response_text}")

        # Phase 7: Measure response time
        turn_response_time = (time.time() - turn_start_time) * 1000  # ms

        # Save to history
        conversation_history.append({
            "turn": turn_count,
            "user": text,
            "assistant": response_text
        })

        # Phase 7: Detect cache hit (check console output)
        # Note: This is a simplified detection - real implementation should
        # modify generate_response_with_context to return cache_hit status
        cache_hit = "[CACHE HIT]" in str(response_text) or False

        # TTS and play
        print("[TTS] Generating speech...")
        tts_start = time.time()
        tts_success = tts_and_play(registry, response_text)
        tts_time = (time.time() - tts_start) * 1000

        # Phase 7: Log this turn
        total_turn_time = turn_response_time + tts_time
        logger.log_turn(
            question=text,
            answer=response_text,
            response_time_ms=total_turn_time,
            cache_hit=cache_hit,
            llm_tokens=0,  # TODO: Extract from Gemini response if needed
            tts_used=tts_success,
            error=None
        )
```

#### 2.3 ì •ìƒ ì¢…ë£Œ ì‹œ ì„¸ì…˜ ì¢…ë£Œ ì¶”ê°€

**ê¸°ì¡´ (ë¼ì¸ 453-457)**:
```python
        # Check for end conversation
        if detect_end_conversation(text):
            print("\n[END] Ending conversation...")
            goodbye_msg = "Goodbye! Say Hey Sena to wake me again."
            print(f"[SENA] {goodbye_msg}")
            tts_and_play(registry, goodbye_msg)
            return True
```

**ìˆ˜ì • í›„**:
```python
        # Check for end conversation
        if detect_end_conversation(text):
            print("\n[END] Ending conversation...")
            goodbye_msg = "Goodbye! Say Hey Sena to wake me again."
            print(f"[SENA] {goodbye_msg}")
            tts_and_play(registry, goodbye_msg)

            # Phase 7: End session logging
            session_duration = time.time() - session_start_time
            print(f"\nğŸ“Š Session lasted {session_duration:.1f}s with {turn_count} turns")

            # Ask for rating (optional - can be removed for silent operation)
            # rating = input("Rate this session (1-5): ").strip()
            # logger.end_session(user_rating=int(rating) if rating.isdigit() else None)

            logger.end_session(user_rating=None, notes="Normal conversation end")
            return True
```

#### 2.4 íƒ€ì„ì•„ì›ƒ ì¢…ë£Œ ì‹œì—ë„ ì„¸ì…˜ ì¢…ë£Œ ì¶”ê°€

**ê¸°ì¡´ (ë¼ì¸ 436-439)**:
```python
            if max_silence_checks <= 0:
                print("\n[TIMEOUT] Returning to listen mode due to silence...")
                tts_and_play(registry, "I'm going back to sleep. Say Hey Sena to wake me.")
                return True
```

**ìˆ˜ì • í›„**:
```python
            if max_silence_checks <= 0:
                print("\n[TIMEOUT] Returning to listen mode due to silence...")
                tts_and_play(registry, "I'm going back to sleep. Say Hey Sena to wake me.")

                # Phase 7: End session due to timeout
                logger.end_session(user_rating=None, notes="Session ended due to silence timeout")
                return True
```

---

## ğŸ”§ ë” ë‚˜ì€ ìºì‹œ ê°ì§€ (ì„ íƒ ì‚¬í•­)

`generate_response_with_context()` í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ cache_hit ìƒíƒœë¥¼ ë°˜í™˜í•˜ë„ë¡ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**ìˆ˜ì • ì „ (ë¼ì¸ 294-338)**:
```python
def generate_response_with_context(user_text, history, use_llm=True):
    # ... existing code ...
    return response_text
```

**ìˆ˜ì • í›„**:
```python
def generate_response_with_context(user_text, history, use_llm=True):
    """
    Generate response with conversation context
    v4.1: Added response caching for performance
    Phase 7: Returns tuple (response, cache_hit)

    Returns:
        tuple: (response_text, cache_hit_bool)
    """
    # ... existing code up to cache check ...

    if use_llm:
        # ... context summary code ...

        # Check cache
        cached_response = cache.get_text_response(user_text, context_summary)
        if cached_response:
            print(f"[CACHE HIT] Using cached LLM response")
            return cached_response, True  # MODIFIED: Return tuple

        # Cache miss - generate new response
        llm_response, error = generate_llm_response(user_text, history)

        if llm_response:
            print(f"[LLM] Generated response successfully")
            cache.set_text_response(user_text, llm_response, context_summary)
            return llm_response, False  # MODIFIED: Return tuple

        if error:
            print(f"[LLM WARNING] Failed: {error}, falling back to rule-based")

    # Fallback responses ...
    # All return statements need to return tuple
    if "hello" in user_lower or "hi" in user_lower:
        if len(history) == 0:
            return "Hello! How can I help you today?", False  # MODIFIED
        else:
            return "What else can I help with?", False  # MODIFIED

    # ... rest of the function, add ", False" to all return statements ...

    return f"I heard you say: {user_text}. How can I help you with that?", False  # MODIFIED
```

ê·¸ë¦¬ê³  conversation_mode_multiturn()ì—ì„œ ì‚¬ìš©:

```python
        # Generate response with LLM!
        response_text, cache_hit = generate_response_with_context(text, conversation_history, use_llm=use_llm)
        print(f"\n[SENA] {response_text}")
```

---

## ğŸš€ ë¹ ë¥¸ ì ìš© ë°©ë²•

### ë°©ë²• 1: ìˆ˜ë™ í¸ì§‘ (ê¶Œì¥)
1. `hey_sena_v4.1_cached.py` íŒŒì¼ì„ ì—ë””í„°ì—ì„œ ì—´ê¸°
2. ìœ„ì˜ ìˆ˜ì • ì‚¬í•­ì„ í•˜ë‚˜ì”© ì ìš©
3. ì €ì¥

### ë°©ë²• 2: ìƒˆ íŒŒì¼ ìƒì„±
1. `hey_sena_v4.1_cached.py`ë¥¼ ë³µì‚¬í•˜ì—¬ `hey_sena_v4.1_logged.py` ìƒì„±
2. ìœ„ì˜ ìˆ˜ì • ì‚¬í•­ ì ìš©
3. ìƒˆ íŒŒì¼ë¡œ ì‹¤í–‰

### ë°©ë²• 3: Wrapper ìŠ¤í¬ë¦½íŠ¸ (ì„ì‹œ)
ê°„ë‹¨í•œ wrapperë¥¼ ë§Œë“¤ì–´ ê¸°ì¡´ ì½”ë“œì— ë¡œê¹… ì¶”ê°€ (ë‹¤ìŒ ì„¹ì…˜ ì°¸ì¡°)

---

## ğŸ“¦ Wrapper ìŠ¤í¬ë¦½íŠ¸ (ì„ì‹œ ì†”ë£¨ì…˜)

ìƒˆ íŒŒì¼ `run_sena_with_logging.py` ìƒì„±:

```python
#!/usr/bin/env python3
"""
Hey Sena v4.1 with Performance Logging Wrapper
Phase 7 ì„ì‹œ ë¡œê¹… ì†”ë£¨ì…˜
"""

import sys
from pathlib import Path

# Add current directory to path
sys.path.insert(0, str(Path(__file__).parent))

from tools.performance_logger import get_logger
from hey_sena_v4_1_cached import main as original_main
from hey_sena_v4_1_cached import conversation_mode_multiturn as original_conversation

logger = get_logger()

# Monkey patch the conversation function
original_conversation_func = original_conversation

def logged_conversation_mode_multiturn(registry, use_llm=True):
    """Wrapped version with logging"""

    # Start session
    session_id = logger.start_session(metadata={"version": "v4.1", "llm_enabled": use_llm})

    try:
        # Call original
        result = original_conversation_func(registry, use_llm)

        # End session
        logger.end_session(user_rating=None, notes="Wrapper logged session")

        return result

    except Exception as e:
        logger.end_session(user_rating=None, notes=f"Error: {str(e)}")
        raise

# Replace with logged version
import hey_sena_v4_1_cached
hey_sena_v4_1_cached.conversation_mode_multiturn = logged_conversation_mode_multiturn

if __name__ == "__main__":
    original_main()
```

**ì£¼ì˜**: WrapperëŠ” ì™„ì „í•œ turnë³„ ë¡œê¹…ì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì „ì²´ ê¸°ëŠ¥ì„ ìœ„í•´ì„œëŠ” ë°©ë²• 1 ë˜ëŠ” 2ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

---

## âœ… ê²€ì¦ ë°©ë²•

ìˆ˜ì • í›„ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸:

```bash
# êµ¬ë¬¸ ì˜¤ë¥˜ ì²´í¬
python -m py_compile hey_sena_v4.1_cached.py

# ì‹¤í–‰ (ì§§ì€ í…ŒìŠ¤íŠ¸)
python hey_sena_v4.1_cached.py
# "Hey Sena" â†’ ì§ˆë¬¸ 1-2ê°œ â†’ "ê·¸ë§Œ"

# ë¡œê·¸ í™•ì¸
ls -la logs/phase7/sessions/
cat logs/phase7/sessions/session_*.json | head -50

# ë¶„ì„ ì‹¤í–‰
python tools/analyze_phase7_data.py

# ëŒ€ì‹œë³´ë“œ ìƒì„±
python tools/generate_dashboard.py
```

---

## ğŸ“Š ì˜ˆìƒ ì¶œë ¥

ë¡œê±°ê°€ í†µí•©ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹¤ì‹œê°„ í”¼ë“œë°±ì´ í‘œì‹œë©ë‹ˆë‹¤:

```
ğŸ“Š [Logger] Session started: a1b2c3d4

[TURN 1] Listening... (5 seconds)
[YOU SAID] "ì•ˆë…•í•˜ì„¸ìš”"
[SENA] Hello! How can I help you today?
[TTS] Generating speech...
ğŸ“Š [Logger] Turn 1: âš¡ LLM | 1523ms

[TURN 2] Listening... (5 seconds)
[YOU SAID] "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?"
[CACHE HIT] Using cached LLM response
[SENA] Let me check the weather for you...
ğŸ“Š [Logger] Turn 2: ğŸ’š HIT | 8ms

...

[END] Ending conversation...
ğŸ“Š Session lasted 45.3s with 5 turns

============================================================
ğŸ“Š SESSION SUMMARY
============================================================
Session ID: a1b2c3d4
Duration: 45.3s
Turns: 5
Cache Hit Rate: 60.0%
Avg Response Time: 623ms
...
============================================================

ğŸ’¾ [Logger] Session saved: logs\phase7\sessions\session_a1b2c3d4_...json
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

í†µí•© ì™„ë£Œ í›„:
1. âœ… ì²« 10íšŒ ì„¸ì…˜ ìˆ˜í–‰
2. âœ… ë°ì´í„° ë¶„ì„ (`analyze_phase7_data.py`)
3. âœ… ëŒ€ì‹œë³´ë“œ í™•ì¸ (`generate_dashboard.py`)
4. âœ… Phase 7 Day 2 ì™„ë£Œ!

---

**ì‘ì„±ì**: ì„¸ë‚˜ (Claude Code AI Agent)
**ë‚ ì§œ**: 2025-10-28
**Phase**: 7 Day 2 - ë¡œê±° í†µí•©
