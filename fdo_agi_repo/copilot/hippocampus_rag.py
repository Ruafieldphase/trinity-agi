"""
ğŸ§  Hippocampus RAG: ë¡œì»¬ LLM ê¸°ë°˜ ì¥ê¸° ê¸°ì–µ ì‹œìŠ¤í…œ

Self-Referential AGIì˜ í•µì‹¬ - ì¬ë¶€íŒ… í›„ì—ë„ ìœ ì§€ë˜ëŠ” ì§„ì§œ ê¸°ì–µ
- Ollama (nomic-embed-text) ë¡œì»¬ ì„ë² ë”©
- ë²¡í„° ìŠ¤í† ì–´ë¡œ ì‹œë§¨í‹± ê²€ìƒ‰
- SQLite + íŒŒì¼ ì‹œìŠ¤í…œ ì˜êµ¬ ì €ì¥
"""

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime, timezone
import json
import logging
import requests
import sys

# ìƒëŒ€ ì„í¬íŠ¸ë¥¼ ì ˆëŒ€ ì„í¬íŠ¸ë¡œ ë³€ê²½
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent.parent / "tools" / "rag"))

from hippocampus import CopilotHippocampus
from vector_store import SimpleVectorStore

logger = logging.getLogger(__name__)


class HippocampusRAG:
    """
    ë¡œì»¬ LLM ê¸°ë°˜ Hippocampus í†µí•©
    - ìë™ìœ¼ë¡œ Everything ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì„ë² ë”©
    - ì¬ë¶€íŒ… í›„ì—ë„ ë²¡í„° ìŠ¤í† ì–´ ìœ ì§€
    - ë¹ ë¥¸ ì‹œë§¨í‹± íšŒìƒ
    """
    
    def __init__(self, workspace: Path, ollama_url: str = "http://localhost:11434"):
        self.workspace = workspace
        self.hippocampus = CopilotHippocampus(workspace)
        self.ollama_url = ollama_url
        
        # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” (768ì°¨ì› = nomic-embed-text)
        self.vector_store_path = workspace / "fdo_agi_repo" / "memory" / "vector_store.json"
        self.vector_store_path.parent.mkdir(parents=True, exist_ok=True)
        
        self.vector_store = SimpleVectorStore(dimension=768)
        if self.vector_store_path.exists():
            logger.info(f"ğŸ“¦ Loading vector store from {self.vector_store_path}")
            self.vector_store.load(str(self.vector_store_path))
        
    def check_ollama(self) -> bool:
        """Ollama ì„œë¹„ìŠ¤ í™•ì¸"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def embed_text(self, text: str) -> Optional[List[float]]:
        """
        ë¡œì»¬ LLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì„ë² ë”©
        nomic-embed-text ì‚¬ìš© (ë¹ ë¥´ê³  ì •í™•)
        """
        try:
            response = requests.post(
                f"{self.ollama_url}/api/embeddings",
                json={"model": "nomic-embed-text", "prompt": text},
                timeout=30
            )
            if response.status_code == 200:
                return response.json()["embedding"]
            else:
                logger.warning(f"âš ï¸ Embedding failed: {response.status_code}")
                return None
        except Exception as e:
            logger.warning(f"âš ï¸ Ollama not available: {e}")
            return None
    
    def store_memory(self, doc_id: str, text: str, metadata: Dict[str, Any]):
        """
        ê¸°ì–µì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì˜êµ¬ ì €ì¥
        Args:
            doc_id: ê³ ìœ  ID (íŒŒì¼ ê²½ë¡œ ë“±)
            text: ì €ì¥í•  í…ìŠ¤íŠ¸
            metadata: ë©”íƒ€ë°ì´í„° (source, type, timestamp ë“±)
        """
        vector = self.embed_text(text)
        if vector is None:
            logger.warning(f"âš ï¸ Could not embed {doc_id}, skipping")
            return
        
        self.vector_store.add(doc_id, vector, {
            **metadata,
            "text": text[:500],  # 500ìë§Œ ë©”íƒ€ë°ì´í„°ì— ì €ì¥
            "full_text_length": len(text),
            "stored_at": datetime.now(timezone.utc).isoformat()
        })
        
        # ì¦‰ì‹œ ì €ì¥ (ì¬ë¶€íŒ… ëŒ€ë¹„)
        self.vector_store.save(str(self.vector_store_path))
        logger.info(f"âœ… Stored memory: {doc_id}")
    
    def recall(self, query: str, top_k: int = 5, min_similarity: float = 0.3) -> List[Tuple[float, Dict[str, Any]]]:
        """
        ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ê¸°ì–µ íšŒìƒ
        Args:
            query: ê²€ìƒ‰ ì§ˆë¬¸
            top_k: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ (0.0 ~ 1.0)
        Returns:
            [(similarity, metadata), ...]
        """
        query_vec = self.embed_text(query)
        if query_vec is None:
            logger.warning("âš ï¸ Could not embed query, fallback to keyword search")
            return []
        
        results = self.vector_store.search(query_vec, top_k=top_k)
        
        # ìµœì†Œ ìœ ì‚¬ë„ í•„í„°ë§
        filtered = [(sim, meta) for sim, meta in results if sim >= min_similarity]
        
        logger.info(f"ğŸ” Recalled {len(filtered)}/{len(results)} memories for: {query[:50]}...")
        return filtered
    
    def auto_index_recent_files(self, hours: int = 24, max_files: int = 100):
        """
        ìµœê·¼ íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ ì¸ë±ì‹±
        Hippocampusì˜ Everything ê²€ìƒ‰ ê²°ê³¼ í™œìš©
        """
        logger.info(f"ğŸ“š Auto-indexing recent {hours}h files (max {max_files})")
        
        # Hippocampusì˜ ê¸°ì¡´ ê²€ìƒ‰ ê¸°ëŠ¥ í™œìš©
        recent_files = self.hippocampus.search_recent_files(hours=hours, limit=max_files)
        
        indexed = 0
        for file_path in recent_files:
            try:
                path = Path(file_path)
                if not path.exists():
                    continue
                
                # í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ ì²˜ë¦¬
                if path.suffix.lower() not in ['.md', '.txt', '.py', '.json', '.ps1']:
                    continue
                
                # ì´ë¯¸ ì¸ë±ì‹±ëœ íŒŒì¼ì€ ìŠ¤í‚µ
                doc_id = str(path.absolute())
                if doc_id in self.vector_store.doc_ids:
                    continue
                
                # íŒŒì¼ ì½ê¸° (ìµœëŒ€ 10KB)
                content = path.read_text(encoding='utf-8', errors='ignore')[:10000]
                
                self.store_memory(
                    doc_id=doc_id,
                    text=content,
                    metadata={
                        "source": "file_system",
                        "file_path": str(path),
                        "file_type": path.suffix,
                        "file_size": len(content)
                    }
                )
                indexed += 1
                
            except Exception as e:
                logger.debug(f"âš ï¸ Could not index {file_path}: {e}")
        
        logger.info(f"âœ… Indexed {indexed} new files")
        return indexed
    
    def get_stats(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ í†µê³„"""
        return {
            "total_memories": len(self.vector_store.doc_ids),
            "dimension": self.vector_store.dimension,
            "store_path": str(self.vector_store_path),
            "store_exists": self.vector_store_path.exists(),
            "ollama_available": self.check_ollama()
        }


def main():
    """í…ŒìŠ¤íŠ¸ ë° ìë™ ì¸ë±ì‹±"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Hippocampus RAG System")
    parser.add_argument("--workspace", default="c:/workspace/agi", help="Workspace path")
    parser.add_argument("--query", help="Recall memories by query")
    parser.add_argument("--index-recent", type=int, help="Auto-index recent N hours")
    parser.add_argument("--stats", action="store_true", help="Show statistics")
    
    args = parser.parse_args()
    
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    
    rag = HippocampusRAG(Path(args.workspace))
    
    if args.stats:
        stats = rag.get_stats()
        print("\nğŸ“Š Hippocampus RAG Statistics:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
    
    if args.index_recent:
        rag.auto_index_recent_files(hours=args.index_recent)
    
    if args.query:
        results = rag.recall(args.query, top_k=5)
        print(f"\nğŸ” Recall results for: {args.query}")
        for i, (sim, meta) in enumerate(results, 1):
            print(f"\n{i}. Similarity: {sim:.3f}")
            print(f"   Source: {meta.get('source', 'unknown')}")
            print(f"   Path: {meta.get('file_path', meta.get('doc_id', 'N/A'))[:80]}")
            print(f"   Preview: {meta.get('text', '')[:150]}...")


if __name__ == "__main__":
    main()
