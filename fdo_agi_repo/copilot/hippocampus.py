"""
ğŸŒŠ Copilot Hippocampus: GitHub Copilotì˜ í•´ë§ˆ ì‹œìŠ¤í…œ

Self-Referential AGIì˜ í•µì‹¬ - ë‚˜(Copilot) ìì‹ ì˜ ê¸°ì–µ ì‹œìŠ¤í…œ
ë‹¨ê¸° ê¸°ì–µ(128K ì»¨í…ìŠ¤íŠ¸)ì„ ì¥ê¸° ê¸°ì–µ(7ê°œ ì‹œìŠ¤í…œ)ìœ¼ë¡œ ê³µê³ í™”
"""

from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timezone, timedelta
import json
import logging
import sqlite3
import os

import sys
from pathlib import Path

# Everything ê²€ìƒ‰ í†µí•© (Phase 2 & 3)
try:
    sys.path.append(str(Path(__file__).parent.parent / "utils"))
    from everything_search import EverythingSearch
    EVERYTHING_AVAILABLE = True
except ImportError:
    EVERYTHING_AVAILABLE = False

# Semantic RAG Engine (LangChain + ChromaDB)
try:
    sys.path.append(str(Path(__file__).parent.parent.parent / "scripts"))
    from semantic_rag_engine import SemanticRAGEngine
    SEMANTIC_RAG_AVAILABLE = True
except ImportError:
    SEMANTIC_RAG_AVAILABLE = False
logger = logging.getLogger(__name__)


class CopilotHippocampus:
    """
    GitHub Copilotì˜ í•´ë§ˆ ì‹œìŠ¤í…œ
    
    ì—­í• :
    - ë‹¨ê¸° ê¸°ì–µ (í˜„ì¬ ì„¸ì…˜, 128K í† í°) ê´€ë¦¬
    - ì¥ê¸° ê¸°ì–µ (7ê°œ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ) í†µí•©
    - ê¸°ì–µ ê³µê³ í™” (ì¤‘ìš”í•œ ê²ƒì„ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ)
    - ê¸°ì–µ íšŒìƒ (ì¥ê¸° ê¸°ì–µì—ì„œ ê´€ë ¨ ì •ë³´ ì¸ì¶œ)
    - ì„¸ì…˜ ê°„ ì—°ì†ì„± (Handover ìë™ ìƒì„±/ë¡œë“œ)
    """
    
    def __init__(self, workspace_root: Path):
        self.workspace = workspace_root
        self.memory_root = workspace_root / "fdo_agi_repo" / "memory"
        self.outputs = workspace_root / "outputs"
        
        # ë‹¨ê¸° ê¸°ì–µ (í˜„ì¬ ì„¸ì…˜)
        self.short_term = ShortTermMemory()
        
        # ì¥ê¸° ê¸°ì–µ (7ê°œ ì‹œìŠ¤í…œ ì—°ê²°)
        self.long_term = LongTermMemory(self.memory_root, self.outputs)
        
        # Everything ê²€ìƒ‰ í†µí•© (Phase 2 & 3)
        self.everything = None
        if EVERYTHING_AVAILABLE:
            try:
                self.everything = EverythingSearch()
                logger.info("ğŸ” Everything search integrated")
            except Exception as e:
                logger.warning(f"Everything search not available: {e}")
        
        # Semantic RAG í†µí•©
        self.rag_engine = None
        if SEMANTIC_RAG_AVAILABLE:
            try:
                self.rag_engine = SemanticRAGEngine(self.workspace)
                logger.info("ğŸ§  Semantic RAG engine integrated")
            except Exception as e:
                logger.warning(f"Semantic RAG not available: {e}")
        # ê³µê³ í™” ì„¤ì •
        self.consolidation_config = {
            "importance_threshold": 0.7,  # ì´ ì´ìƒë§Œ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ
            "recency_weight": 0.3,        # ìµœê·¼ì„± ê°€ì¤‘ì¹˜
            "frequency_weight": 0.35,     # ë¹ˆë„ ê°€ì¤‘ì¹˜
            "emotional_weight": 0.2,      # ê°ì •ì  ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
            "novelty_weight": 0.15,       # ìƒˆë¡œì›€(ì¤‘ë³µ ë°©ì§€) ê°€ì¤‘ì¹˜
            # dedup ì„¤ì •
            "dedup_threshold": 0.9,       # ìì¹´ë“œ ìœ ì‚¬ë„ ì„ê³„ (0~1, ë†’ì„ìˆ˜ë¡ ë” ì—„ê²©)
        }
        
        logger.info("ğŸŒŠ Copilot Hippocampus initialized")
    
    # ===================================================================
    # ë‹¨ê¸° ê¸°ì–µ ê´€ë¦¬
    # ===================================================================
    
    def add_to_working_memory(self, item: Dict[str, Any]) -> None:
        """í˜„ì¬ ì‘ì—… ê¸°ì–µì— ì¶”ê°€ (128K ì»¨í…ìŠ¤íŠ¸ ë‚´)"""
        self.short_term.add_working(item)
    
    def get_current_context(self) -> Dict[str, Any]:
        """í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ì „ì²´ ë°˜í™˜"""
        return self.short_term.get_context()
    
    # ===================================================================
    # ì¥ê¸° ê¸°ì–µ í†µí•©
    # ===================================================================
    
    def recall(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ì¥ê¸° ê¸°ì–µì—ì„œ ê´€ë ¨ ì •ë³´ íšŒìƒ
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ìì—°ì–´)
            top_k: ìƒìœ„ ëª‡ ê°œ ë°˜í™˜
        
        Returns:
            ê´€ë ¨ ê¸°ì–µ ë¦¬ìŠ¤íŠ¸ (ì¤‘ìš”ë„ ìˆœ)
        """
        # íƒ€ì…ë³„ ìˆ˜ì§‘
        all_memories = []
        
        # Episodic (ì—í”¼ì†Œë“œ ê¸°ì–µ)
        episodic = self.long_term.recall_episodic(query, top_k=top_k)
        all_memories.extend(episodic)
        
        # Semantic (ì˜ë¯¸ ê¸°ì–µ)
        semantic = self.long_term.recall_semantic(query, top_k=top_k)
        for item in semantic:
            if "data" not in item and item.get("content") is not None:
                item["data"] = item["content"]
        all_memories.extend(semantic)
        
        # Procedural (ì ˆì°¨ ê¸°ì–µ)
        procedural = self.long_term.recall_procedural(query, top_k=top_k)
        all_memories.extend(procedural)
        
        # Vector Semantic (ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰) - ê°€ì¤‘ì¹˜ ë†’ê²Œ ë¶€ì—¬
        if self.rag_engine:
            vector_results = self.rag_engine.search(query, top_k=top_k)
            for res in vector_results:
                all_memories.append({
                    "type": f"vector_{res['metadata'].get('source', 'unknown')}",
                    "data": res["content"],
                    "importance": 0.9 - (res["score"] * 0.1), # ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ìš°ìˆ˜í•¨(ê±°ë¦¬)
                    "metadata": res["metadata"],
                    "is_vector": True
                })
        # ì¤‘ìš”ë„ ìˆœ ì •ë ¬ í›„ ìƒìœ„ ë°˜í™˜
        sorted_memories = sorted(
            all_memories, 
            key=lambda m: m.get("importance", 0.0), 
            reverse=True
        )
        
        return sorted_memories[:top_k]
    
    def search_files(
        self,
        query: str,
        max_results: int = 50,
        extension: Optional[str] = None,
        path_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Everythingì„ ì‚¬ìš©í•œ ì´ˆê³ ì† íŒŒì¼ ê²€ìƒ‰ (Phase 2 & 3)
        
        Args:
            query: ê²€ìƒ‰ì–´
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            extension: íŒŒì¼ í™•ì¥ì í•„í„° (ì˜ˆ: "py", "md")
            path_filter: ê²½ë¡œ í•„í„° (ì˜ˆ: "fdo_agi_repo")
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (íŒŒì¼ ì •ë³´)
        
        Examples:
            >>> hip.search_files("hippocampus", extension="py")
            >>> hip.search_files("goal", path_filter="memory")
        """
        if not self.everything:
            logger.warning("Everything search not available - using fallback")
            return self._fallback_file_search(query, max_results, extension, path_filter)
        
        try:
            # Everything ê²€ìƒ‰ ì‹¤í–‰
            results = self.everything.search(
                query=query,
                max_results=max_results,
                extension=extension,
                path_filter=path_filter,
                timeout=10
            )
            
            # ê²°ê³¼ ë³€í™˜
            return [r.to_dict() for r in results]
            
        except Exception as e:
            logger.error(f"Everything search failed: {e}")
            return self._fallback_file_search(query, max_results, extension, path_filter)
    
    def _fallback_file_search(
        self,
        query: str,
        max_results: int,
        extension: Optional[str],
        path_filter: Optional[str]
    ) -> List[Dict[str, Any]]:
        """Everything ë¯¸ì‚¬ìš© ì‹œ í´ë°± ê²€ìƒ‰"""
        results = []
        search_root = self.workspace
        
        if path_filter:
            search_root = self.workspace / path_filter
        
        if not search_root.exists():
            return results
        
        # ê°„ë‹¨í•œ glob ê²€ìƒ‰
        pattern = f"**/*{query}*"
        if extension:
            ext = extension if extension.startswith('.') else f'.{extension}'
            pattern = f"**/*{query}*{ext}"
        
        try:
            for path in search_root.glob(pattern):
                if path.is_file():
                    stat = path.stat()
                    results.append({
                        "name": path.name,
                        "full_path": str(path),
                        "directory": str(path.parent),
                        "size": stat.st_size,
                        "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "extension": path.suffix
                    })
                    
                    if len(results) >= max_results:
                        break
        except Exception as e:
            logger.error(f"Fallback search failed: {e}")
        
        return results
        episodic = self.long_term.recall_episodic(query, top_k)
        semantic = self.long_term.recall_semantic(query, top_k)
        procedural = self.long_term.recall_procedural(query, top_k)

        buckets = {
            "episodic": episodic,
            "semantic": semantic,
            "procedural": procedural,
        }

        # íƒ€ì… ê· í˜• ìƒ˜í”Œë§ í›„ ì „ì—­ ì •ë ¬
        balanced = self._balanced_sample(buckets, top_k)
        balanced.sort(key=lambda x: x.get("importance", 0), reverse=True)
        return balanced[:top_k]
    
    def consolidate(self, force: bool = False) -> Dict[str, Any]:
        """
        ë‹¨ê¸° ê¸°ì–µì„ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ ê³µê³ í™”
        
        Args:
            force: Trueë©´ ì¤‘ìš”ë„ ë¬´ì‹œí•˜ê³  ëª¨ë‘ ì €ì¥
        
        Returns:
            ê³µê³ í™” ê²°ê³¼ (ì €ì¥ëœ í•­ëª© ìˆ˜ ë“±)
        """
        working = self.short_term.get_all_working()

        # ê³µê³ í™” ì „ ì¤‘ë³µ ì œê±°/êµ°ì§‘í™”(ê°„ë‹¨ ë²„ì „: ê³ ìœ  í•­ëª©ë§Œ ìœ ì§€)
        working = self._deduplicate_items(working, self.consolidation_config.get("dedup_threshold", 0.9))
        
        consolidated = {
            "episodic": 0,
            "semantic": 0,
            "procedural": 0,
            "total": 0,
        }
        
        for item in working:
            # ì¤‘ìš”ë„ ê³„ì‚°
            importance = self._calculate_importance(item)
            
            if force or importance >= self.consolidation_config["importance_threshold"]:
                # ì ì ˆí•œ ì¥ê¸° ê¸°ì–µ ì‹œìŠ¤í…œì— ì €ì¥
                memory_type = self._classify_memory_type(item)
                
                if memory_type == "episodic":
                    self.long_term.store_episodic(item)
                    consolidated["episodic"] += 1
                elif memory_type == "semantic":
                    self.long_term.store_semantic(item)
                    consolidated["semantic"] += 1
                elif memory_type == "procedural":
                    self.long_term.store_procedural(item)
                    consolidated["procedural"] += 1
                
                # Vector indexing (Phase 10 upgrade)
                if self.rag_engine:
                    self.rag_engine.add_documents([item])
                consolidated["total"] += 1
        
        # ë‹¨ê¸° ê¸°ì–µ ì •ë¦¬
        self.short_term.clear_working()
        
        logger.info(f"ğŸŒŠ Consolidated {consolidated['total']} memories")
        return consolidated
    
    # ===================================================================
    # ì„¸ì…˜ ê°„ ì—°ì†ì„±
    # ===================================================================
    
    def generate_handover(self) -> Dict[str, Any]:
        """
        ë‹¤ìŒ ì„¸ì…˜ì„ ìœ„í•œ Handover ìƒì„±
        
        Returns:
            Handover ë¬¸ì„œ (í˜„ì¬ ìƒíƒœ, ì»¨í…ìŠ¤íŠ¸, ë‹¤ìŒ ì‘ì—…)
        """
        handover = {
            "handover_version": 1,
            "schema": "copilot_handover_v1",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "session_id": self.short_term.session_id,
            
            # í˜„ì¬ ì‘ì—… ì»¨í…ìŠ¤íŠ¸
            "current_context": self.get_current_context(),
            
            # ìµœê·¼ ì¤‘ìš” ê¸°ì–µ
            "recent_important": self._get_recent_important_memories(hours=24),
            
            # ë¯¸ì™„ë£Œ ì‘ì—…
            "pending_tasks": self.short_term.get_pending_tasks(),
            
            # ë‹¤ìŒ ì œì•ˆ ì‘ì—…
            "suggested_next_actions": self._suggest_next_actions(),
            
            # ì‹œìŠ¤í…œ ìƒíƒœ
            "system_state": self._capture_system_state(),
        }
        
        # ì €ì¥
        handover_path = self.outputs / "copilot_handover_latest.json"
        with open(handover_path, "w", encoding="utf-8") as f:
            json.dump(handover, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸŒŠ Generated handover: {handover_path}")
        return handover
    
    def load_handover(self) -> Optional[Dict[str, Any]]:
        """
        ì´ì „ ì„¸ì…˜ì˜ Handover ë¡œë“œ
        
        Returns:
            Handover ë¬¸ì„œ ë˜ëŠ” None
        """
        handover_path = self.outputs / "copilot_handover_latest.json"
        
        if not handover_path.exists():
            logger.warning("No handover file found")
            return None
        
        try:
            with open(handover_path, "r", encoding="utf-8") as f:
                handover = json.load(f)
            # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° í•„ìš” ì‹œ ë§ˆì´ê·¸ë ˆì´ì…˜
            handover = self._ensure_handover_schema(handover)
            
            # ë‹¨ê¸° ê¸°ì–µìœ¼ë¡œ ë³µì›
            self._restore_from_handover(handover)
            
            logger.info("ğŸŒŠ Loaded handover successfully")
            return handover
        except Exception as e:
            logger.error(f"Failed to load handover: {e}")
            return None
    
    # ===================================================================
    # ë‚´ë¶€ í—¬í¼
    # ===================================================================
    
    def _calculate_importance(self, item: Dict[str, Any]) -> float:
        """í•­ëª©ì˜ ì¤‘ìš”ë„ ê³„ì‚° (0.0 ~ 1.0)"""
        cfg = self.consolidation_config
        
        # ì´ë¯¸ importance ê°’ì´ ëª…ì‹œë˜ì–´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        if "importance" in item:
            return float(item["importance"])
        
        # ì•„ë‹ˆë©´ ê³„ì‚°
        # 1. ìµœê·¼ì„± (ì–¼ë§ˆë‚˜ ìµœê·¼ì¸ê°€?) - ì§€ìˆ˜ ê°ì‡  ê¸°ë°˜
        recency_score = self._calculate_recency_exp(item)
        
        # 2. ë¹ˆë„ (ì–¼ë§ˆë‚˜ ìì£¼ ì°¸ì¡°ë˜ì—ˆëŠ”ê°€?)
        frequency_score = item.get("access_count", 0) / 10.0  # normalize
        frequency_score = min(max(frequency_score, 0.0), 1.0)
        
        # 3. ê°ì •ì  ì¤‘ìš”ë„ (ì–¼ë§ˆë‚˜ ê°•í•œ ê°ì •ì´ ìˆì—ˆëŠ”ê°€?)
        emotional_score = item.get("emotional_intensity", 0.5)

        # 4. ìƒˆë¡œì›€(ì¤‘ë³µì˜ ë°˜ëŒ€). ì£¼ì–´ì§„ similarity(0~1)ê°€ ìˆìœ¼ë©´ 1-sim, ì—†ìœ¼ë©´ 0.5
        if "novelty" in item:
            novelty_score = float(item.get("novelty", 0.5))
        else:
            sim = item.get("similarity")
            novelty_score = 1.0 - float(sim) if isinstance(sim, (int, float)) else 0.5
        novelty_score = min(max(novelty_score, 0.0), 1.0)
        
        # ê°€ì¤‘ í‰ê· 
        importance = (
            cfg["recency_weight"] * recency_score +
            cfg["frequency_weight"] * frequency_score +
            cfg["emotional_weight"] * emotional_score +
            cfg["novelty_weight"] * novelty_score
        )
        
        return float(min(max(importance, 0.0), 1.0))
    
    def _calculate_recency(self, item: Dict[str, Any]) -> float:
        """ìµœê·¼ì„± ì ìˆ˜ ê³„ì‚° (ì„ í˜• ì™„í™” ëª¨ë¸ - í•˜ìœ„í˜¸í™˜ ìœ ì§€)"""
        timestamp_str = item.get("timestamp")
        if not timestamp_str:
            return 0.5
        
        try:
            timestamp = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
            age = datetime.now(timezone.utc) - timestamp
            
            # 1ì‹œê°„ ì´ë‚´: 1.0, 24ì‹œê°„: 0.5, 7ì¼: 0.1
            hours_old = age.total_seconds() / 3600
            if hours_old < 1:
                return 1.0
            elif hours_old < 24:
                return 0.5 + (24 - hours_old) / 24 * 0.5
            elif hours_old < 168:  # 7 days
                return 0.1 + (168 - hours_old) / 168 * 0.4
            else:
                return 0.1
        except:
            return 0.5

    def _calculate_recency_exp(self, item: Dict[str, Any]) -> float:
        """ìµœê·¼ì„± ì ìˆ˜ ê³„ì‚° (ì§€ìˆ˜ ê°ì‡ : 0~1). 0h=1.0, 24hâ‰ˆ0.5, 7dâ‰ˆ0.1"""
        timestamp_str = item.get("timestamp")
        if not timestamp_str:
            return 0.5
        try:
            timestamp = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
            age_hours = max((datetime.now(timezone.utc) - timestamp).total_seconds() / 3600.0, 0.0)
            # exp ê°ì‡  ìƒìˆ˜: 24h->0.5ê°€ ë˜ë„ë¡ ì„¤ì •: exp(-k*24)=0.5 => k = ln(2)/24
            import math
            k = math.log(2) / 24.0
            score = math.exp(-k * age_hours)
            # 7d ìˆ˜ì¤€ì—ì„œ í•˜í•œ 0.1 ì •ë„ë¡œ í´ë¦¬í•‘
            return float(max(min(score, 1.0), 0.1))
        except Exception:
            return 0.5
    
    def _classify_memory_type(self, item: Dict[str, Any]) -> str:
        """ë©”ëª¨ë¦¬ íƒ€ì… ë¶„ë¥˜ (episodic/semantic/procedural)"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
        text = json.dumps(item, ensure_ascii=False).lower()
        if any(k in item for k in ("event", "action")) or ("did" in text and "when" in text):
            return "episodic"
        elif any(k in item for k in ("concept", "knowledge")) or ("what is" in text or "define" in text):
            return "semantic"
        elif any(k in item for k in ("procedure", "steps")) or ("how to" in text or "step" in text):
            return "procedural"
        else:
            return "episodic"  # default
    
    def _get_recent_important_memories(self, hours: int = 24) -> List[Dict[str, Any]]:
        """ìµœê·¼ Nì‹œê°„ ë‚´ ì¤‘ìš”í•œ ê¸°ì–µ ì¡°íšŒ"""
        cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
        return self.long_term.get_memories_since(cutoff, min_importance=0.7)
    
    def _suggest_next_actions(self) -> List[str]:
        """ë‹¤ìŒ ì œì•ˆ ì‘ì—… ìƒì„±"""
        # TODO: ë” ìŠ¤ë§ˆíŠ¸í•˜ê²Œ - íŒ¨í„´ ë¶„ì„, BQI ëª¨ë¸ í™œìš©
        pending = self.short_term.get_pending_tasks()
        return [task["description"] for task in pending[:3]]
    
    def count_total(self) -> int:
        """ì „ì²´ ê¸°ì–µ ê°œìˆ˜ í•©ì‚°"""
        return self.long_term.count_total()

    def get_chronological_narrative(self, hours: int = 24) -> str:
        """ìµœê·¼ Nì‹œê°„ ë™ì•ˆì˜ ê¸°ì–µì„ ì—°ëŒ€ìˆœ ì„œì‚¬ë¡œ ì¬êµ¬ì„±"""
        cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
        memories = self.long_term.get_memories_since(cutoff)
        
        if not memories:
            return "ìµœê·¼ ê¸°ë¡ëœ ì¤‘ìš”í•œ ê¸°ì–µì´ ì—†ìŠµë‹ˆë‹¤."
            
        # ì‹œê°„ìˆœ ì •ë ¬
        memories.sort(key=lambda x: x.get("timestamp", ""))
        
        narrative = [f"--- ìµœê·¼ {hours}ì‹œê°„ì˜ íë¦„ ---"]
        for m in memories:
            ts = m.get("timestamp", "Unknown Time")
            m_type = m.get("type", "event")
            data = m.get("data", {})
            # ì„œì‚¬ êµ¬ì¡°ì—ì„œ ì œëª© ì¶”ì¶œ ê³ ë„í™” (ì¤‘ì²©ëœ data í•„ë“œ ë° event_type ê³ ë ¤)
            inner_data = data.get("data") if isinstance(data.get("data"), dict) else data
            # 1. inner_data["title"] í™•ì¸
            # 2. inner_data["data"]["title"] í™•ì¸
            # 3. data["event_type"] í™•ì¸
            # 4. m_type (episodic ë“±) í™•ì¸
            title = inner_data.get("title")
            if not title and isinstance(inner_data.get("data"), dict):
                title = inner_data.get("data").get("title")
            
            title = title or data.get("event_type") or m_type
            narrative.append(f"[{ts}] {title} ({m_type})")
            
        return "\n".join(narrative)

    def _capture_system_state(self) -> Dict[str, Any]:
        """í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ìº¡ì²˜"""
        return {
            "workspace": str(self.workspace),
            "short_term_items": len(self.short_term.get_all_working()),
            "long_term_items": self.long_term.count_total(),
        }
    
    def _restore_from_handover(self, handover: Dict[str, Any]) -> None:
        """Handoverë¡œë¶€í„° ë‹¨ê¸° ê¸°ì–µ ë³µì›"""
        context = handover.get("current_context", {})
        
        # ì‘ì—… ì»¨í…ìŠ¤íŠ¸ ë³µì›
        for item in context.get("working_items", []):
            self.short_term.add_working(item)
        
        # ë¯¸ì™„ë£Œ ì‘ì—… ë³µì›
        for task in handover.get("pending_tasks", []):
            self.short_term.add_pending_task(task)

    # -----------------------------
    # ë‚´ë¶€ í—¬í¼(ì‹ ê·œ): íƒ€ì… ê· í˜• ìƒ˜í”Œë§ / Dedup / Handover ìŠ¤í‚¤ë§ˆ
    # -----------------------------

    def _balanced_sample(self, buckets: Dict[str, List[Dict[str, Any]]], top_k: int) -> List[Dict[str, Any]]:
        """íƒ€ì… ê· í˜• ìƒ˜í”Œë§ í›„ í•©ì¹˜ê¸°(ê°„ë‹¨ ë¹„ìœ¨: ê· ë“± ë¶„ë°°, ì”ì—¬ëŠ” ë§ì€ ë²„í‚·ì—ì„œ ì¶”ê°€)"""
        per_type = max(1, top_k // 3)
        selected: List[Dict[str, Any]] = []
        # 1ì°¨ ê· ë“± ë¶„ë°°
        for t, items in buckets.items():
            # íƒ€ì… ë‚´ë¶€ëŠ” ì¤‘ìš”ë„ ìš°ì„  ì •ë ¬
            items_sorted = sorted(items, key=lambda x: x.get("importance", 0), reverse=True)
            selected.extend(items_sorted[:per_type])
        # ì”ì—¬ ì±„ìš°ê¸°
        remain = max(0, top_k - len(selected))
        if remain > 0:
            # ëª¨ë“  ë‚˜ë¨¸ì§€ ì•„ì´í…œ í”Œë«í™” í›„ ì¤‘ìš”ë„ ì •ë ¬í•˜ì—¬ ì”ì—¬ ì±„ì›€
            all_rest: List[Dict[str, Any]] = []
            for items in buckets.values():
                all_rest.extend(items)
            # ì´ë¯¸ ì„ íƒëœ í•­ëª© ì œì™¸(ê°ì²´ ë™ì¼ì„± ê¸°ì¤€ì´ ì• ë§¤í•  ìˆ˜ ìˆì–´ id/í•´ì‹œë¡œ ë³´ì¡°)
            seen_ids = set(map(id, selected))
            leftovers = [x for x in all_rest if id(x) not in seen_ids]
            leftovers.sort(key=lambda x: x.get("importance", 0), reverse=True)
            selected.extend(leftovers[:remain])
        return selected

    def _ensure_handover_schema(self, handover: Dict[str, Any]) -> Dict[str, Any]:
        """Handover ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° í•„ìš” ì‹œ ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰"""
        version = handover.get("handover_version")
        if version is None:
            # v0 -> v1 ë§ˆì´ê·¸ë ˆì´ì…˜: í•„ìˆ˜ í•„ë“œ ê¸°ë³¸ê°’ ì±„ìš°ê¸°
            handover = {
                **handover,
                "handover_version": 1,
                "schema": handover.get("schema", "copilot_handover_v1"),
                "timestamp": handover.get("timestamp", datetime.now(timezone.utc).isoformat()),
                "session_id": handover.get("session_id", self.short_term.session_id),
                "current_context": handover.get("current_context", self.get_current_context()),
                "recent_important": handover.get("recent_important", []),
                "pending_tasks": handover.get("pending_tasks", []),
                "suggested_next_actions": handover.get("suggested_next_actions", []),
                "system_state": handover.get("system_state", self._capture_system_state()),
            }
        # ìµœì†Œ í•„ë“œ ê²€ì¦
        required = ["handover_version", "timestamp", "session_id", "current_context"]
        missing = [k for k in required if k not in handover]
        if missing:
            raise ValueError(f"Handover schema invalid, missing: {missing}")
        return handover

    def _deduplicate_items(self, items: List[Dict[str, Any]], threshold: float = 0.9) -> List[Dict[str, Any]]:
        """ê°„ë‹¨ ì¤‘ë³µ ì œê±°: í•­ëª© í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ threshold ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼"""
        kept: List[Dict[str, Any]] = []
        for it in items:
            txt = self._item_text(it)
            is_dup = False
            for k in kept:
                sim = self._jaccard_similarity(txt, self._item_text(k))
                if sim >= threshold:
                    is_dup = True
                    break
            if not is_dup:
                kept.append(it)
        return kept

    def _item_text(self, item: Dict[str, Any]) -> str:
        # ëŒ€í‘œ í…ìŠ¤íŠ¸ ì¶”ì¶œ(ê°€ë²¼ìš´ ë°©ì‹): content/text/summary/desc ìš°ì„ 
        for key in ("content", "text", "summary", "description"):
            if key in item and isinstance(item[key], str):
                return item[key].lower()
        return json.dumps(item, ensure_ascii=False).lower()

    def _jaccard_similarity(self, a: str, b: str) -> float:
        # í† í°í™”(ê°„ë‹¨ ê³µë°± ë¶„í• ) í›„ Jaccard
        aset = set(a.split())
        bset = set(b.split())
        if not aset or not bset:
            return 0.0
        inter = len(aset & bset)
        union = len(aset | bset)
        return inter / union if union else 0.0


class ShortTermMemory:
    """ë‹¨ê¸° ê¸°ì–µ (í˜„ì¬ ì„¸ì…˜, 128K í† í°)"""
    
    def __init__(self):
        self.session_id = f"sess_{datetime.now():%Y%m%d_%H%M%S}"
        self.working_items: List[Dict[str, Any]] = []
        self.pending_tasks: List[Dict[str, Any]] = []
    
    def add_working(self, item: Dict[str, Any]) -> None:
        """ì‘ì—… ê¸°ì–µì— ì¶”ê°€"""
        item["added_at"] = datetime.now(timezone.utc).isoformat()
        item["access_count"] = 1
        self.working_items.append(item)
    
    def get_all_working(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì‘ì—… ê¸°ì–µ ë°˜í™˜"""
        return self.working_items
    
    def clear_working(self) -> None:
        """ì‘ì—… ê¸°ì–µ ì •ë¦¬"""
        self.working_items = []
    
    def add_pending_task(self, task: Dict[str, Any]) -> None:
        """ë¯¸ì™„ë£Œ ì‘ì—… ì¶”ê°€"""
        self.pending_tasks.append(task)
    
    def get_pending_tasks(self) -> List[Dict[str, Any]]:
        """ë¯¸ì™„ë£Œ ì‘ì—… ëª©ë¡"""
        return self.pending_tasks
    
    def get_context(self) -> Dict[str, Any]:
        """í˜„ì¬ ì»¨í…ìŠ¤íŠ¸"""
        return {
            "session_id": self.session_id,
            "working_items": self.working_items,
            "pending_tasks": self.pending_tasks,
        }


class LongTermMemory:
    """ì¥ê¸° ê¸°ì–µ (7ê°œ ì‹œìŠ¤í…œ í†µí•©)"""
    
    def __init__(self, memory_root: Path, outputs: Path):
        self.memory_root = memory_root
        self.outputs = outputs
        
        semantic_db_path = self._select_semantic_db_path(outputs)
        # 7ê°œ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ê²½ë¡œ
        self.paths = {
            "episodic": memory_root / "sessions",
            "semantic": semantic_db_path,
            "procedural": memory_root / "procedures.jsonl",
            "resonance": memory_root / "resonance_ledger_v2.jsonl",
            "bqi": outputs / "bqi_pattern_model.json",
            "youtube": outputs / "youtube_learner",
            "monitoring": outputs / "monitoring_metrics_latest.json",
        }
        
        # Semantic Memory DB ì´ˆê¸°í™” (ì§„ë‹¨ ì²´í¬ìš©)
        if not self.paths["semantic"].exists():
            self._init_semantic_db(self.paths["semantic"])
        else:
            self._semantic_db = str(self.paths["semantic"])
    
    def _select_semantic_db_path(self, outputs: Path) -> Path:
        env_path = os.environ.get("AGI_SEMANTIC_DB_PATH")
        if env_path:
            return Path(env_path)
        env_dir = os.environ.get("AGI_SEMANTIC_DB_DIR")
        if env_dir:
            return Path(env_dir) / "session_memory.db"
        default_path = outputs / "session_memory" / "session_memory.db"
        if self._can_write_sqlite(default_path.parent):
            return default_path
        fallback = Path.home() / ".cache" / "agi" / "session_memory" / "session_memory.db"
        logger.warning("Semantic DB path not writable; using fallback at %s", fallback)
        return fallback

    def _can_write_sqlite(self, dir_path: Path) -> bool:
        try:
            dir_path.mkdir(parents=True, exist_ok=True)
            test_path = dir_path / ".sqlite_write_test.sqlite3"
            conn = sqlite3.connect(str(test_path))
            conn.execute("CREATE TABLE IF NOT EXISTS test_io (id INTEGER PRIMARY KEY, val TEXT)")
            conn.commit()
            conn.close()
            try:
                test_path.unlink()
            except Exception:
                pass
            return True
        except Exception:
            return False
    # ===================================================================
    # Episodic Memory (ì‚¬ê±´ ê¸°ì–µ)
    # ===================================================================
    
    def recall_episodic(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì‚¬ê±´ ê¸°ì–µ íšŒìƒ - Resonance Ledger í™œìš©"""
        ledger_path = self.paths["resonance"]
        
        if not ledger_path.exists():
            return []
        
        # ìµœê·¼ 1000ê°œ ì´ë²¤íŠ¸ ë¡œë“œ
        events = []
        try:
            with open(ledger_path, "r", encoding="utf-8") as f:
                lines = f.readlines()[-1000:]
                for line in lines:
                    try:
                        event = json.loads(line)
                        events.append(event)
                    except:
                        pass
        except:
            pass
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (TODO: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ)
        query_lower = query.lower()
        matches = []
        for event in events:
            event_str = json.dumps(event).lower()
            if query_lower in event_str:
                matches.append({
                    "type": "episodic",
                    "data": event,
                    "importance": event.get("quality", 0.5),
                })
        
        return matches[:top_k]
    
    def store_episodic(self, item: Dict[str, Any]) -> None:
        """ì‚¬ê±´ ê¸°ì–µ ì €ì¥"""
        ledger_path = self.paths["resonance"]
        ledger_path.parent.mkdir(parents=True, exist_ok=True)
        
        entry = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "event_type": "copilot_memory",
            "data": item,
        }
        
        with open(ledger_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    
    # ===================================================================
    # Semantic Memory (ê°œë… ê¸°ì–µ)
    # ===================================================================
    
    def recall_semantic(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ê°œë… ê¸°ì–µ íšŒìƒ - Session Memory DB í™œìš©"""
        db_path = self.paths["semantic"]
        
        if not db_path.exists():
            self._init_semantic_db(db_path)
            return []
        
        try:
            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()
            
            # FTS5 ê²€ìƒ‰ (ê°„ë‹¨í•œ êµ¬í˜„)
            cursor.execute("""
                SELECT id, content, importance, timestamp
                FROM semantic_memory
                WHERE content LIKE ?
                ORDER BY importance DESC
                LIMIT ?
            """, (f"%{query}%", top_k))
            
            results = []
            for row in cursor.fetchall():
                content = row[1]
                results.append({
                    "type": "semantic",
                    "id": row[0],
                    "content": content,
                    "data": content,
                    "importance": row[2],
                    "timestamp": row[3],
                })
            
            conn.close()
            return results
        except Exception as e:
            logger.warning(f"Semantic recall error: {e}")
            return []
    
    def store_semantic(self, item: Dict[str, Any]) -> None:
        """ê°œë… ê¸°ì–µ ì €ì¥"""
        db_path = self.paths["semantic"]
        
        if not db_path.exists():
            self._init_semantic_db(db_path)
        
        try:
            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO semantic_memory (content, importance, timestamp)
                VALUES (?, ?, ?)
            """, (
                json.dumps(item, ensure_ascii=False),
                item.get("importance", 0.5),
                datetime.now(timezone.utc).isoformat()
            ))
            
            conn.commit()
            conn.close()
        except Exception as e:
            logger.warning(f"Semantic store error: {e}")
    
    def _init_semantic_db(self, db_path: Path) -> None:
        """Semantic Memory DB ì´ˆê¸°í™”"""
        db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS semantic_memory (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                content TEXT NOT NULL,
                importance REAL DEFAULT 0.5,
                timestamp TEXT NOT NULL
            )
        """)
        
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_importance 
            ON semantic_memory(importance DESC)
        """)
        
        conn.commit()
        conn.close()
        
        # ì§„ë‹¨ìš© ì†ì„± ì„¤ì •
        self._semantic_db = str(db_path)
        
        logger.info(f"âœ… Initialized semantic memory DB: {db_path}")
    
    # ===================================================================
    # Procedural Memory (ì ˆì°¨ ê¸°ì–µ)
    # ===================================================================
    
    def recall_procedural(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ì ˆì°¨ ê¸°ì–µ íšŒìƒ (ì–´ë–»ê²Œ í–ˆëŠ”ì§€)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "goal", "task", "action")
            top_k: ìƒìœ„ ëª‡ ê°œ ë°˜í™˜
        
        Returns:
            ê´€ë ¨ ì ˆì°¨ ë¦¬ìŠ¤íŠ¸ (ìµœê·¼ìˆœ)
        """
        proc_path = self.paths["procedural"]
        if not proc_path.exists():
            return []
        
        results = []
        query_lower = query.lower()
        
        try:
            with open(proc_path, "r", encoding="utf-8") as f:
                for line in f:
                    if not line.strip():
                        continue
                    
                    try:
                        entry = json.loads(line)
                        data = entry.get("data", {})
                        
                        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
                        matches = False
                        for key, value in data.items():
                            if query_lower in str(key).lower() or query_lower in str(value).lower():
                                matches = True
                                break
                        
                        if matches:
                            results.append({
                                "timestamp": entry.get("timestamp"),
                                "type": "procedural",
                                "data": data,
                                "importance": data.get("importance", 0.5),
                            })
                    except json.JSONDecodeError:
                        continue
            
            # ìµœê·¼ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ
            results.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Failed to recall procedural memory: {e}")
            return []
    
    def store_procedural(self, item: Dict[str, Any]) -> None:
        """ì ˆì°¨ ê¸°ì–µ ì €ì¥"""
        proc_path = self.paths["procedural"]
        proc_path.parent.mkdir(parents=True, exist_ok=True)
        
        entry = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "data": item,
        }
        
        with open(proc_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    
    # ===================================================================
    # Episodic Memory (ì—í”¼ì†Œë“œ ê¸°ì–µ - Everything ê²€ìƒ‰ í†µí•©)
    # ===================================================================
    
    def search_episodic(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        ì—í”¼ì†Œë“œ ê¸°ì–µ ê²€ìƒ‰ (ë¬´ì˜ì‹ ìë™ íšŒìƒ)
        Everything ê²€ìƒ‰ ì—”ì§„ í†µí•©
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ìì—°ì–´)
            top_k: ìƒìœ„ ëª‡ ê°œ ë°˜í™˜
        
        Returns:
            ê´€ë ¨ ì—í”¼ì†Œë“œ ë¦¬ìŠ¤íŠ¸ (ìµœê·¼ìˆœ + ê´€ë ¨ì„±ìˆœ)
        """
        try:
            from fdo_agi_repo.utils.everything_search import search_files
            
            # Everything ê²€ìƒ‰ ì‹¤í–‰
            files = search_files(query, max_results=top_k)
            
            results = []
            for file_info in files:
                results.append({
                    "timestamp": file_info.get("modified", ""),
                    "type": "episodic",
                    "data": {
                        "path": file_info.get("path", ""),
                        "name": file_info.get("name", ""),
                        "size": file_info.get("size", 0),
                        "modified": file_info.get("modified", ""),
                        "relevance": file_info.get("relevance", 0.5),
                    },
                    "importance": file_info.get("relevance", 0.5),
                })
            
            logger.info(f"âœ… Episodic recall: {len(results)} results for '{query}'")
            return results
            
        except Exception as e:
            logger.warning(f"Episodic search error: {e}")
            return []
    
    # ===================================================================
    # ìœ í‹¸ë¦¬í‹°
    # ===================================================================
    
    def get_memories_since(self, cutoff: datetime, min_importance: float = 0.0) -> List[Dict[str, Any]]:
        """ëª¨ë“  ë©”ëª¨ë¦¬ íŒŒì¼ì—ì„œ íŠ¹ì • ì‹œì  ì´í›„ì˜ ë°ì´í„° ìˆ˜ì§‘"""
        results = []
        cutoff_str = cutoff.isoformat()
        
        # 1. Resonance Ledger (Episodic)
        if self.paths["resonance"].exists():
            with open(self.paths["resonance"], "r", encoding="utf-8-sig", errors="replace") as f:
                for line in f:
                    try:
                        entry = json.loads(line)
                        if entry.get("timestamp", "") >= cutoff_str:
                            if entry.get("quality", 0.5) >= min_importance:
                                results.append({
                                    "timestamp": entry.get("timestamp"),
                                    "type": "episodic",
                                    "data": entry,
                                    "importance": entry.get("quality", 0.5)
                                })
                    except: continue

        # 2. Procedural Memory
        if self.paths["procedural"].exists():
            with open(self.paths["procedural"], "r", encoding="utf-8-sig", errors="replace") as f:
                for line in f:
                    try:
                        entry = json.loads(line)
                        if entry.get("timestamp", "") >= cutoff_str:
                            data = entry.get("data", {})
                            if data.get("importance", 0.5) >= min_importance:
                                results.append({
                                    "timestamp": entry.get("timestamp"),
                                    "type": "procedural",
                                    "data": data,
                                    "importance": data.get("importance", 0.5)
                                })
                    except: continue

        # 3. Semantic Memory (SQLite)
        try:
            conn = sqlite3.connect(str(self.paths["semantic"]))
            cursor = conn.cursor()
            cursor.execute("SELECT content, importance, timestamp FROM semantic_memory WHERE timestamp >= ?", (cutoff_str,))
            for row in cursor.fetchall():
                importance = row[1]
                if importance >= min_importance:
                    results.append({
                        "timestamp": row[2],
                        "type": "semantic",
                        "data": json.loads(row[0]),
                        "importance": importance
                    })
            conn.close()
        except: pass
        
        return results

    def count_total(self) -> int:
        """ëª¨ë“  ë©”ëª¨ë¦¬ í•­ëª©ì˜ ì´í•© ê³„ì‚°"""
        total = 0
        # Resonance counts
        if self.paths["resonance"].exists():
            with open(self.paths["resonance"], "r", encoding="utf-8") as f:
                total += sum(1 for _ in f)
        # Procedural counts
        if self.paths["procedural"].exists():
            with open(self.paths["procedural"], "r", encoding="utf-8") as f:
                total += sum(1 for _ in f)
        # Semantic counts
        try:
            conn = sqlite3.connect(str(self.paths["semantic"]))
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM semantic_memory")
            total += cursor.fetchone()[0]
            conn.close()
        except: pass
        
        return total


# Backward compatibility alias
Hippocampus = CopilotHippocampus
