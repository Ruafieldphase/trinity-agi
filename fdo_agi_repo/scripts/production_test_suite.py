#!/usr/bin/env python3
"""
Production í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸

P2.2 dotenv ìˆ˜ì • í›„ ì‹œìŠ¤í…œ ì „ì²´ ì•ˆì •ì„± ê²€ì¦
- ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ (20ê°œ)
- Replan ë°œìƒ ì—¬ë¶€ í™•ì¸
- Corrections_enabled ì„¤ì • ê²€ì¦
- Evidence gate ì •ìƒ ì‘ë™ í™•ì¸
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Any

# Repo root ê²½ë¡œ ì¶”ê°€
repo_root = Path(__file__).parent.parent
sys.path.insert(0, str(repo_root))

from orchestrator.pipeline import run_task


# í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
TEST_SCENARIOS = [
    # ì¹´í…Œê³ ë¦¬ 1: ì¼ë°˜ ì‘ì—… (5ê°œ)
    {
        "id": "general_001",
        "title": "ìµœê·¼ AGI ê°œì„ ì‚¬í•­ ìš”ì•½",
        "goal": "AGI ì‹œìŠ¤í…œì˜ ìµœê·¼ 1ì£¼ì¼ê°„ ê°œì„ ì‚¬í•­ì„ ìš”ì•½í•˜ê³  ì„±ê³¼ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "general_002",
        "title": "Replan ë¬¸ì œ ë¶„ì„",
        "goal": "Replanì´ ë°œìƒí•˜ëŠ” ì›ì¸ê³¼ í•´ê²° ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "general_003",
        "title": "Evidence gate ë™ì‘ ì„¤ëª…",
        "goal": "Evidence gateê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "general_004",
        "title": "Config ì„¤ì • ê°€ì´ë“œ",
        "goal": "AGI ì‹œìŠ¤í…œì˜ config ì„¤ì • ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "general_005",
        "title": "dotenv ì‚¬ìš©ë²•",
        "goal": "Pythonì—ì„œ dotenvë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ê³¼ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },

    # ì¹´í…Œê³ ë¦¬ 2: ë³µì¡í•œ ì‘ì—… (5ê°œ)
    {
        "id": "complex_001",
        "title": "AGI ì•„í‚¤í…ì²˜ ë¶„ì„",
        "goal": "AGI ì‹œìŠ¤í…œì˜ ì „ì²´ ì•„í‚¤í…ì²˜ë¥¼ ë¶„ì„í•˜ê³ , Thesis-Antithesis-Synthesis íŒ¨í„´ì˜ ì¥ë‹¨ì ì„ í‰ê°€í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "complex_002",
        "title": "ì„±ëŠ¥ ìµœì í™” ì „ëµ",
        "goal": "Local LLM, Second Pass, Replan ìµœì í™” ì‘ì—…ì˜ ì „ì²´ì ì¸ ì „ëµê³¼ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "complex_003",
        "title": "ì‹œìŠ¤í…œ í†µí•© ë¶„ì„",
        "goal": "Core Gateway, Phase 5 ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜, AGI ì‹œìŠ¤í…œì´ ì–´ë–»ê²Œ í†µí•©ë˜ì–´ ì‘ë™í•˜ëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "complex_004",
        "title": "ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ê³„",
        "goal": "AGI ì‹œìŠ¤í…œì˜ ëª¨ë‹ˆí„°ë§ ì „ëµì„ ì„¤ê³„í•˜ê³ , í•µì‹¬ ì§€í‘œì™€ ì•ŒëŒ ê¸°ì¤€ì„ ì œì•ˆí•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "complex_005",
        "title": "ë°°í¬ ì „ëµ ìˆ˜ë¦½",
        "goal": "AGI ì‹œìŠ¤í…œì˜ Production ë°°í¬ ì „ëµì„ ìˆ˜ë¦½í•˜ê³ , ë¦¬ìŠ¤í¬ì™€ ëŒ€ì‘ ë°©ì•ˆì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },

    # ì¹´í…Œê³ ë¦¬ 3: ê¸°ìˆ ì  ì§ˆë¬¸ (5ê°œ)
    {
        "id": "technical_001",
        "title": "Meta-Cognition êµ¬í˜„",
        "goal": "Meta-Cognition ì‹œìŠ¤í…œì˜ êµ¬í˜„ ì›ë¦¬ì™€ confidence ê³„ì‚° ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "technical_002",
        "title": "RAG í†µí•©",
        "goal": "RAGê°€ AGI ì‹œìŠ¤í…œì— ì–´ë–»ê²Œ í†µí•©ë˜ì–´ ìˆê³ , evidence gateì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "technical_003",
        "title": "LRU Cache ê´€ë¦¬",
        "goal": "@lru_cacheë¥¼ ì‚¬ìš©í•œ config ìºì‹±ì˜ ì¥ë‹¨ì ê³¼ ì£¼ì˜ì‚¬í•­ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "technical_004",
        "title": "í™˜ê²½ë³€ìˆ˜ ìš°ì„ ìˆœìœ„",
        "goal": "Pythonì—ì„œ í™˜ê²½ë³€ìˆ˜ì˜ ìš°ì„ ìˆœìœ„ (.env, export, config file)ì™€ best practiceë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "technical_005",
        "title": "Pydantic ê²€ì¦",
        "goal": "TaskSpecì—ì„œ Pydanticì„ ì‚¬ìš©í•œ ê²€ì¦ì˜ ì¥ì ê³¼ í™œìš© ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },

    # ì¹´í…Œê³ ë¦¬ 4: ë¹„êµ/ë¶„ì„ (5ê°œ)
    {
        "id": "comparison_001",
        "title": "P2.1 vs P2.2 ë¹„êµ",
        "goal": "P2.1 (í”„ë¡¬í”„íŠ¸ ê°œì„ )ê³¼ P2.2 (Evidence gate)ì˜ ì°¨ì´ì ê³¼ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "comparison_002",
        "title": "Before vs After",
        "goal": "dotenv ìˆ˜ì • ì „í›„ì˜ ì‹œìŠ¤í…œ ë™ì‘ ì°¨ì´ë¥¼ ë¹„êµí•˜ê³ , ê°œì„  íš¨ê³¼ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "comparison_003",
        "title": "Local vs Cloud LLM",
        "goal": "Local LLM (Gemini 2.0 Flash)ê³¼ Cloud API ì‚¬ìš©ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "comparison_004",
        "title": "Thesis vs Synthesis",
        "goal": "Thesisì™€ Synthesis í˜ë¥´ì†Œë‚˜ì˜ ì—­í•  ì°¨ì´ì™€ ê°ê°ì˜ ì¤‘ìš”ì„±ì„ ë¹„êµí•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    },
    {
        "id": "comparison_005",
        "title": "í…ŒìŠ¤íŠ¸ vs Production",
        "goal": "í…ŒìŠ¤íŠ¸ í™˜ê²½ê³¼ Production í™˜ê²½ì—ì„œì˜ ì°¨ì´ì ê³¼ ê²€ì¦ ì „ëµì„ ë¹„êµí•´ì£¼ì„¸ìš”",
        "expected": "quality >= 0.6, evidence_ok=True, replan=False"
    }
]


def run_test_scenario(scenario: Dict[str, Any], tool_cfg: Dict[str, Any]) -> Dict[str, Any]:
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
    test_id = scenario["id"]
    print(f"\n{'='*60}")
    print(f"Test: {test_id} - {scenario['title']}")
    print(f"{'='*60}")
    print(f"Goal: {scenario['goal']}")
    print(f"Expected: {scenario['expected']}")

    # TaskSpec ìƒì„±
    spec = {
        "task_id": f"prod_test_{test_id}_{int(time.time())}",
        "title": scenario["title"],
        "goal": scenario["goal"]
    }

    # ì‹¤í–‰
    start_time = time.perf_counter()
    try:
        result = run_task(tool_cfg, spec)
        duration = time.perf_counter() - start_time

        # ê²°ê³¼ ìˆ˜ì§‘
        task_id = result.get("task_id")
        citations = result.get("citations", [])

        # ë ˆì €ì—ì„œ í‰ê°€ ì •ë³´ ì¶”ì¶œ
        eval_info = extract_eval_from_ledger(task_id)

        test_result = {
            "test_id": test_id,
            "title": scenario["title"],
            "task_id": task_id,
            "status": "success",
            "duration_sec": duration,
            "citations_count": len(citations),
            **eval_info
        }

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nResult:")
        print(f"  Status: SUCCESS")
        print(f"  Duration: {duration:.2f}s")
        print(f"  Corrections enabled: {test_result.get('corrections_enabled')}")
        print(f"  Quality: {test_result.get('quality')}")
        print(f"  Evidence OK: {test_result.get('evidence_ok')}")
        print(f"  Replan: {test_result.get('replan')}")
        print(f"  Citations: {len(citations)}")
        print(f"  Evidence correction: {test_result.get('evidence_correction_attempted')}")

        return test_result

    except Exception as e:
        duration = time.perf_counter() - start_time
        print(f"\nResult: FAILED - {type(e).__name__}: {e}")
        return {
            "test_id": test_id,
            "title": scenario["title"],
            "status": "failed",
            "error": f"{type(e).__name__}: {e}",
            "duration_sec": duration
        }


def extract_eval_from_ledger(task_id: str) -> Dict[str, Any]:
    """ë ˆì €ì—ì„œ taskì˜ í‰ê°€ ì •ë³´ ì¶”ì¶œ"""
    ledger_path = repo_root / "memory" / "resonance_ledger.jsonl"

    eval_info = {
        "corrections_enabled": None,
        "quality": None,
        "evidence_ok": None,
        "replan": None,
        "evidence_correction_attempted": False,
        "evidence_correction_added": 0
    }

    try:
        with open(ledger_path, 'r', encoding='utf-8') as f:
            for line in f:
                evt = json.loads(line.strip())
                if evt.get("task_id") == task_id:
                    event_type = evt.get("event")

                    if event_type == "run_config":
                        eval_info["corrections_enabled"] = evt.get("corrections", {}).get("enabled")

                    elif event_type == "eval":
                        # ì²« ë²ˆì§¸ evalë§Œ ì‚¬ìš© (initial evaluation)
                        if eval_info["quality"] is None:
                            eval_info["quality"] = evt.get("quality")
                            eval_info["evidence_ok"] = evt.get("evidence_ok")

                    elif event_type == "rune":
                        # ì²« ë²ˆì§¸ runeë§Œ ì‚¬ìš©
                        if eval_info["replan"] is None:
                            rune = evt.get("rune", {})
                            eval_info["replan"] = rune.get("replan")

                    elif event_type == "evidence_correction":
                        eval_info["evidence_correction_attempted"] = evt.get("attempted", False)
                        eval_info["evidence_correction_added"] = evt.get("added", 0)

    except Exception as e:
        print(f"Warning: Failed to extract eval info from ledger: {e}")

    return eval_info


def analyze_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
    total = len(results)
    successful = sum(1 for r in results if r["status"] == "success")
    failed = total - successful

    # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë§Œ ë¶„ì„
    success_results = [r for r in results if r["status"] == "success"]

    if not success_results:
        return {
            "total_tests": total,
            "successful": 0,
            "failed": total,
            "success_rate": 0.0
        }

    # í•µì‹¬ ì§€í‘œ
    corrections_enabled_count = sum(1 for r in success_results if r.get("corrections_enabled") is True)
    high_quality_count = sum(1 for r in success_results if r.get("quality", 0) >= 0.6)
    evidence_ok_count = sum(1 for r in success_results if r.get("evidence_ok") is True)
    replan_count = sum(1 for r in success_results if r.get("replan") is True)
    evidence_correction_count = sum(1 for r in success_results if r.get("evidence_correction_attempted") is True)

    # í‰ê· ê°’
    avg_quality = sum(r.get("quality", 0) for r in success_results) / len(success_results)
    avg_duration = sum(r.get("duration_sec", 0) for r in success_results) / len(success_results)
    avg_citations = sum(r.get("citations_count", 0) for r in success_results) / len(success_results)

    analysis = {
        "total_tests": total,
        "successful": successful,
        "failed": failed,
        "success_rate": successful / total,

        # P2.2 ê²€ì¦ í•µì‹¬ ì§€í‘œ
        "corrections_enabled_rate": corrections_enabled_count / len(success_results),
        "high_quality_rate": high_quality_count / len(success_results),
        "evidence_ok_rate": evidence_ok_count / len(success_results),
        "replan_rate": replan_count / len(success_results),
        "evidence_correction_rate": evidence_correction_count / len(success_results),

        # í‰ê· ê°’
        "avg_quality": avg_quality,
        "avg_duration_sec": avg_duration,
        "avg_citations": avg_citations,

        # ì ˆëŒ€ê°’
        "replan_count": replan_count,
        "evidence_correction_count": evidence_correction_count,
        "corrections_enabled_count": corrections_enabled_count
    }

    return analysis


def print_summary(analysis: Dict[str, Any], results: List[Dict[str, Any]]):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*60)
    print("PRODUCTION TEST SUMMARY")
    print("="*60)

    print(f"\nğŸ“Š Overall Results:")
    print(f"  Total tests: {analysis['total_tests']}")
    print(f"  Successful: {analysis['successful']} ({analysis['success_rate']*100:.1f}%)")
    print(f"  Failed: {analysis['failed']}")

    print(f"\nğŸ¯ P2.2 Validation (í•µì‹¬ ì§€í‘œ):")
    print(f"  Corrections enabled: {analysis['corrections_enabled_count']}/{analysis['successful']} ({analysis['corrections_enabled_rate']*100:.1f}%)")
    print(f"  High quality (>=0.6): {analysis['successful'] if analysis.get('high_quality_rate') else 0}/{analysis['successful']} ({analysis.get('high_quality_rate', 0)*100:.1f}%)")
    print(f"  Evidence OK: {analysis['successful'] if analysis.get('evidence_ok_rate') else 0}/{analysis['successful']} ({analysis.get('evidence_ok_rate', 0)*100:.1f}%)")
    print(f"  Replan count: {analysis['replan_count']} ({analysis['replan_rate']*100:.1f}%)")
    print(f"  Evidence correction: {analysis['evidence_correction_count']} ({analysis['evidence_correction_rate']*100:.1f}%)")

    print(f"\nğŸ“ˆ Performance:")
    print(f"  Avg quality: {analysis['avg_quality']:.3f}")
    print(f"  Avg duration: {analysis['avg_duration_sec']:.2f}s")
    print(f"  Avg citations: {analysis['avg_citations']:.1f}")

    # P2.2 ê²€ì¦ ê²°ê³¼
    print(f"\nâœ… P2.2 Validation Result:")
    if analysis['corrections_enabled_rate'] >= 0.95 and analysis['replan_rate'] <= 0.10:
        print(f"  ğŸŸ¢ PASS - dotenv ìˆ˜ì • ì •ìƒ ì‘ë™!")
        print(f"    - Corrections enabled: {analysis['corrections_enabled_rate']*100:.1f}% (>= 95%)")
        print(f"    - Replan rate: {analysis['replan_rate']*100:.1f}% (<= 10%)")
    else:
        print(f"  ğŸ”´ FAIL - ì¶”ê°€ ì¡°ì‚¬ í•„ìš”")
        if analysis['corrections_enabled_rate'] < 0.95:
            print(f"    âŒ Corrections enabled: {analysis['corrections_enabled_rate']*100:.1f}% (target >= 95%)")
        if analysis['replan_rate'] > 0.10:
            print(f"    âŒ Replan rate: {analysis['replan_rate']*100:.1f}% (target <= 10%)")

    # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ í‘œì‹œ
    if analysis['failed'] > 0:
        print(f"\nâŒ Failed Tests:")
        for r in results:
            if r["status"] == "failed":
                print(f"  - {r['test_id']}: {r.get('error', 'Unknown error')}")

    # Replan ë°œìƒ ì¼€ì´ìŠ¤ í‘œì‹œ
    if analysis['replan_count'] > 0:
        print(f"\nâš ï¸ Replan Cases:")
        for r in results:
            if r["status"] == "success" and r.get("replan") is True:
                print(f"  - {r['test_id']}: quality={r.get('quality')}, evidence_ok={r.get('evidence_ok')}")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("PRODUCTION INTEGRATION TEST SUITE")
    print("="*60)
    print(f"Start time: {datetime.now(timezone.utc).isoformat()}")
    print(f"Total scenarios: {len(TEST_SCENARIOS)}")
    print(f"Target: Validate P2.2 dotenv fix")
    print("="*60)

    tool_cfg = {}
    results = []

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for i, scenario in enumerate(TEST_SCENARIOS, 1):
        print(f"\n\n[{i}/{len(TEST_SCENARIOS)}] Running test...")
        result = run_test_scenario(scenario, tool_cfg)
        results.append(result)

        # ì§§ì€ íœ´ì‹ (LLM API rate limit ê³ ë ¤)
        if i < len(TEST_SCENARIOS):
            time.sleep(2)

    # ê²°ê³¼ ë¶„ì„
    analysis = analyze_results(results)

    # ìš”ì•½ ì¶œë ¥
    print_summary(analysis, results)

    # ê²°ê³¼ ì €ì¥
    output_dir = repo_root / "outputs"
    output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"production_test_results_{timestamp}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "analysis": analysis,
            "results": results
        }, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“ Results saved to: {output_file}")

    # ìµœì¢… íŒì •
    if analysis['corrections_enabled_rate'] >= 0.95 and analysis['replan_rate'] <= 0.10:
        print(f"\nğŸ‰ P2.2 VALIDATION: PASS")
        return 0
    else:
        print(f"\nâš ï¸ P2.2 VALIDATION: NEEDS INVESTIGATION")
        return 1


if __name__ == "__main__":
    sys.exit(main())
