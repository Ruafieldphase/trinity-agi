#!/usr/bin/env python3
"""
Replan ì›ì¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
Runeì—ì„œ replan=Trueê°€ ë°œìƒí•œ ì¼€ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì„  í¬ì¸íŠ¸ ë„ì¶œ
"""

import sys
import json
from pathlib import Path
from datetime import datetime, timedelta, timezone
from collections import Counter
from typing import Dict, List, Any

# Repo root ê²½ë¡œ ì¶”ê°€
repo_root = Path(__file__).parent.parent
sys.path.insert(0, str(repo_root))


def load_ledger(ledger_path: Path) -> List[Dict[str, Any]]:
    """resonance_ledger.jsonl ë¡œë“œ"""
    events = []
    with open(ledger_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    events.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    return events


def analyze_replan_causes(events: List[Dict[str, Any]], hours: float = 24) -> Dict[str, Any]:
    """Replan ì›ì¸ ë¶„ì„"""
    
    # ì‹œê°„ í•„í„°ë§ (UTC ê¸°ì¤€, aware datetime)
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
    
    # Taskë³„ ì´ë²¤íŠ¸ ê·¸ë£¹í•‘
    task_events: Dict[str, List[Dict]] = {}
    for evt in events:
        # ts(Unix epoch) ìš°ì„  ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ ISO ë¬¸ìì—´(timestamp) ì‚¬ìš©
        evt_time = None
        ts_val = evt.get('ts')
        if isinstance(ts_val, (int, float)):
            try:
                # tsëŠ” UTC epoch secondsë¡œ ê°€ì •
                evt_time = datetime.fromtimestamp(ts_val, tz=timezone.utc)
            except Exception:
                evt_time = None
        if evt_time is None:
            ts_str = evt.get('timestamp', '')
            if ts_str:
                try:
                    dt = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
                    evt_time = dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)
                    evt_time = evt_time.astimezone(timezone.utc)
                except Exception:
                    evt_time = None
        # ì»·ì˜¤í”„ ì´ì „ì´ë©´ ìŠ¤í‚µ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ëŠ” ì´ë²¤íŠ¸ëŠ” ìœ ì§€)
        if evt_time is not None and evt_time < cutoff:
            continue
        
        task_id = evt.get('task_id', '')
        if task_id:
            if task_id not in task_events:
                task_events[task_id] = []
            # ì •ë ¬ì„ ìœ„í•´ ë‚´ë¶€ ì‹œê°„ ë³´ê´€
            if evt_time is not None:
                evt['_evt_time'] = evt_time
            task_events[task_id].append(evt)
    
    # Replan ì¼€ì´ìŠ¤ ì°¾ê¸°
    replan_cases = []
    
    for task_id, task_evts in task_events.items():
        # í…ŒìŠ¤íŠ¸ íƒœìŠ¤í¬ ì œì™¸
        if any(prefix in task_id for prefix in ['integration_test_', 'low_confidence_test_', 'temp_low_conf_']):
            continue

        # ìµœì‹  ì´ë²¤íŠ¸ ìš°ì„ ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìµœê·¼ ìƒíƒœ ë°˜ì˜
        task_evts_sorted = sorted(task_evts, key=lambda e: e.get('_evt_time', cutoff), reverse=True)
        # Rune ì´ë²¤íŠ¸ì—ì„œ replan=True ì°¾ê¸°
        for evt in task_evts_sorted:
            if evt.get('event') == 'rune' and evt.get('rune', {}).get('replan', False):
                # í•´ë‹¹ íƒœìŠ¤í¬ì˜ eval, meta_cognition ì •ë³´ ìˆ˜ì§‘ (ê°€ì¥ ìµœê·¼ ê²ƒì„ ì„ íƒ)
                eval_evt = next((e for e in task_evts_sorted if e.get('event') == 'eval'), None)
                meta_evt = next((e for e in task_evts_sorted if e.get('event') == 'meta_cognition'), None)

                case = {
                    'task_id': task_id,
                    'replan': True,
                }

                # Eval ì •ë³´
                if eval_evt:
                    case['quality'] = eval_evt.get('quality')
                    # min_qualityëŠ” ìƒìœ„ ë° eval ì„œë¸Œí•„ë“œ ëª¨ë‘ì—ì„œ íƒìƒ‰
                    min_q = eval_evt.get('min_quality')
                    if min_q is None:
                        min_q = (eval_evt.get('eval') or {}).get('min_quality')
                    if min_q is None:
                        min_q = 0.6
                    case['min_quality'] = min_q
                    case['evidence_ok'] = eval_evt.get('evidence_ok')
                    if case['quality'] is not None and case['min_quality'] is not None:
                        case['quality_gap'] = case['min_quality'] - case['quality']

                # Meta-cognition ì •ë³´
                if meta_evt:
                    case['confidence'] = meta_evt.get('confidence')
                    case['past_performance'] = meta_evt.get('past_performance')

                # Rune ì •ë³´
                rune = evt.get('rune', {})
                case['rune_confidence'] = rune.get('confidence')
                case['recommendations'] = rune.get('recommendations', [])
                case['reasoning'] = rune.get('reasoning', '')

                replan_cases.append(case)
                break  # í•œ taskë‹¹ ì²« replanë§Œ ê¸°ë¡
    
    # í†µê³„ ê³„ì‚°
    total_cases = len(replan_cases)
    
    if total_cases == 0:
        return {
            'timestamp': datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
            'window_hours': hours,
            'total_cases': 0,
            'statistics': {
                'avg_quality_gap': 0,
                'median_quality_gap': 0,
                'avg_confidence': 0,
                'avg_quality': 0,
                'evidence_failure_rate': 0,
                'quality_failure_rate': 0,
            },
            'top_recommendations': {},
            'reasoning_keywords': {},
            'cases': [],
        }
    
    # Quality gap í†µê³„
    quality_gaps = [c['quality_gap'] for c in replan_cases if 'quality_gap' in c]
    avg_quality_gap = sum(quality_gaps) / len(quality_gaps) if quality_gaps else 0
    # ì¤‘ì•™ê°’ ê³„ì‚° (ì§ìˆ˜ ê°œìˆ˜ì¼ ê²½ìš° ì¤‘ì•™ ë‘ ê°’ì˜ í‰ê· )
    if quality_gaps:
        q_sorted = sorted(quality_gaps)
        n = len(q_sorted)
        mid = n // 2
        if n % 2 == 1:
            median_quality_gap = q_sorted[mid]
        else:
            median_quality_gap = (q_sorted[mid - 1] + q_sorted[mid]) / 2
    else:
        median_quality_gap = 0
    
    # Evidence ë¬¸ì œ
    evidence_issues = sum(1 for c in replan_cases if c.get('evidence_ok') == False)
    evidence_failure_rate = evidence_issues / total_cases if total_cases > 0 else 0
    
    # Quality ë¯¸ë‹¬
    quality_failures = sum(1 for c in replan_cases if c.get('quality_gap', 0) > 0)
    quality_failure_rate = quality_failures / total_cases if total_cases > 0 else 0
    
    # Confidence í†µê³„
    confidences = [c['confidence'] for c in replan_cases if 'confidence' in c]
    avg_confidence = sum(confidences) / len(confidences) if confidences else 0
    
    qualities = [c['quality'] for c in replan_cases if 'quality' in c]
    avg_quality = sum(qualities) / len(qualities) if qualities else 0
    
    # Recommendations ì§‘ê³„
    all_recs = []
    for c in replan_cases:
        all_recs.extend(c.get('recommendations', []))
    rec_counter = Counter(all_recs)
    
    # Reasoning í‚¤ì›Œë“œ ë¶„ì„
    all_reasoning = ' '.join([c.get('reasoning', '') for c in replan_cases])
    reasoning_keywords = []
    for keyword in ['ê·¼ê±°', 'ì¦ê±°', 'í’ˆì§ˆ', 'ëª¨í˜¸', 'ë¶ˆëª…í™•', 'ë¶€ì¡±', 'ë¯¸í¡']:
        if keyword in all_reasoning:
            count = all_reasoning.count(keyword)
            reasoning_keywords.append((keyword, count))
    reasoning_keywords.sort(key=lambda x: x[1], reverse=True)
    
    return {
        'timestamp': datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
        'window_hours': hours,
        'total_cases': total_cases,
        'statistics': {
            'avg_quality_gap': avg_quality_gap,
            'median_quality_gap': median_quality_gap,
            'avg_confidence': avg_confidence,
            'avg_quality': avg_quality,
            'evidence_failure_rate': evidence_failure_rate,
            'quality_failure_rate': quality_failure_rate,
        },
        'top_recommendations': dict(rec_counter.most_common(5)),
        'reasoning_keywords': dict(reasoning_keywords[:5]),
        'cases': replan_cases,
    }


def print_analysis(analysis: Dict[str, Any]):
    """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
    total = analysis['total_cases']
    stats = analysis['statistics']
    
    print(f"\nğŸ” Replan ì›ì¸ ë¶„ì„ ({total} cases)")
    print("=" * 80)
    
    if total == 0:
        print("\nâœ… Replan ì¼€ì´ìŠ¤ ì—†ìŒ (ìµœê·¼ 24ì‹œê°„)")
        return
    
    print(f"\nğŸ“Š Quality & Confidence")
    print(f"   í‰ê·  Quality: {stats['avg_quality']:.3f}")
    print(f"   í‰ê·  Confidence: {stats['avg_confidence']:.3f}")
    print(f"   í‰ê·  Quality Gap: {stats['avg_quality_gap']:.3f}")
    print(f"   ì¤‘ì•™ê°’ Quality Gap: {stats['median_quality_gap']:.3f}")
    
    print(f"\nğŸ“ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„")
    print(f"   Evidence ë¬¸ì œ: {int(stats['evidence_failure_rate'] * 100)}% ({int(stats['evidence_failure_rate'] * total)}/{total} cases)")
    print(f"   Quality ë¯¸ë‹¬: {int(stats['quality_failure_rate'] * 100)}% ({int(stats['quality_failure_rate'] * total)}/{total} cases)")
    
    print(f"\nğŸ’¡ ìì£¼ ë“±ì¥í•˜ëŠ” Recommendations")
    for rec, count in analysis['top_recommendations'].items():
        pct = (count / total) * 100
        print(f"   '{rec}': {count}íšŒ ({pct:.1f}%)")
    
    print(f"\nğŸ”‘ Reasoning í‚¤ì›Œë“œ ë¶„ì„")
    for keyword, count in analysis['reasoning_keywords'].items():
        print(f"   '{keyword}': {count}íšŒ")
    
    print(f"\nğŸ“‹ ê°œë³„ ì¼€ì´ìŠ¤ ìƒì„¸ (ìµœê·¼ 3ê±´)")
    for i, case in enumerate(analysis['cases'][-3:], 1):
        print(f"\n   Case {i}: {case['task_id'][:8]}...")
        if 'quality' in case:
            print(f"      Quality: {case['quality']:.2f} (ëª©í‘œ: {case['min_quality']:.2f}, ë¶€ì¡±: {case.get('quality_gap', 0):.2f})")
        if 'confidence' in case:
            print(f"      Confidence: {case['confidence']:.2f}")
        if 'evidence_ok' in case:
            print(f"      Evidence OK: {case['evidence_ok']}")
        if case.get('recommendations'):
            print(f"      Recommendations: {', '.join(case['recommendations'])}")
    
    print(f"\n\nğŸ¯ ê°œì„  ë°©ì•ˆ (ìš°ì„ ìˆœìœ„)")
    print("=" * 80)
    
    priorities = []
    
    if stats['quality_failure_rate'] > 0.5:
        priorities.append(f"1ï¸âƒ£ CRITICAL: Quality ë¯¸ë‹¬ ë¹„ìœ¨ ë†’ìŒ ({int(stats['quality_failure_rate']*100)}%)")
        priorities.append(f"   â†’ P2.1 í”„ë¡¬í”„íŠ¸ ê°œì„  íš¨ê³¼ í™•ì¸ (ê·¼ê±° ê°•í™”)")
    
    if stats['evidence_failure_rate'] > 0.5:
        priorities.append(f"2ï¸âƒ£ HIGH: Evidence ê²€ì¦ ì‹¤íŒ¨ìœ¨ ë†’ìŒ ({int(stats['evidence_failure_rate']*100)}%)")
        priorities.append(f"   â†’ P2.1ê³¼ ì—°ê³„, í”„ë¡¬í”„íŠ¸ ê°œì„  íš¨ê³¼ 24h í›„ ì¬ì¸¡ì •")
    
    if stats['avg_quality_gap'] > 0.15:
        priorities.append(f"3ï¸âƒ£ MEDIUM: Quality Gap í¼ ({stats['avg_quality_gap']:.3f})")
        priorities.append(f"   â†’ min_quality ì¡°ì • ë˜ëŠ” í‰ê°€ ê¸°ì¤€ ì™„í™” ê³ ë ¤")
    
    if analysis['top_recommendations']:
        top_rec = list(analysis['top_recommendations'].keys())[0]
        priorities.append(f"4ï¸âƒ£ LOW: ë°˜ë³µë˜ëŠ” ì¶”ì²œ '{top_rec}'")
        priorities.append(f"   â†’ Few-shot learningì— í•´ë‹¹ íŒ¨í„´ ì¶”ê°€")
    
    if not priorities:
        priorities.append("âœ… ì£¼ìš” ë¬¸ì œì  ì—†ìŒ, ëª¨ë‹ˆí„°ë§ ì§€ì†")
    
    for p in priorities:
        print(p)


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Replan ì›ì¸ ë¶„ì„')
    parser.add_argument('--hours', type=float, default=24, help='ë¶„ì„ ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸: 24ì‹œê°„)')
    args = parser.parse_args()
    
    # ë ˆì € íŒŒì¼ ë¡œë“œ
    ledger_path = repo_root / "memory" / "resonance_ledger.jsonl"
    
    if not ledger_path.exists():
        print(f"âŒ Error: {ledger_path} not found")
        sys.exit(1)
    
    print(f"ğŸ“‚ Loading: {ledger_path}")
    events = load_ledger(ledger_path)
    print(f"   Total events: {len(events)}")
    
    # ë¶„ì„ ì‹¤í–‰
    analysis = analyze_replan_causes(events, hours=args.hours)
    
    # ê²°ê³¼ ì¶œë ¥
    print_analysis(analysis)
    
    # JSON ì¶œë ¥
    output_path = repo_root / "outputs" / "replan_analysis.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… Analysis exported: {output_path}")


if __name__ == '__main__':
    main()
