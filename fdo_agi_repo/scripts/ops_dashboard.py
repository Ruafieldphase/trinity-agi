#!/usr/bin/env python3
"""
í†µí•© ìš´ì˜ ëŒ€ì‹œë³´ë“œ
AGI + Lumen + Proxy + System ìƒíƒœë¥¼ í•œëˆˆì— í‘œì‹œ
"""

import sys
import json
from pathlib import Path
from typing import Dict, Any

# monitor ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
repo_root = Path(__file__).parent.parent
sys.path.insert(0, str(repo_root))

from monitor.metrics_collector import MetricsCollector


class OpsDashboard:
    """ìš´ì˜ ëŒ€ì‹œë³´ë“œ ìƒì„±"""

    def __init__(self):
        self.collector = MetricsCollector()
        self.colors = {
            'green': '\033[92m',
            'yellow': '\033[93m',
            'red': '\033[91m',
            'cyan': '\033[96m',
            'bold': '\033[1m',
            'reset': '\033[0m',
        }

    def _colorize(self, text: str, color: str) -> str:
        """í…ìŠ¤íŠ¸ì— ìƒ‰ìƒ ì ìš©"""
        return f"{self.colors.get(color, '')}{text}{self.colors['reset']}"

    def _status_icon(self, ok: bool) -> str:
        """ìƒíƒœ ì•„ì´ì½˜"""
        if ok:
            return self._colorize("âœ…", "green")
        else:
            return self._colorize("âŒ", "red")

    def print_dashboard(self, hours: float = 1.0):
        """ëŒ€ì‹œë³´ë“œ ì¶œë ¥ (í„°ë¯¸ë„)"""
        print(self._colorize("\n" + "="*60, "cyan"))
        print(self._colorize("ğŸš€ AGI ìš´ì˜ ëŒ€ì‹œë³´ë“œ", "bold"))
        print(self._colorize("="*60 + "\n", "cyan"))

        # í—¬ìŠ¤ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        health = self.collector.get_health_status()
        metrics_snapshot = self.collector.get_realtime_metrics(hours=hours)
        event_counts = metrics_snapshot.get('event_counts', {})
        metrics_values = metrics_snapshot.get('metrics', {})

        # ì „ì²´ ìƒíƒœ
        overall_icon = self._status_icon(health['healthy'])
        print(f"{overall_icon} {self._colorize('ì „ì²´ ìƒíƒœ:', 'bold')} " +
              (self._colorize("HEALTHY", "green") if health['healthy'] else self._colorize("UNHEALTHY", "red")))
        print()

        # AGI ë©”íŠ¸ë¦­ with Policy Info
        policy = health.get('policy', {})
        recent_hrs = policy.get('recent_hours', 1.0)
        samples = policy.get('samples', {})
        notes = policy.get('notes', {})
        
        print(self._colorize(f"ğŸ“Š AGI ë©”íŠ¸ë¦­ (ìµœê·¼ {recent_hrs}ì‹œê°„, min samples={policy.get('min_samples', {}).get('confidence', 5)})", "bold"))
        
        # Confidence
        conf_note = notes.get('confidence', '')
        conf_samples = samples.get('confidence', 0)
        print(f"  â€¢ Confidence: {health['current_values']['confidence']:.3f} " +
              f"(min: {health['thresholds']['min_confidence']}, samples: {conf_samples}) " +
              self._status_icon(health['checks']['confidence_ok']))
        if conf_note:
            print(f"    â„¹ï¸  {conf_note}")
        
        # Quality
        qual_note = notes.get('quality', '')
        qual_samples = samples.get('quality', 0)
        print(f"  â€¢ Quality:    {health['current_values']['quality']:.3f} " +
              f"(min: {health['thresholds']['min_quality']}, samples: {qual_samples}) " +
              self._status_icon(health['checks']['quality_ok']))
        if qual_note:
            print(f"    â„¹ï¸  {qual_note}")
        
        # Second Pass
        sp_rate = health['current_values']['second_pass_rate']
        print(f"  â€¢ 2nd Pass:   {sp_rate:.3f} " +
              f"(max: {health['thresholds']['max_second_pass_rate']}) " +
              self._status_icon(health['checks']['second_pass_ok']))
        print()

        # Evidence & RAG ë©”íŠ¸ë¦­ í‘œì‹œ
        evidence_metrics = metrics_values.get('evidence_correction', {})
        if evidence_metrics.get('attempts', 0) > 0:
            print(self._colorize(f"ğŸ” Evidence & RAG ë©”íŠ¸ë¦­ (ìµœê·¼ {hours}h)", "bold"))
            
            attempts = evidence_metrics.get('attempts', 0)
            avg_hits = evidence_metrics.get('avg_hits', 0.0)
            avg_added = evidence_metrics.get('avg_added', 0.0)
            avg_relevance = evidence_metrics.get('avg_relevance', 0.0)
            success_rate = evidence_metrics.get('success_rate', 0.0)
            
            # Success rateì— ë”°ë¥¸ ìƒ‰ìƒ
            if success_rate >= 0.5:
                rate_color = "green"
            elif success_rate >= 0.2:
                rate_color = "yellow"
            else:
                rate_color = "red"
            
            print(f"  â€¢ ì‹œë„ íšŸìˆ˜:    {attempts}")
            print(f"  â€¢ ì„±ê³µë¥ :       {self._colorize(f'{success_rate:.1%}', rate_color)}")
            print(f"  â€¢ í‰ê·  hits:    {avg_hits:.2f}")
            print(f"  â€¢ í‰ê·  ì¶”ê°€:    {avg_added:.2f} citations")
            print(f"  â€¢ í‰ê·  ê´€ë ¨ë„:  {avg_relevance:.3f}")
            # Optional: Fallback rate
            fb_rate = evidence_metrics.get('fallback_rate')
            if fb_rate is not None:
                print(f"  â€¢ í´ë°± ë¹„ìœ¨:    {fb_rate:.1%}")
            print()

        # Second Pass ìƒì„¸ ë¶„ì„ (second_pass_analysis.json ìˆìœ¼ë©´ í‘œì‹œ)
        sp_analysis_path = repo_root / "outputs" / "second_pass_analysis.json"
        if sp_analysis_path.exists():
            try:
                with open(sp_analysis_path, 'r', encoding='utf-8') as f:
                    sp_data = json.load(f)
                
                print(self._colorize("ğŸ” Second Pass ë¶„ì„ (24h)", "bold"))
                
                total_cases = sp_data.get('total_cases', 0)
                cases = sp_data.get('cases', [])
                
                # Evidence failure rate ê³„ì‚°
                evidence_failures = sum(1 for c in cases if not c.get('evidence_ok', True))
                evidence_failure_rate = evidence_failures / total_cases if total_cases > 0 else 0
                
                # Quality gap í‰ê·  ê³„ì‚°
                quality_gaps = [c.get('quality_gap', 0) for c in cases]
                avg_quality_gap = sum(quality_gaps) / len(quality_gaps) if quality_gaps else 0
                
                # Top recommendations ì§‘ê³„
                all_recs = {}
                for c in cases:
                    for rec in c.get('recommendations', []):
                        all_recs[rec] = all_recs.get(rec, 0) + 1
                
                print(f"  â€¢ Total cases: {total_cases}")
                print(f"  â€¢ Evidence failure: {int(evidence_failure_rate * 100)}%")
                print(f"  â€¢ Avg quality gap: {avg_quality_gap:.3f}")
                
                if all_recs:
                    top_rec = max(all_recs.items(), key=lambda x: x[1])
                    print(f"  â€¢ Top issue: {top_rec[0]} ({top_rec[1]} cases)")
                print()
            except Exception as e:
                # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë„˜ì–´ê°
                pass

        # Replan ìƒì„¸ ë¶„ì„ (replan_analysis.json ìˆìœ¼ë©´ í‘œì‹œ)
        replan_analysis_path = repo_root / "outputs" / "replan_analysis.json"
        if replan_analysis_path.exists():
            try:
                with open(replan_analysis_path, 'r', encoding='utf-8') as f:
                    rp_data = json.load(f)

                print(self._colorize("ğŸ”„ Replan ë¶„ì„ (24h)", "bold"))

                total_cases = rp_data.get('total_cases', 0)
                cases = rp_data.get('cases', [])
                stats = rp_data.get('statistics', {})

                # Evidence failure rate
                evidence_fail_rate = stats.get('evidence_failure_rate')
                if evidence_fail_rate is None and cases:
                    failures = sum(1 for c in cases if not c.get('evidence_ok', True))
                    evidence_fail_rate = failures / total_cases if total_cases else 0

                # Avg quality gap
                avg_quality_gap = stats.get('avg_quality_gap')
                if avg_quality_gap is None:
                    qgaps = [c.get('quality_gap', 0) for c in cases]
                    avg_quality_gap = (sum(qgaps) / len(qgaps)) if qgaps else 0

                # Top recommendations
                all_recs = {}
                for c in cases:
                    for rec in c.get('recommendations', []):
                        all_recs[rec] = all_recs.get(rec, 0) + 1

                print(f"  â€¢ Total cases: {total_cases}")
                print(f"  â€¢ Evidence failure: {int((evidence_fail_rate or 0) * 100)}%")
                print(f"  â€¢ Avg quality gap: {float(avg_quality_gap or 0):.3f}")

                if all_recs:
                    top_rec = max(all_recs.items(), key=lambda x: x[1])
                    print(f"  â€¢ Top issue: {top_rec[0]} ({top_rec[1]} cases)")
                print()
            except Exception:
                pass


        # ë©”íƒ€ì¸ì§€/ìœ„ì„ ì‹œê·¸ë„
        low_conf_count = int(event_counts.get('low_confidence_warning', 0))
        meta_eval_count = int(event_counts.get('meta_cognition', 0))
        warning_ratio = (low_conf_count / meta_eval_count) if meta_eval_count else None
        total_tasks = int(metrics_values.get('total_tasks', 0))
        replan_count = int(metrics_values.get('replan_count', 0))
        second_pass_count = int(metrics_values.get('second_pass_count', 0))

        print(self._colorize("\U0001f9e0 Meta-Cognition Signals", "bold"))
        print(f"  {self._status_icon(low_conf_count == 0)} Low confidence warnings: {low_conf_count}")
        if warning_ratio is not None:
            print(f"  \u2022 Warning ratio: {warning_ratio:.2f} ({low_conf_count}/{meta_eval_count} evaluations)")
        else:
            print("  \u2022 Warning ratio: N/A (no meta-cognition events)")
        if low_conf_count:
            print("  \u26a0\ufe0f  Investigate disabled tools or delegate as suggested")
        print()

        print(self._colorize("\U0001f4dd Task Flow", "bold"))
        print(f"  \u2022 Tasks processed: {total_tasks}")
        print(f"  \u2022 Replans: {replan_count}")
        print(f"  \u2022 Second passes: {second_pass_count}")
        print()

        # Canary Monitoring
        print(self._colorize("ğŸš€ Canary Deployment", "bold"))
        try:
            # ìµœê·¼ ë¡œê·¸ íŒŒì¼ í™•ì¸ìœ¼ë¡œ ë£¨í”„ ì‘ë™ ì—¬ë¶€ íŒë‹¨
            import glob
            from datetime import datetime, timedelta
            import sys
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from workspace_utils import find_workspace_root
            
            workspace = find_workspace_root(Path(__file__).parent)
            log_pattern = str(workspace / "LLM_Unified" / "ion-mentoring" / "logs" / "monitor_loop_*.log")
            log_files = glob.glob(log_pattern)
            
            canary_loop_running = False
            if log_files:
                latest_log = max(log_files, key=lambda p: Path(p).stat().st_mtime)
                log_age_mins = (datetime.now().timestamp() - Path(latest_log).stat().st_mtime) / 60
                canary_loop_running = log_age_mins < 35  # 30ë¶„ ê°„ê²© + 5ë¶„ ì—¬ìœ 
                
                print(f"  {self._status_icon(canary_loop_running)} Monitoring Loop: " +
                      ("Running (30min intervals)" if canary_loop_running else f"Idle ({log_age_mins:.0f}m ago)"))
            else:
                print(f"  {self._status_icon(False)} Monitoring Loop: No logs found")
            
            # ìµœê·¼ probe ê²°ê³¼ ì½ê¸° ì‹œë„
            probe_pattern = str(workspace / \"LLM_Unified\" / \"ion-mentoring\" / \"logs\" / \"probe_iter_*.json\")
            probe_files = glob.glob(probe_pattern)
            if probe_files:
                latest_probe = max(probe_files, key=lambda p: Path(p).stat().st_mtime)
                probe_data = json.loads(Path(latest_probe).read_text(encoding='utf-8'))
                probe_time = datetime.fromisoformat(probe_data.get('timestamp', ''))
                # Make timezone-naive for comparison
                probe_time = probe_time.replace(tzinfo=None)
                age_mins = (datetime.now() - probe_time).total_seconds() / 60
                
                if age_mins < 60:  # ìµœê·¼ 1ì‹œê°„ ì´ë‚´ ë°ì´í„°ë§Œ í‘œì‹œ
                    legacy = probe_data.get('legacy', {})
                    canary = probe_data.get('canary', {})
                    legacy_rate = legacy.get('success_rate', 0)
                    canary_rate = canary.get('success_rate', 0)
                    legacy_ms = legacy.get('avg_response_ms', 0)
                    canary_ms = canary.get('avg_response_ms', 0)
                    print(f"  â€¢ Latest Probe ({age_mins:.0f}m ago):")
                    print(f"    Legacy: {legacy_rate:.0f}% success, {legacy_ms:.0f}ms avg")
                    print(f"    Canary: {canary_rate:.0f}% success, {canary_ms:.0f}ms avg")
                else:
                    print(f"  â„¹ï¸  Probe data stale ({age_mins:.0f}m old)")
            else:
                print("  â„¹ï¸  No probe data yet (first run pending)")
        except Exception as e:
            print(f"  âš ï¸  Canary status check failed: {str(e)[:50]}")
        print()

        # Lumen Gateway
        print(self._colorize("ğŸŒ Lumen Gateway", "bold"))
        lumen = health['external_services']['lumen']
        print(f"  {self._status_icon(lumen['ok'])} Gateway: " +
              (lumen.get('response_preview', lumen.get('error', 'unknown'))[:80]))
        print()

        # Local Proxy (with resolved port)
        print(self._colorize("ğŸ”Œ Local Proxy", "bold"))
        proxy = health['external_services']['proxy']
        resolved_port = policy.get('resolved_proxy_port', proxy.get('port', 8090))
        print(f"  {self._status_icon(proxy['ok'])} Port {resolved_port}: {proxy.get('status', proxy.get('error', 'unknown'))}")
        if resolved_port != proxy.get('port', 8090):
            print(f"    â„¹ï¸  Resolved from proxy_info.json (configured: {proxy.get('port', 8090)})")
        print()

        # System Resources
        print(self._colorize("ğŸ’» System Resources", "bold"))
        system = health['external_services']['system']
        if 'note' in system:
            print(f"  â„¹ï¸  {system['note']}")
        elif 'error' in system:
            print(f"  {self._status_icon(False)} Error: {system['error']}")
        else:
            print(f"  â€¢ CPU:    {system['cpu_percent']}% " +
                  ("âš ï¸" if system['warnings']['cpu'] else "âœ…"))
            print(f"  â€¢ Memory: {system['memory_percent']}% " +
                  ("âš ï¸" if system['warnings']['memory'] else "âœ…"))
            print(f"  â€¢ Disk:   {system['disk_percent']}% " +
                  ("âš ï¸" if system['warnings']['disk'] else "âœ…"))
        print()

        # Timeline ë¯¸ë¦¬ë³´ê¸°
        print(self._colorize("ğŸ“ˆ Timeline (ìµœê·¼ 6ì‹œê°„)", "bold"))
        timeline = self.collector.get_timeline_data(hours=6.0, interval_minutes=30)
        for entry in timeline[-3:]:  # ë§ˆì§€ë§‰ 3ê°œ êµ¬ê°„
            ts = entry['timestamp'].split('T')[1][:5]  # HH:MMë§Œ ì¶”ì¶œ
            print(f"  {ts}: {entry['event_count']:3d} events, " +
                  f"Q={entry['avg_quality'] or 'N/A'}, C={entry['avg_confidence'] or 'N/A'}")

        print(self._colorize("\n" + "="*60 + "\n", "cyan"))

    def generate_json_report(self, hours: float = 1.0) -> Dict[str, Any]:
        """JSON ë¦¬í¬íŠ¸ ìƒì„±"""
        health = self.collector.get_health_status()
        realtime = self.collector.get_realtime_metrics(hours=hours)
        timeline = self.collector.get_timeline_data(hours=6.0, interval_minutes=30)

        return {
            'timestamp': realtime['timestamp'],
            'healthy': health['healthy'],
            'health_checks': health['checks'],
            'policy': health.get('policy', {}),  # Policy ì •ë³´ ì¶”ê°€
            'metrics': realtime['metrics'],
            'external_services': health['external_services'],
            'timeline': timeline[-6:],  # ë§ˆì§€ë§‰ 6ê°œ êµ¬ê°„
        }


def main():
    """CLI ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸"""
    import argparse
    import sys
    import io

    # UTF-8 ì¶œë ¥ ê°•ì œ ì„¤ì • (Windows cp949 ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
    if sys.platform == 'win32':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

    parser = argparse.ArgumentParser(description='AGI í†µí•© ìš´ì˜ ëŒ€ì‹œë³´ë“œ')
    parser.add_argument('--json', action='store_true', help='JSON í¬ë§·ìœ¼ë¡œ ì¶œë ¥')
    parser.add_argument('--hours', type=float, default=1.0, help='ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸: 1ì‹œê°„)')
    args = parser.parse_args()

    dashboard = OpsDashboard()

    if args.json:
        report = dashboard.generate_json_report(hours=args.hours)
        print(json.dumps(report, indent=2, ensure_ascii=False))
    else:
        dashboard.print_dashboard(hours=args.hours)


if __name__ == '__main__':
    main()
