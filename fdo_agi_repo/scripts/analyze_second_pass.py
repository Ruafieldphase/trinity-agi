#!/usr/bin/env python3
"""
AGI Second Pass ì›ì¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- quality < min_quality ì¼€ì´ìŠ¤ ë¶„ì„
- confidence vs quality ìƒê´€ê´€ê³„
- second_pass ë°œìƒ íŒ¨í„´ ë„ì¶œ
"""

import json
from pathlib import Path
from collections import defaultdict
from datetime import datetime, timedelta
import statistics


def load_ledger(ledger_path: Path):
    """Load resonance ledger JSONL"""
    events = []
    with open(ledger_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                events.append(json.loads(line))
    return events


def analyze_second_pass_causes(events, hours=24):
    """Analyze causes of second_pass events"""
    
    # Filter by time window
    now = datetime.now().timestamp()
    cutoff = now - (hours * 3600)
    recent_events = [e for e in events if e.get('ts', 0) >= cutoff]
    
    # Exclude test tasks
    exclude_prefixes = ['integration_test_', 'low_confidence_test_', 'temp_low_conf_']
    def is_real_task(task_id):
        return not any(task_id.startswith(prefix) for prefix in exclude_prefixes)
    
    # Group by task_id
    tasks = defaultdict(list)
    for event in recent_events:
        task_id = event.get('task_id')
        if task_id and is_real_task(task_id):
            tasks[task_id].append(event)
    
    # Analyze second_pass tasks
    second_pass_analysis = []
    
    for task_id, task_events in tasks.items():
        # Find second_pass event
        has_second_pass = any(e.get('event') == 'second_pass' for e in task_events)
        if not has_second_pass:
            continue
        
        # Extract relevant metrics
        run_config = next((e for e in task_events if e.get('event') == 'run_config'), {})
        meta_cognition = next((e for e in task_events if e.get('event') == 'meta_cognition'), {})
        
        # Find eval results (pass 1)
        eval_events = [e for e in task_events if e.get('event') == 'eval' and e.get('quality') is not None]
        
        if eval_events:
            first_eval = eval_events[0]
            quality = first_eval.get('quality', 0)
            evidence_ok = first_eval.get('evidence_ok', False)
            
            # Find rune (recommendations)
            rune_event = next((e for e in task_events if e.get('event') == 'rune'), {})
            rune_data = rune_event.get('rune', {})
            
            second_pass_analysis.append({
                'task_id': task_id,
                'min_quality': run_config.get('evaluation', {}).get('min_quality', 0.6),
                'first_quality': quality,
                'quality_gap': run_config.get('evaluation', {}).get('min_quality', 0.6) - quality,
                'evidence_ok': evidence_ok,
                'confidence': meta_cognition.get('confidence', 0),
                'past_performance': meta_cognition.get('past_performance', 0),
                'impact': rune_data.get('impact', 0),
                'transparency': rune_data.get('transparency', 0),
                'rune_confidence': rune_data.get('confidence', 0),
                'recommendations': rune_data.get('recommendations', []),
                'replan': rune_data.get('replan', False)
            })
    
    return second_pass_analysis


def print_analysis(analysis):
    """Print analysis results"""
    
    if not analysis:
        print("âš ï¸  No second_pass events found in the time window")
        return
    
    print(f"\nğŸ” Second Pass ì›ì¸ ë¶„ì„ ({len(analysis)} cases)")
    print("=" * 80)
    
    # 1. Quality Gap ë¶„ì„
    quality_gaps = [case['quality_gap'] for case in analysis]
    print(f"\nğŸ“Š Quality Gap (min_quality - actual_quality)")
    print(f"   í‰ê· : {statistics.mean(quality_gaps):.3f}")
    print(f"   ì¤‘ì•™ê°’: {statistics.median(quality_gaps):.3f}")
    print(f"   ë²”ìœ„: {min(quality_gaps):.3f} ~ {max(quality_gaps):.3f}")
    
    # 2. Confidence vs Quality ìƒê´€ê´€ê³„
    confidences = [case['confidence'] for case in analysis]
    qualities = [case['first_quality'] for case in analysis]
    print(f"\nğŸ¯ Confidence vs Quality")
    print(f"   í‰ê·  Confidence: {statistics.mean(confidences):.3f}")
    print(f"   í‰ê·  Quality: {statistics.mean(qualities):.3f}")
    print(f"   Confidence ë†’ì€ë° Quality ë‚®ìŒ: ì˜ˆì¸¡ ë¶ˆì¼ì¹˜")
    
    # 3. Evidence ë¬¸ì œ
    evidence_failures = sum(1 for case in analysis if not case['evidence_ok'])
    print(f"\nğŸ“ Evidence ë¬¸ì œ")
    print(f"   evidence_ok=False: {evidence_failures}/{len(analysis)} ({evidence_failures/len(analysis)*100:.1f}%)")
    
    # 4. Recommendations íŒ¨í„´
    all_recommendations = []
    for case in analysis:
        all_recommendations.extend(case['recommendations'])
    
    recommendation_counts = defaultdict(int)
    for rec in all_recommendations:
        recommendation_counts[rec] += 1
    
    print(f"\nğŸ’¡ ìì£¼ ë“±ì¥í•˜ëŠ” Recommendations (ê°œì„  í¬ì¸íŠ¸)")
    for rec, count in sorted(recommendation_counts.items(), key=lambda x: -x[1]):
        print(f"   '{rec}': {count}íšŒ ({count/len(analysis)*100:.1f}%)")
    
    # 5. ê°œë³„ ì¼€ì´ìŠ¤ ìƒì„¸
    print(f"\nğŸ“‹ ê°œë³„ ì¼€ì´ìŠ¤ ìƒì„¸ (ìµœê·¼ 3ê±´)")
    for i, case in enumerate(analysis[-3:], 1):
        print(f"\n   Case {i}: {case['task_id'][:8]}...")
        print(f"      Quality: {case['first_quality']:.2f} (ëª©í‘œ: {case['min_quality']:.2f}, ë¶€ì¡±: {case['quality_gap']:.2f})")
        print(f"      Confidence: {case['confidence']:.2f}")
        print(f"      Evidence OK: {case['evidence_ok']}")
        print(f"      Recommendations: {', '.join(case['recommendations'][:2])}")
    
    # 6. ê°œì„  ë°©ì•ˆ ì œì‹œ
    print(f"\n\nğŸ¯ ê°œì„  ë°©ì•ˆ (ìš°ì„ ìˆœìœ„)")
    print("=" * 80)
    
    avg_quality_gap = statistics.mean(quality_gaps)
    if avg_quality_gap > 0.15:
        print(f"1ï¸âƒ£ CRITICAL: Quality Gap ë„ˆë¬´ í¼ ({avg_quality_gap:.3f})")
        print(f"   â†’ min_quality 0.6 â†’ 0.5ë¡œ ì™„í™” (ë˜ëŠ” quality í‰ê°€ ê¸°ì¤€ ê°œì„ )")
    
    if evidence_failures / len(analysis) > 0.5:
        print(f"2ï¸âƒ£ HIGH: Evidence ê²€ì¦ ì‹¤íŒ¨ìœ¨ ë†’ìŒ ({evidence_failures/len(analysis)*100:.1f}%)")
        print(f"   â†’ Promptì— 'êµ¬ì²´ì  ê·¼ê±° í¬í•¨' ëª…ì‹œ ê°•í™”")
    
    most_common_rec = max(recommendation_counts.items(), key=lambda x: x[1])[0] if recommendation_counts else None
    if most_common_rec:
        print(f"3ï¸âƒ£ MEDIUM: ë°˜ë³µë˜ëŠ” ì¶”ì²œ '{most_common_rec}'")
        print(f"   â†’ Few-shot learningì— í•´ë‹¹ íŒ¨í„´ ì¶”ê°€")
    
    avg_confidence = statistics.mean(confidences)
    avg_quality = statistics.mean(qualities)
    if avg_confidence - avg_quality > 0.15:
        print(f"4ï¸âƒ£ LOW: Confidence vs Quality ë¶ˆì¼ì¹˜ (conf={avg_confidence:.2f}, qual={avg_quality:.2f})")
        print(f"   â†’ Meta-cognition calibration í•„ìš”")


def main():
    ledger_path = Path(__file__).parent.parent / "memory" / "resonance_ledger.jsonl"
    
    if not ledger_path.exists():
        print(f"âŒ Ledger not found: {ledger_path}")
        return
    
    print(f"ğŸ“‚ Loading: {ledger_path}")
    events = load_ledger(ledger_path)
    print(f"   Total events: {len(events)}")
    
    # Analyze last 24 hours
    analysis = analyze_second_pass_causes(events, hours=24)
    print_analysis(analysis)
    
    # Export to JSON for dashboard
    output_path = Path(__file__).parent.parent / "outputs" / "second_pass_analysis.json"
    output_path.parent.mkdir(exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'window_hours': 24,
            'total_cases': len(analysis),
            'cases': analysis,
            'summary': {
                'avg_quality_gap': statistics.mean([c['quality_gap'] for c in analysis]) if analysis else 0,
                'avg_confidence': statistics.mean([c['confidence'] for c in analysis]) if analysis else 0,
                'avg_quality': statistics.mean([c['first_quality'] for c in analysis]) if analysis else 0,
                'evidence_failure_rate': sum(1 for c in analysis if not c['evidence_ok']) / len(analysis) if analysis else 0,
                'recommendations': dict(defaultdict(int, {
                    rec: sum(1 for c in analysis for r in c['recommendations'] if r == rec)
                    for rec in set(r for c in analysis for r in c['recommendations'])
                }))
            }
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… Analysis exported: {output_path}")


if __name__ == "__main__":
    main()
