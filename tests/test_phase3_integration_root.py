"""
Phase 3 Day 1 í†µí•© í…ŒìŠ¤íŠ¸
- ìë™ ì¬ì‹œë„/ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- ê²€ì¦ ê³ ë„í™” (í™”ë©´ ìº¡ì²˜, OCR)
- ì„±ëŠ¥ ìµœì í™” (ë³‘ë ¬ ì²˜ë¦¬, ìºì‹±)
"""
import pytest
pytestmark = pytest.mark.skip(reason="duplicate of fdo_agi_repo/tests/test_phase3_integration.py (renamed)")
import sys
import time
from pathlib import Path
from typing import Dict, Any

# Add fdo_agi_repo to path
sys.path.insert(0, str(Path(__file__).parent.parent / "fdo_agi_repo"))

from rpa.action_mapper import ActionMapper
from rpa.execution_engine import ExecutionEngine, ExecutionConfig, ExecutionMode
from rpa.actions import Action, ActionResult


def test_action_mapper_caching():
    """ActionMapper ìºì‹± í…ŒìŠ¤íŠ¸ (lru_cache ê¸°ë°˜)"""
    print("\nğŸ§ª Test: ActionMapper Caching")
    print("="*70)
    
    mapper = ActionMapper()
    
    # ë™ì¼í•œ í…ìŠ¤íŠ¸ë¡œ _extract_action_from_text í˜¸ì¶œ (lru_cache í…ŒìŠ¤íŠ¸)
    text = "open notepad"
    
    # ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤)
    start1 = time.time()
    action_type1, target1 = mapper._extract_action_from_text(text)
    duration1 = time.time() - start1
    
    print(f"  First call (cache miss): {duration1*1000:.2f}ms")
    assert action_type1 is not None
    
    # ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸)
    start2 = time.time()
    action_type2, target2 = mapper._extract_action_from_text(text)
    duration2 = time.time() - start2
    
    print(f"  Second call (cache hit): {duration2*1000:.2f}ms")
    assert action_type2 == action_type1
    assert target2 == target1
    
    # ìºì‹±ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ê°œì„  í™•ì¸
    # lru_cacheëŠ” ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ ë‹¨ìˆœíˆ ë™ì‘ í™•ì¸ë§Œ
    print(f"  âœ… LRU cache is working (functools.lru_cache)")
    print()


def test_action_mapper_cache_invalidation():
    """ActionMapper ìºì‹œ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸ (lru_cache)"""
    print("\nğŸ§ª Test: ActionMapper Cache Invalidation")
    print("="*70)
    
    mapper = ActionMapper()
    
    # ìºì‹œì— í•­ëª© ì¶”ê°€
    text = "type hello world"
    action_type1, target1 = mapper._extract_action_from_text(text)
    assert action_type1 is not None
    
    # lru_cache ì •ë³´ í™•ì¸
    cache_info = mapper._extract_action_from_text.cache_info()
    print(f"  Cache info: hits={cache_info.hits}, misses={cache_info.misses}, size={cache_info.currsize}")
    
    # ìºì‹œ í´ë¦¬ì–´
    mapper._extract_action_from_text.cache_clear()
    
    cache_info_after = mapper._extract_action_from_text.cache_info()
    print(f"  After clear: hits={cache_info_after.hits}, misses={cache_info_after.misses}, size={cache_info_after.currsize}")
    
    assert cache_info_after.currsize == 0, "Cache should be empty after clear"
    
    # ìºì‹œ í´ë¦¬ì–´ í›„ ë‹¤ì‹œ í˜¸ì¶œ
    action_type2, target2 = mapper._extract_action_from_text(text)
    assert action_type2 == action_type1
    
    print("  âœ… Cache invalidation works correctly")
    print()


def test_execution_with_retry():
    """ìë™ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª Test: Execution with Retry")
    print("="*70)
    
    # ì‹¤íŒ¨ ê°€ëŠ¥ì„±ì´ ìˆëŠ” íŠœí† ë¦¬ì–¼ (DRY_RUN ëª¨ë“œ)
    tutorial = """
How to test retry:
1. Open nonexistent_app_12345
2. Type 'Hello'
3. Press Enter
    """.strip()
    
    config = ExecutionConfig(
        mode=ExecutionMode.DRY_RUN,
        enable_verification=False,
        enable_failsafe=True,
        timeout=10.0,
    )
    
    engine = ExecutionEngine(config)
    result = engine.execute_tutorial(tutorial)
    
    print(f"  Total Actions: {result.total_actions}")
    print(f"  Executed: {result.executed_actions}")
    print(f"  Failed: {result.failed_actions}")
    
    # DRY_RUNì—ì„œëŠ” ì‹¤íŒ¨í•˜ì§€ ì•Šì•„ì•¼ í•¨
    assert result.executed_actions >= 2, "Should execute at least 2 actions"
    
    print("  âœ… Retry mechanism is ready (DRY_RUN passes)")
    print()


def test_execution_error_capture():
    """ì˜¤ë¥˜ ë°œìƒ ì‹œ ìº¡ì²˜ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª Test: Error Capture")
    print("="*70)
    
    tutorial = """
How to test error capture:
1. Invalid action that will fail
2. Type 'Test'
    """.strip()
    
    config = ExecutionConfig(
        mode=ExecutionMode.DRY_RUN,
        enable_verification=False,
        enable_failsafe=True,
        timeout=5.0,
    )
    
    engine = ExecutionEngine(config)
    result = engine.execute_tutorial(tutorial)
    
    print(f"  Total Actions: {result.total_actions}")
    print(f"  Executed: {result.executed_actions}")
    print(f"  Failed: {result.failed_actions}")
    print(f"  Errors: {len(result.errors)}")
    
    # ì—ëŸ¬ê°€ ìº¡ì²˜ë˜ì–´ì•¼ í•¨
    if result.failed_actions > 0:
        assert len(result.errors) > 0, "Errors should be captured"
        print(f"  âœ… Error capture works ({len(result.errors)} errors captured)")
    else:
        print(f"  â„¹ï¸  No errors in DRY_RUN mode")
    
    print()


def test_parallel_execution_readiness():
    """ë³‘ë ¬ ì²˜ë¦¬ ì¤€ë¹„ ìƒíƒœ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª Test: Parallel Execution Readiness")
    print("="*70)
    
    # ì—¬ëŸ¬ íŠœí† ë¦¬ì–¼ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€ (ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜)
    tutorials = [
        "1. Type 'Hello'",
        "1. Press Enter",
        "1. Open notepad",
    ]
    
    config = ExecutionConfig(
        mode=ExecutionMode.DRY_RUN,
        enable_verification=False,
        enable_failsafe=True,
        timeout=5.0,
    )
    
    results = []
    start = time.time()
    
    for tutorial in tutorials:
        engine = ExecutionEngine(config)
        result = engine.execute_tutorial(tutorial)
        results.append(result)
    
    duration = time.time() - start
    
    print(f"  Tutorials executed: {len(results)}")
    print(f"  Total duration: {duration:.2f}s")
    print(f"  Avg duration per tutorial: {duration/len(tutorials):.2f}s")
    
    # ëª¨ë“  íŠœí† ë¦¬ì–¼ì´ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
    assert all(r.total_actions > 0 for r in results), "All tutorials should execute"
    
    print("  âœ… Parallel execution is ready (sequential test passed)")
    print()


def test_cache_statistics():
    """ìºì‹œ í†µê³„ í…ŒìŠ¤íŠ¸ (lru_cache)"""
    print("\nğŸ§ª Test: Cache Statistics")
    print("="*70)
    
    mapper = ActionMapper()
    
    # ìºì‹œ ì´ˆê¸°í™”
    mapper._extract_action_from_text.cache_clear()
    mapper._parse_key_combination.cache_clear()
    
    # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    texts = [
        "open notepad",
        "type hello",
        "press enter",
        "open notepad",  # ì¤‘ë³µ (ìºì‹œ íˆíŠ¸)
        "type hello",    # ì¤‘ë³µ (ìºì‹œ íˆíŠ¸)
    ]
    
    for text in texts:
        mapper._extract_action_from_text(text)
    
    # ìºì‹œ í†µê³„ í™•ì¸
    cache_info = mapper._extract_action_from_text.cache_info()
    print(f"  Total calls: {cache_info.hits + cache_info.misses}")
    print(f"  Cache hits: {cache_info.hits}")
    print(f"  Cache misses: {cache_info.misses}")
    print(f"  Cache size: {cache_info.currsize}")
    
    # ì¤‘ë³µ í˜¸ì¶œ í™•ì¸ (2ê°œì˜ ìºì‹œ íˆíŠ¸ ì˜ˆìƒ)
    assert cache_info.hits >= 2, f"Expected at least 2 cache hits, got {cache_info.hits}"
    assert cache_info.currsize == 3, f"Expected 3 unique items, got {cache_info.currsize}"
    
    print("  âœ… Cache statistics are correct")
    print()


def test_performance_baseline():
    """ì„±ëŠ¥ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •"""
    print("\nğŸ§ª Test: Performance Baseline")
    print("="*70)
    
    tutorial = """
How to measure performance:
1. Open notepad
2. Type 'Performance Test'
3. Press Ctrl+S
4. Type 'test.txt'
5. Press Enter
    """.strip()
    
    config = ExecutionConfig(
        mode=ExecutionMode.DRY_RUN,
        enable_verification=False,
        enable_failsafe=True,
        timeout=10.0,
    )
    
    # ì„±ëŠ¥ ì¸¡ì • (3íšŒ í‰ê· )
    durations = []
    for i in range(3):
        engine = ExecutionEngine(config)
        start = time.time()
        result = engine.execute_tutorial(tutorial)
        duration = time.time() - start
        durations.append(duration)
        print(f"  Run {i+1}: {duration:.3f}s")
    
    avg_duration = sum(durations) / len(durations)
    print(f"\n  Average duration: {avg_duration:.3f}s")
    print(f"  Actions per second: {result.total_actions/avg_duration:.1f}")
    
    # ì„±ëŠ¥ ì²´í¬ (DRY_RUNì—ì„œëŠ” ë§¤ìš° ë¹ ë¦„)
    assert avg_duration < 5.0, f"Performance too slow: {avg_duration:.3f}s"
    
    print("  âœ… Performance baseline established")
    print()


def main():
    """Phase 3 Day 1 í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "ğŸš€"*35)
    print("Phase 3 Day 1 Integration Tests")
    print("Auto-retry / Error Recovery / Caching / Parallel")
    print("ğŸš€"*35 + "\n")
    
    test_results = []
    
    tests = [
        ("ActionMapper Caching", test_action_mapper_caching),
        ("Cache Invalidation", test_action_mapper_cache_invalidation),
        ("Execution with Retry", test_execution_with_retry),
        ("Error Capture", test_execution_error_capture),
        ("Parallel Execution Readiness", test_parallel_execution_readiness),
        ("Cache Statistics", test_cache_statistics),
        ("Performance Baseline", test_performance_baseline),
    ]
    
    for name, test_func in tests:
        try:
            test_func()
            test_results.append((name, "âœ… PASS"))
        except Exception as e:
            test_results.append((name, f"âŒ FAIL: {e}"))
            print(f"  âŒ Test failed: {e}\n")
    
    # ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ“Š TEST SUMMARY")
    print("="*70)
    
    for name, status in test_results:
        print(f"  {status.split(':')[0]:3s} {name}")
    
    passed = sum(1 for _, s in test_results if "âœ…" in s)
    total = len(test_results)
    
    print()
    print(f"  Total: {total}")
    print(f"  Passed: {passed}")
    print(f"  Failed: {total - passed}")
    print(f"  Pass Rate: {passed/total*100:.0f}%")
    print("="*70 + "\n")
    
    if passed == total:
        print("ğŸ‰ ALL PHASE 3 TESTS PASSED! ğŸ‰\n")
        return 0
    else:
        print(f"âš ï¸  {total - passed} TEST(S) FAILED\n")
        return 1


if __name__ == "__main__":
    sys.exit(main())
