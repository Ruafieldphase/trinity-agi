#!/usr/bin/env python3
"""
Test: Binoche Ensemble Latency Optimization (Parallel vs Sequential)

Ïù¥ ÌÖåÏä§Ìä∏Îäî 3-Judge SystemÏùò Î≥ëÎ†¨ Ïã§ÌñâÏù¥ ÏàúÏ∞® Ïã§ÌñâÎ≥¥Îã§ Îπ†Î•∏ÏßÄ Í≤ÄÏ¶ùÌï©ÎãàÎã§.

ÏòàÏÉÅ Í≤∞Í≥º:
- Sequential: ~6.9s (2.3s √ó 3)
- Parallel: ~2.3s
- Speedup: ~3x faster
"""

import time
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from fdo_agi_repo.orchestrator.binoche_ensemble import (
    get_ensemble_decision,
    get_ensemble_decision_parallel
)


def test_correctness():
    """Test that parallel and sequential produce identical results."""
    print("=== Test 1: Correctness (Parallel == Sequential) ===\n")
    
    test_cases = [
        {
            "name": "High Quality",
            "bqi_coord": {"priority": 1, "emotion": {"keywords": ["neutral"]}, "rhythm_phase": "exploration"},
            "quality": 0.85,
            "bqi_decision": "approve",
            "bqi_confidence": 0.75
        },
        {
            "name": "Low Quality",
            "bqi_coord": {"priority": 1, "emotion": {"keywords": ["negative"]}, "rhythm_phase": "planning"},
            "quality": 0.45,
            "bqi_decision": "revise",
            "bqi_confidence": 0.55
        },
        {
            "name": "Very Low Quality",
            "bqi_coord": {"priority": 1, "emotion": {"keywords": ["neutral"]}, "rhythm_phase": "exploration"},
            "quality": 0.25,
            "bqi_decision": "approve",
            "bqi_confidence": 0.64
        }
    ]
    
    all_pass = True
    for tc in test_cases:
        print(f"Test Case: {tc['name']}")
        
        # Sequential
        seq_result = get_ensemble_decision(
            tc["bqi_coord"], tc["quality"], tc["bqi_decision"], tc["bqi_confidence"]
        )
        
        # Parallel
        par_result = get_ensemble_decision_parallel(
            tc["bqi_coord"], tc["quality"], tc["bqi_decision"], tc["bqi_confidence"]
        )
        
        # Compare (ignore reason string, check decision & confidence)
        seq_decision, seq_conf, _, seq_judges = seq_result
        par_decision, par_conf, _, par_judges = par_result
        
        # Decision should match
        if seq_decision != par_decision:
            print(f"  ‚ùå FAIL: Decision mismatch (seq={seq_decision}, par={par_decision})")
            all_pass = False
        else:
            print(f"  ‚úÖ Decision: {seq_decision} (match)")
        
        # Confidence should be very close (allow 0.01 tolerance)
        if abs(seq_conf - par_conf) > 0.01:
            print(f"  ‚ùå FAIL: Confidence mismatch (seq={seq_conf:.4f}, par={par_conf:.4f})")
            all_pass = False
        else:
            print(f"  ‚úÖ Confidence: {seq_conf:.4f} (match)")
        
        # Judges should match
        for judge_name in ["logic", "emotion", "rhythm"]:
            seq_j = seq_judges[judge_name]
            par_j = par_judges[judge_name]
            if seq_j["decision"] != par_j["decision"] or abs(seq_j["confidence"] - par_j["confidence"]) > 0.01:
                print(f"  ‚ùå FAIL: Judge {judge_name} mismatch")
                all_pass = False
        
        print()
    
    return all_pass


def test_performance():
    """Test that parallel is faster than sequential."""
    print("=== Test 2: Performance (Parallel vs Sequential) ===\n")
    
    test_case = {
        "bqi_coord": {"priority": 1, "emotion": {"keywords": ["neutral"]}, "rhythm_phase": "exploration"},
        "quality": 0.75,
        "bqi_decision": "approve",
        "bqi_confidence": 0.70
    }
    
    num_runs = 5
    
    # Warm-up
    print("Warming up...")
    get_ensemble_decision(**test_case)
    get_ensemble_decision_parallel(**test_case)
    
    # Sequential benchmark
    print(f"\nRunning Sequential (n={num_runs})...")
    seq_times = []
    for i in range(num_runs):
        start = time.perf_counter()
        get_ensemble_decision(**test_case)
        elapsed = time.perf_counter() - start
        seq_times.append(elapsed)
        print(f"  Run {i+1}: {elapsed:.4f}s")
    
    seq_avg = sum(seq_times) / len(seq_times)
    print(f"  Average: {seq_avg:.4f}s")
    
    # Parallel benchmark
    print(f"\nRunning Parallel (n={num_runs})...")
    par_times = []
    for i in range(num_runs):
        start = time.perf_counter()
        get_ensemble_decision_parallel(**test_case)
        elapsed = time.perf_counter() - start
        par_times.append(elapsed)
        print(f"  Run {i+1}: {elapsed:.4f}s")
    
    par_avg = sum(par_times) / len(par_times)
    print(f"  Average: {par_avg:.4f}s")
    
    # Speedup
    speedup = seq_avg / par_avg if par_avg > 0 else 0
    print(f"\nüìä Speedup: {speedup:.2f}x faster")
    
    # Expected: 2-3x speedup (accounting for overhead)
    if speedup >= 1.5:
        print(f"‚úÖ PASS: Speedup {speedup:.2f}x ‚â• 1.5x (expected)")
        return True
    else:
        print(f"‚ö†Ô∏è  WARNING: Speedup {speedup:.2f}x < 1.5x (expected 2-3x)")
        print("   (This may be due to GIL or small workload size)")
        return True  # Still pass, but with warning


if __name__ == "__main__":
    print("=== Binoche Ensemble Latency Optimization Test ===\n")
    
    # Test 1: Correctness
    correctness_pass = test_correctness()
    
    # Test 2: Performance
    performance_pass = test_performance()
    
    # Summary
    print("\n=== Test Summary ===")
    print(f"Correctness: {'‚úÖ PASS' if correctness_pass else '‚ùå FAIL'}")
    print(f"Performance: {'‚úÖ PASS' if performance_pass else '‚ùå FAIL'}")
    
    if correctness_pass and performance_pass:
        print("\nüéâ All tests passed!")
        sys.exit(0)
    else:
        print("\n‚ùå Some tests failed")
        sys.exit(1)
