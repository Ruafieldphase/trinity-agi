import logging
import os
import time
from typing import Any, Dict, List, Optional, Tuple

import vertexai
from vertexai.preview.generative_models import GenerativeModel


RATE_LIMIT_MARKERS = ("429", "rate limit", "quota", "exhausted", "exceeded")


class ModelSelector:
    """
    Centralized Gemini model selector with graceful fallback.

    - Chooses model based on intent/length/urgency/vision needs.
    - Retries with lower-cost models on 429/quota errors.
    - Caches GenerativeModel instances per model name.
    """

    def __init__(
        self,
        project: Optional[str] = None,
        location: Optional[str] = None,
        logger: Optional[logging.Logger] = None,
    ):
        self.logger = logger or logging.getLogger("ModelSelector")
        self.project = project or os.getenv("GOOGLE_CLOUD_PROJECT")
        self.location = location or os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")

        # Model preferences (override with env to try Gemini 3.0 / experimental models)
        # Default to current generally available model IDs (-002). Override via env if needed.
        self.fast_model = os.getenv("GEMINI_FAST_MODEL", "gemini-1.5-flash-002")
        self.balanced_model = os.getenv("GEMINI_BALANCED_MODEL", "gemini-1.5-pro-002")
        self.vision_model = os.getenv("GEMINI_VISION_MODEL", self.balanced_model)
        self.top_model = (
            os.getenv("GEMINI_TOP_TIER_MODEL")
            or os.getenv("GEMINI_30_MODEL")
            or os.getenv("GEMINI_EXPERIMENTAL_MODEL")
        )

        self.available = False
        self._cache: Dict[str, GenerativeModel] = {}
        self._init_vertex()

    def _init_vertex(self) -> None:
        """Init Vertex; keeps module safe if env is missing."""
        if not self.project:
            self.logger.warning("VertexAI project not configured; model selection disabled.")
            return
        try:
            vertexai.init(project=self.project, location=self.location)
            self.available = True
            self.logger.info(f"Vertex initialized (project={self.project}, location={self.location})")
        except Exception as e:
            self.logger.error(f"Vertex init failed: {e}")
            self.available = False

    def _dedup(self, models: List[str]) -> List[str]:
        seen = set()
        ordered = []
        for m in models:
            if not m:
                continue
            if m in seen:
                continue
            seen.add(m)
            ordered.append(m)
        return ordered

    def select_candidates(
        self,
        *,
        intent: str = "",
        text_length: int = 0,
        urgency: bool = False,
        high_precision: bool = False,
        vision: bool = False,
    ) -> List[str]:
        """Return ordered model candidates for this request."""
        candidates: List[str] = []

        # Vision prefers higher fidelity, then balanced.
        if vision:
            if high_precision and self.top_model:
                candidates.append(self.top_model)
            candidates.append(self.vision_model)

        # Complex or long tasks prefer top/balanced.
        if high_precision or text_length > 800 or intent in ["CREATE", "MODIFY", "VERIFY"]:
            if self.top_model:
                candidates.append(self.top_model)
            candidates.append(self.balanced_model)

        # Default path.
        if not candidates:
            if urgency or text_length > 300:
                candidates.append(self.balanced_model)
            else:
                candidates.append(self.fast_model)

        # Fallbacks always available.
        for fallback in (self.balanced_model, self.fast_model):
            if fallback not in candidates:
                candidates.append(fallback)

        ordered = self._dedup(candidates)

        # Add alternates for legacy model IDs to reduce 404 risk.
        alt_flash = [
            "gemini-1.5-flash-002",
            "gemini-1.5-flash-001",
            "gemini-1.5-flash",
        ]
        alt_pro = [
            "gemini-1.5-pro-002",
            "gemini-1.5-pro-001",
            "gemini-1.5-pro",
        ]
        for alt in alt_flash:
            if self.fast_model.startswith("gemini-1.5-flash") and alt not in ordered:
                ordered.append(alt)
        for alt in alt_pro:
            if self.balanced_model.startswith("gemini-1.5-pro") and alt not in ordered:
                ordered.append(alt)

        return ordered

    def _get_model(self, model_name: str) -> GenerativeModel:
        if model_name not in self._cache:
            self._cache[model_name] = GenerativeModel(model_name)
        return self._cache[model_name]

    def generate_content(
        self,
        content: Any,
        *,
        intent: str = "",
        text_length: int = 0,
        urgency: bool = False,
        high_precision: bool = False,
        vision: bool = False,
        generation_config: Optional[dict] = None,
        **kwargs,
    ) -> Tuple[Any, str]:
        """
        Try generation with prioritized models; falls back on quota/rate errors.
        Returns (response, model_used) or raises on total failure.
        """
        if not self.available:
            raise RuntimeError("Vertex AI not configured")

        candidates = self.select_candidates(
            intent=intent,
            text_length=text_length,
            urgency=urgency,
            high_precision=high_precision,
            vision=vision,
        )

        errors = []
        for model_name in candidates:
            try:
                model = self._get_model(model_name)
                response = model.generate_content(
                    content,
                    generation_config=generation_config or {"temperature": 0.35},
                    **kwargs,
                )
                return response, model_name
            except Exception as e:
                msg = str(e)
                errors.append(f"{model_name}: {msg}")
                lower = msg.lower()
                if any(marker in lower for marker in RATE_LIMIT_MARKERS):
                    time.sleep(1.2)
                continue

        raise RuntimeError(f"All Gemini candidates failed. Last errors: {' | '.join(errors[-3:])}")

    def try_generate_content(self, *args, **kwargs) -> Tuple[Optional[Any], Optional[str]]:
        """Safe wrapper that returns (None, None) on failure."""
        try:
            return self.generate_content(*args, **kwargs)
        except Exception as e:
            self.logger.warning(f"Model selection failed: {e}")
            return None, None
