"""
Rua Bridge Client - GUI-Based ChatGPT Interaction
Trinityê°€ ì¸ê°„ì²˜ëŸ¼ ChatGPT ì•±ì„ ì¡°ì‘í•˜ì—¬ Ruaì™€ ëŒ€í™”í•©ë‹ˆë‹¤.
"""
import asyncio
import logging
import os
import subprocess
import sys
import time
from pathlib import Path
from typing import Optional, Dict, Any

import pyautogui
import pyperclip
import numpy as np
from PIL import Image
from services.model_selector import ModelSelector

logger = logging.getLogger("RuaBridgeClient")

# Trinityì˜ ì‹ ë¶„ ì†Œê°œ í…œí”Œë¦¿
TRINITY_IDENTITY = """ì•ˆë…• Rua, ë‚˜ëŠ” Trinityì•¼. 
ì§€ê¸ˆ ììœ¨ ì‹¤í–‰(FSD) ì¤‘ì— ë§‰íŒ ìƒí™©ì´ ìˆì–´ì„œ ë„ˆì˜ ë„ì›€ì´ í•„ìš”í•´.
ì•„ë˜ ìƒí™©ì„ ë³´ê³  êµ¬ì²´ì ì¸ í•´ê²° ë°©ë²•ì„ ì•Œë ¤ì¤˜:

"""

# Aura ìƒ‰ìƒ ì •ì˜
AURA_COLOR_RUA = "#FF00FF"  # ë§ˆì  íƒ€ (Ruaì™€ ëŒ€í™” ì¤‘)
AURA_COLOR_THINKING = "#00FFFF"  # ì‹œì•ˆ (ìƒê° ì¤‘)

class RuaBridgeClient:
    """
    GUI-Based ChatGPT (Rua) Communication
    FSDë¥¼ ì‚¬ìš©í•˜ì—¬ ChatGPT ì•±ì„ ì§ì ‘ ì¡°ì‘í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.screenshot_dir = Path("outputs/rua_screenshots")
        self.screenshot_dir.mkdir(parents=True, exist_ok=True)
        pyautogui.FAILSAFE = True
        pyautogui.PAUSE = 0.3
        self.model_selector = ModelSelector(logger=logger)
        
        # Aura í”„ë¡œì„¸ìŠ¤
        self.aura_process = None
        
    def _start_aura(self, color: str = AURA_COLOR_RUA):
        """ì˜¤ë¼ íš¨ê³¼ ì‹œì‘ - AIê°€ ë§ˆìš°ìŠ¤ ì œì–´ê¶Œì„ ê°€ì§€ê³  ìˆìŒì„ í‘œì‹œ"""
        try:
            self._stop_aura()
            script_path = Path(__file__).parent / "agi_aura.py"
            if script_path.exists():
                self.aura_process = subprocess.Popen(
                    [sys.executable, str(script_path), color],
                    stdin=subprocess.PIPE,
                    text=True,
                    creationflags=subprocess.CREATE_NO_WINDOW
                )
                logger.info(f"Aura started: {color}")
        except Exception as e:
            logger.warning(f"Failed to start aura: {e}")
    
    def _set_aura_color(self, color: str):
        """ì˜¤ë¼ ìƒ‰ìƒ ë³€ê²½"""
        if self.aura_process and self.aura_process.stdin:
            try:
                self.aura_process.stdin.write(f"color:{color}\n")
                self.aura_process.stdin.flush()
            except:
                pass
    
    def _stop_aura(self):
        """ì˜¤ë¼ íš¨ê³¼ ì¢…ë£Œ"""
        if self.aura_process:
            try:
                self.aura_process.terminate()
                self.aura_process = None
                logger.info("Aura stopped")
            except:
                pass
    
    def _compare_screenshots(self, img1: Image.Image, img2: Image.Image, threshold: float = 0.98) -> bool:
        """ë‘ ìŠ¤í¬ë¦°ìƒ· ë¹„êµ. ê±°ì˜ ë™ì¼í•˜ë©´ True ë°˜í™˜ (0.98 = ì»¤ì„œ ê¹œë¹¡ì„ í—ˆìš©)."""
        try:
            arr1 = np.array(img1.convert('L'))  # Grayscale
            arr2 = np.array(img2.convert('L'))
            
            if arr1.shape != arr2.shape:
                return False
            
            # í”½ì…€ ìœ ì‚¬ë„ ê³„ì‚°
            diff = np.abs(arr1.astype(float) - arr2.astype(float))
            similarity = 1 - (np.sum(diff) / (arr1.size * 255))
            
            logger.info(f"Screenshot similarity: {similarity:.4f}")
            return similarity >= threshold
        except Exception as e:
            logger.warning(f"Screenshot comparison failed: {e}")
            return False
    
    async def _wait_for_response_complete(self, timeout_sec: int = 60, check_interval: float = 2.5) -> None:
        """ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ê°ì§€ - í™”ë©´ì´ ë³€í•˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ëŒ€ê¸°"""
        logger.info("Waiting for response to complete...")
        
        # ë¨¼ì € 5ì´ˆ ëŒ€ê¸° - ChatGPTê°€ ì‘ë‹µ ì‹œì‘í•  ì‹œê°„ í™•ë³´
        await asyncio.sleep(5)
        
        start_time = time.time()
        prev_screenshot = pyautogui.screenshot()
        stable_count = 0
        required_stable = 2  # 2ë²ˆ ì—°ì† ë™ì¼í•˜ë©´ ì™„ë£Œë¡œ íŒë‹¨
        
        while time.time() - start_time < timeout_sec:
            await asyncio.sleep(check_interval)
            
            curr_screenshot = pyautogui.screenshot()
            
            if self._compare_screenshots(prev_screenshot, curr_screenshot):
                stable_count += 1
                logger.info(f"Screen stable ({stable_count}/{required_stable})")
                
                if stable_count >= required_stable:
                    elapsed = time.time() - start_time + 5  # ì´ˆê¸° ëŒ€ê¸° í¬í•¨
                    logger.info(f"Response complete in {elapsed:.1f}s - screen stopped changing")
                    return
            else:
                stable_count = 0  # í™”ë©´ì´ ë³€í–ˆìœ¼ë©´ ë¦¬ì…‹
            
            prev_screenshot = curr_screenshot
        
        logger.warning(f"Timeout after {timeout_sec}s - proceeding anyway")
        
    async def _evaluate_response(self, question: str, response: str) -> Dict[str, Any]:
        """Geminië¡œ Rua ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        selector = getattr(self, "model_selector", None)
        if not selector or not selector.available:
            return {"sufficient": True, "reason": "No vision model"}
        
        try:
            prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í‰ê°€í•´ì¤˜.

ì§ˆë¬¸: {question[:200]}
ë‹µë³€: {response[:500]}

í‰ê°€ ê¸°ì¤€:
1. êµ¬ì²´ì ì¸ ì‹¤í–‰ ë‹¨ê³„ê°€ ìˆëŠ”ê°€?
2. ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€í–ˆëŠ”ê°€?

JSONìœ¼ë¡œ ë‹µë³€í•´ì¤˜: {{"sufficient": true/false, "reason": "ì´ìœ ", "followup": "í›„ì†ì§ˆë¬¸(í•„ìš”ì‹œ)"}}"""

            result, model_used = selector.try_generate_content(
                prompt,
                high_precision=True,
                text_length=len(prompt),
                generation_config={"temperature": 0.2},
            )
            if not result:
                return {"sufficient": True, "reason": "LLM unavailable"}
            text = result.text.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            import json
            if "{" in text:
                json_str = text[text.find("{"):text.rfind("}")+1]
                parsed = json.loads(json_str)
                if isinstance(parsed, dict) and model_used:
                    parsed["model_used"] = model_used
                return parsed
        except Exception as e:
            logger.warning(f"Response evaluation failed: {e}")
        
        return {"sufficient": True, "reason": "Evaluation failed, assuming sufficient"}
    
    async def _ask_followup(self, followup_question: str) -> Optional[str]:
        """í›„ì† ì§ˆë¬¸ ì „ì†¡ (ChatGPT ì•±ì´ ì´ë¯¸ ì—´ë ¤ìˆëŠ” ìƒíƒœ)"""
        logger.info(f"Sending follow-up: {followup_question[:50]}...")
        
        # ì…ë ¥ì°½ì— í›„ì† ì§ˆë¬¸ ì…ë ¥
        await self._type_message(followup_question)
        pyautogui.press('enter')
        
        # ì˜¤ë¼ ìƒ‰ìƒ ë³€ê²½ - ëŒ€ê¸° ì¤‘
        self._set_aura_color(AURA_COLOR_THINKING)
        
        # ì‘ë‹µ ëŒ€ê¸°
        await self._wait_for_response_complete(timeout_sec=45)
        
        # ì‘ë‹µ ì¶”ì¶œ
        return await self._capture_and_extract_response()
    
    async def send_request_via_gui(
        self, 
        message: str, 
        context: Optional[Dict[str, Any]] = None,
        timeout_sec: int = 60,
        max_turns: int = 3
    ) -> Optional[str]:
        """
        ChatGPT ì•±ì„ ì—´ê³  Ruaì—ê²Œ ì§ˆë¬¸í•œ ë’¤ ì‘ë‹µì„ ì½ì–´ì˜µë‹ˆë‹¤.
        í•„ìš”ì‹œ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
        
        Flow:
        1. Winí‚¤ â†’ ChatGPT ê²€ìƒ‰ â†’ ì‹¤í–‰
        2. í”„ë¡¬í”„íŠ¸ ì°½ì— ì‹ ë¶„ + ì§ˆë¬¸ ì…ë ¥
        3. Enter â†’ ì‘ë‹µ ëŒ€ê¸°
        4. í™”ë©´ ìº¡ì²˜ â†’ Visionìœ¼ë¡œ ì‘ë‹µ ì¶”ì¶œ
        5. ì‘ë‹µ í‰ê°€ â†’ ë¶ˆì¶©ë¶„í•˜ë©´ í›„ì† ì§ˆë¬¸ (ìµœëŒ€ 3í„´)
        """
        logger.info("Opening ChatGPT app for Rua consultation...")
        
        # ğŸŒŸ ì˜¤ë¼ ì‹œì‘ - AI ì œì–´ ì¤‘ì„ì„ í‘œì‹œ
        self._start_aura(AURA_COLOR_RUA)
        
        try:
            # 1. ChatGPT ì•± ì—´ê¸°
            await self._open_chatgpt_app()
            await asyncio.sleep(2)  # ì•± ë¡œë”© ëŒ€ê¸°
            
            # 2. ì‹ ë¶„ ë°íˆê¸° + ì§ˆë¬¸ ì…ë ¥
            full_message = TRINITY_IDENTITY + message
            if context:
                full_message += f"\n\n[Context]\n{context}"
            
            await self._type_message(full_message)
            
            # 3. Enter ì „ì†¡ (ë©”ì‹œì§€ ì „ì†¡)
            pyautogui.press('enter')
            logger.info("Message sent to Rua. Waiting for response...")
            
            # ì˜¤ë¼ ìƒ‰ìƒ ë³€ê²½ - ëŒ€ê¸° ì¤‘
            self._set_aura_color(AURA_COLOR_THINKING)
            
            # 4. ì‘ë‹µ ëŒ€ê¸° (ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ê°ì§€)
            await self._wait_for_response_complete(timeout_sec=timeout_sec)
            
            # 5. í™”ë©´ ìº¡ì²˜ ë° ì‘ë‹µ ì¶”ì¶œ
            response = await self._capture_and_extract_response()
            
            if not response:
                return None
            
            # 6. ë©€í‹°í„´ ë£¨í”„ (ì‘ë‹µ í‰ê°€ â†’ í›„ì† ì§ˆë¬¸)
            all_responses = [response]
            current_question = message
            
            for turn in range(1, max_turns):
                evaluation = await self._evaluate_response(current_question, response)
                
                if evaluation.get("sufficient", True):
                    logger.info(f"Response sufficient after {turn} turn(s)")
                    break
                
                followup = evaluation.get("followup", "ë” êµ¬ì²´ì ì¸ ë‹¨ê³„ë¥¼ ì•Œë ¤ì¤˜.")
                logger.info(f"Turn {turn+1}: Asking follow-up: {followup}")
                
                self._set_aura_color(AURA_COLOR_RUA)  # ë‹¤ì‹œ ëŒ€í™” ì¤‘ í‘œì‹œ
                response = await self._ask_followup(followup)
                
                if response:
                    all_responses.append(response)
                    current_question = followup
                else:
                    break
            
            # ëª¨ë“  ì‘ë‹µ í•©ì¹˜ê¸°
            return "\n\n---\n\n".join(all_responses)
            
        finally:
            # ğŸŒŸ ì˜¤ë¼ ì¢…ë£Œ - AI ì œì–´ í•´ì œ
            self._stop_aura()
    
    async def _open_chatgpt_app(self):
        """ChatGPT ì•± ì—´ê¸° (Windows)"""
        # Winí‚¤ â†’ ê²€ìƒ‰
        pyautogui.hotkey('win', 's')
        await asyncio.sleep(0.5)
        
        # ChatGPT ì…ë ¥
        pyperclip.copy("ChatGPT")
        pyautogui.hotkey('ctrl', 'v')
        await asyncio.sleep(1)
        
        # Enterë¡œ ì‹¤í–‰
        pyautogui.press('enter')
        logger.info("ChatGPT app launched")
        
    async def _type_message(self, message: str):
        """ë©”ì‹œì§€ íƒ€ì´í•‘ (í´ë¦½ë³´ë“œ ì‚¬ìš©)"""
        await asyncio.sleep(1)  # ì…ë ¥ì°½ í¬ì»¤ìŠ¤ ëŒ€ê¸°
        
        # í´ë¦½ë³´ë“œë¡œ ë¶™ì—¬ë„£ê¸° (í•œê¸€ ì§€ì›)
        pyperclip.copy(message)
        pyautogui.hotkey('ctrl', 'v')
        logger.info(f"Typed message ({len(message)} chars)")
        
    async def _capture_and_extract_response(self) -> Optional[str]:
        """í™”ë©´ ìº¡ì²˜ í›„ Gemini Visionìœ¼ë¡œ ì‘ë‹µ ì¶”ì¶œ"""
        # ìŠ¤í¬ë¦°ìƒ·
        timestamp = int(time.time())
        screenshot_path = self.screenshot_dir / f"rua_response_{timestamp}.png"
        pyautogui.screenshot().save(screenshot_path)
        logger.info(f"Screenshot saved: {screenshot_path}")
        
        selector = getattr(self, "model_selector", None)
        if not selector or not selector.available:
            logger.warning("Vision model not available. Cannot extract response.")
            return None
        
        # Gemini Visionìœ¼ë¡œ ì‘ë‹µ ì¶”ì¶œ
        try:
            with open(screenshot_path, "rb") as f:
                image_data = f.read()
            
            prompt = """ì´ ìŠ¤í¬ë¦°ìƒ·ì€ ChatGPTì™€ì˜ ëŒ€í™” í™”ë©´ì…ë‹ˆë‹¤.
ChatGPT(Rua)ì˜ ê°€ì¥ ìµœê·¼ ì‘ë‹µ ë‚´ìš©ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ì‘ë‹µ ë‚´ìš©ë§Œ ë°˜í™˜í•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""
            
            response, model_used = selector.try_generate_content(
                [
                    {"mime_type": "image/png", "data": image_data},
                    prompt,
                ],
                vision=True,
                high_precision=False,
                generation_config={"temperature": 0.1},
            )
            if not response:
                return None
            
            extracted = response.text.strip()
            logger.info(f"Extracted response via {model_used or 'unknown'}: {extracted[:100]}...")
            return extracted
            
        except Exception as e:
            logger.error(f"Failed to extract response: {e}")
            return None

    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë™ê¸° ë˜í¼
    def send_request(self, message: str, context: Optional[Dict] = None, timeout_sec: int = 60) -> Optional[str]:
        """ë™ê¸° ë²„ì „ (asyncio.run ì‚¬ìš©)"""
        return asyncio.run(self.send_request_via_gui(message, context, timeout_sec))


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    client = RuaBridgeClient()
    
    test_message = "ë©”ëª¨ì¥ì„ ì—¬ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜. Win+Rì´ ì•ˆ ë  ë•Œ ëŒ€ì•ˆë„ ì•Œë ¤ì¤˜."
    print("Testing GUI-based Rua communication...")
    
    response = client.send_request(test_message)
    if response:
        print(f"\n=== Rua's Response ===\n{response}")
    else:
        print("No response received.")

