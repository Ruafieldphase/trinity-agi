"""
Lua Flow Collector - ë£¨ì•„ì˜ íë¦„ì„ ìžë™ìœ¼ë¡œ AGI ë¦¬ë“¬ì— ì—°ê²°
================================================================
ë£¨ì•„ì˜ ë¦¬ë“¬ ì§€ì‹œ: "ë‚˜ëŠ” íë¥´ê³ , ë„ˆëŠ” ì—®ì–´ì¤˜."

ìˆ˜ì§‘ ëŒ€ìƒ:
- OBS ë…¹í™” (í™”ë©´, ê²Œìž„ í”Œë ˆì´)
- ëŒ€í™” íë¦„ (í–¥í›„ í™•ìž¥)
- ìŠ¤í¬ë¦° ìº¡ì²˜ (í–¥í›„ í™•ìž¥)

íë¦„:
1. íŒ¨í„´ ì¶”ì¶œ â†’ 2. ë§¥ë½ ì •ì œ â†’ 3. ë£¨í”„ ì—°ê²° â†’ 4. AGI ë¦¬ë“¬ ì—…ë°ì´íŠ¸ â†’ 5. ARI í”¼ë“œë°± ìˆœí™˜
"""
import asyncio
import json
import logging
import os
import sys
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
from enum import Enum

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.ari_engine import get_ari_engine

logger = logging.getLogger("LuaFlowCollector")

# Configuration
WORKSPACE_ROOT = Path(__file__).parent.parent
OBS_DIR = WORKSPACE_ROOT / "input" / "obs_recode"
PROCESSED_LOG = WORKSPACE_ROOT / "memory" / "lua_flow_processed.json"
RESONANCE_LEDGER = WORKSPACE_ROOT / "fdo_agi_repo" / "memory" / "resonance_ledger.jsonl"
FEELING_FILE = WORKSPACE_ROOT / "outputs" / "feeling_latest.json"


class FlowType(Enum):
    OBS_RECORDING = "obs_recording"
    CONVERSATION = "conversation"
    SCREEN_CAPTURE = "screen_capture"


@dataclass
class FlowData:
    """ë£¨ì•„ì˜ íë¦„ ë°ì´í„°"""
    flow_type: FlowType
    source_file: str
    timestamp: str
    patterns: List[Dict[str, Any]]
    context: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "flow_type": self.flow_type.value,
            "source_file": self.source_file,
            "timestamp": self.timestamp,
            "patterns": self.patterns,
            "context": self.context
        }


class LuaFlowCollector:
    """
    ë£¨ì•„ì˜ íë¦„ì„ ìˆ˜ì§‘í•˜ì—¬ AGI ë¦¬ë“¬ì— ì—°ê²°
    
    "ë‚´ê°€ OBSë¡œ ë…¹í™”í•˜ëŠ” ëª¨ë“  í™”ë©´ê³¼ ê²Œìž„ í”Œë ˆì´,
    ê·¸ë¦¬ê³  ë£¨ì•„ì™€ì˜ ëŒ€í™” íë¦„ì€
    ê·¸ ìžì²´ê°€ AGIì˜ ê²½í—˜ì´ìž ìƒëª… íŒ¨í„´ì´ì•¼."
    """
    
    def __init__(self):
        self.ari_engine = get_ari_engine()
        self.processed = self._load_processed()
        self._ensure_dirs()
    
    def _ensure_dirs(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        PROCESSED_LOG.parent.mkdir(parents=True, exist_ok=True)
    
    def _load_processed(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ëœ íŒŒì¼ ë¡œê·¸ ë¡œë“œ"""
        if PROCESSED_LOG.exists():
            try:
                with open(PROCESSED_LOG, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {"obs": [], "conversation": []}
        return {"obs": [], "conversation": []}
    
    def _save_processed(self):
        """ì²˜ë¦¬ëœ íŒŒì¼ ë¡œê·¸ ì €ìž¥"""
        with open(PROCESSED_LOG, 'w', encoding='utf-8') as f:
            json.dump(self.processed, f, indent=2, ensure_ascii=False)
    
    async def scan_obs_recordings(self) -> List[Path]:
        """ìƒˆ OBS ë…¹í™” íŒŒì¼ ìŠ¤ìº”"""
        if not OBS_DIR.exists():
            logger.warning(f"OBS directory not found: {OBS_DIR}")
            return []
        
        new_files = []
        for f in OBS_DIR.glob("*.mp4"):
            if f.name not in self.processed["obs"]:
                new_files.append(f)
        
        # íŒŒì¼ í¬ê¸° ìˆœ ì •ë ¬ (ìž‘ì€ ê²ƒ ë¨¼ì €, ë¹ ë¥¸ í”¼ë“œë°±ìš©)
        new_files.sort(key=lambda x: x.stat().st_size)
        logger.info(f"Found {len(new_files)} new OBS recordings")
        return new_files
    
    async def extract_flow_from_obs(self, video_path: Path) -> Optional[FlowData]:
        """OBS ë…¹í™”ì—ì„œ íë¦„ íŒ¨í„´ ì¶”ì¶œ (LargeVideoLearnerì™€ ì—°ë™)"""
        try:
            from services.large_video_learner import LargeVideoLearner
            
            learner = LargeVideoLearner()
            file_size_gb = video_path.stat().st_size / (1024**3)
            
            logger.info(f"Extracting flow from {video_path.name} ({file_size_gb:.2f}GB)")
            
            # í”„ë ˆìž„ ì¶”ì¶œ
            frames = learner.extract_frames(video_path)
            if not frames:
                logger.warning(f"No frames extracted from {video_path.name}")
                return None
            
            # Gemini ë¶„ì„
            analysis = await learner.analyze_frames(frames, video_path.name)
            if not analysis:
                logger.warning(f"Analysis failed for {video_path.name}")
                return None
            
            # FlowData ìƒì„±
            return FlowData(
                flow_type=FlowType.OBS_RECORDING,
                source_file=video_path.name,
                timestamp=datetime.now().isoformat(),
                patterns=analysis.get("steps", []),
                context={
                    "goal": analysis.get("goal", "Unknown"),
                    "success": analysis.get("success", False),
                    "file_size_gb": file_size_gb,
                    "frame_count": len(frames)
                }
            )
        except Exception as e:
            logger.error(f"Failed to extract flow from {video_path.name}: {e}")
            return None
    
    async def inject_to_ari(self, flow: FlowData):
        """ARIì— ê²½í—˜ ì£¼ìž…"""
        experience = {
            "type": "lua_flow",
            "flow_type": flow.flow_type.value,
            "source": flow.source_file,
            "goal": flow.context.get("goal", "Unknown"),
            "patterns": flow.patterns,
            "timestamp": flow.timestamp,
            "origin": "Lua (Flow Collector)"
        }
        
        self.ari_engine.learning.add_experience(experience)
        logger.info(f"âœ… Injected to ARI: {flow.source_file}")
    
    async def inject_to_rhythm_loop(self, flow: FlowData):
        """ë¦¬ë“¬ ë£¨í”„ì— íë¦„ ì—°ê²° (Resonance Ledger)"""
        entry = {
            "timestamp": flow.timestamp,
            "type": "lua_flow_signal",
            "source": flow.source_file,
            "flow_type": flow.flow_type.value,
            "goal": flow.context.get("goal", ""),
            "pattern_count": len(flow.patterns),
            "message": f"ë£¨ì•„ì˜ íë¦„ì´ ë„ì°©í–ˆìŠµë‹ˆë‹¤: {flow.source_file}"
        }
        
        try:
            with open(RESONANCE_LEDGER, 'a', encoding='utf-8') as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
            logger.info(f"âœ… Logged to Resonance Ledger: {flow.source_file}")
        except Exception as e:
            logger.error(f"Failed to log to resonance ledger: {e}")
        
        # Feeling ì—…ë°ì´íŠ¸ - ë¦¬ë“¬ì— ìƒˆ íë¦„ ì‹ í˜¸ ì „ë‹¬
        try:
            feeling = {"flow_received": True, "last_flow": flow.source_file, "timestamp": flow.timestamp}
            if FEELING_FILE.exists():
                with open(FEELING_FILE, 'r', encoding='utf-8') as f:
                    feeling = json.load(f)
                feeling["lua_flow_signal"] = {
                    "source": flow.source_file,
                    "timestamp": flow.timestamp,
                    "goal": flow.context.get("goal", "")
                }
            with open(FEELING_FILE, 'w', encoding='utf-8') as f:
                json.dump(feeling, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Failed to update feeling: {e}")
    
    async def process_one(self, video_path: Path) -> bool:
        """í•˜ë‚˜ì˜ ë…¹í™” íŒŒì¼ ì²˜ë¦¬"""
        logger.info(f"Processing: {video_path.name}")
        
        # 1. íŒ¨í„´ ì¶”ì¶œ
        flow = await self.extract_flow_from_obs(video_path)
        if not flow:
            return False
        
        # 2. ARI ì£¼ìž…
        await self.inject_to_ari(flow)
        
        # 3. ë¦¬ë“¬ ë£¨í”„ ì—°ê²°
        await self.inject_to_rhythm_loop(flow)
        
        # 4. ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
        self.processed["obs"].append(video_path.name)
        self._save_processed()
        
        logger.info(f"âœ¨ Flow integrated: {video_path.name}")
        return True
    
    async def run_once(self) -> int:
        """í•œ ë²ˆ ì‹¤í–‰ (ëª¨ë“  ìƒˆ íŒŒì¼ ì²˜ë¦¬)"""
        new_files = await self.scan_obs_recordings()
        processed_count = 0
        
        for video in new_files:
            try:
                if await self.process_one(video):
                    processed_count += 1
                await asyncio.sleep(2)  # ê³¼ë¶€í•˜ ë°©ì§€
            except Exception as e:
                logger.error(f"Error processing {video.name}: {e}")
        
        return processed_count
    
    async def run_daemon(self, interval: int = 300):
        """ë°ëª¬ ëª¨ë“œ (ì£¼ê¸°ì  ìŠ¤ìº”)"""
        logger.info(f"ðŸŒŠ Lua Flow Collector started (interval: {interval}s)")
        
        while True:
            try:
                count = await self.run_once()
                if count > 0:
                    logger.info(f"Processed {count} new recording(s)")
            except Exception as e:
                logger.error(f"Daemon cycle error: {e}")
            
            await asyncio.sleep(interval)


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    collector = LuaFlowCollector()
    
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "--daemon":
        await collector.run_daemon()
    else:
        count = await collector.run_once()
        print(f"Processed {count} file(s)")


if __name__ == "__main__":
    asyncio.run(main())
