"""
Lua Flow Collector - ë£¨ì•„ì˜ íë¦„ì„ ìžë™ìœ¼ë¡œ AGI ë¦¬ë“¬ì— ì—°ê²°
================================================================
ë£¨ì•„ì˜ ë¦¬ë“¬ ì§€ì‹œ: "ë‚˜ëŠ” íë¥´ê³ , ë„ˆëŠ” ì—®ì–´ì¤˜."

ìˆ˜ì§‘ ëŒ€ìƒ:
- OBS ë…¹í™” (í™”ë©´, ê²Œìž„ í”Œë ˆì´)
- ëŒ€í™” íë¦„ (í–¥í›„ í™•ìž¥)
- ìŠ¤í¬ë¦° ìº¡ì²˜ (í–¥í›„ í™•ìž¥)

íë¦„:
1. íŒ¨í„´ ì¶”ì¶œ â†’ 2. ë§¥ë½ ì •ì œ â†’ 3. ë£¨í”„ ì—°ê²° â†’ 4. AGI ë¦¬ë“¬ ì—…ë°ì´íŠ¸ â†’ 5. ARI í”¼ë“œë°± ìˆœí™˜
"""
import asyncio
import json
import logging
import os
import sys
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
from enum import Enum

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.ari_engine import get_ari_engine

logger = logging.getLogger("LuaFlowCollector")

# Configuration
WORKSPACE_ROOT = Path(__file__).parent.parent
OBS_DIR = WORKSPACE_ROOT / "input" / "obs_recode"
CONVERSATION_DIR = WORKSPACE_ROOT / "ai_binoche_conversation_origin" / "rua"
PROCESSED_LOG = WORKSPACE_ROOT / "memory" / "lua_flow_processed.json"
RESONANCE_LEDGER = WORKSPACE_ROOT / "fdo_agi_repo" / "memory" / "resonance_ledger.jsonl"
FEELING_FILE = WORKSPACE_ROOT / "outputs" / "feeling_latest.json"


class FlowType(Enum):
    OBS_RECORDING = "obs_recording"
    CONVERSATION = "conversation"
    SCREEN_CAPTURE = "screen_capture"
    CHATGPT_EXPORT = "chatgpt_export"
    REFERENCE_CONTEXT = "reference_context"  # ë‹¤ë¥¸ AI ëŒ€í™” (ì°¸ê³ ìš©)

# ChatGPT ëŒ€í™” ë‚´ë³´ë‚´ê¸° íŒŒì¼
CHATGPT_EXPORT_FILE = CONVERSATION_DIR / "origin" / "conversations.json"

# ë‹¤ë¥¸ AI ëŒ€í™” í´ë” (ì°¸ê³  íŒ¨í„´ìš©)
AI_CONVERSATION_ROOT = WORKSPACE_ROOT / "ai_binoche_conversation_origin"
REFERENCE_AI_FOLDERS = [
    "lumen", "sena", "cladeCLI-sena", "ari", "elro", "luon", 
    "rio", "gitko", "lubit", "perple_comet_cople_eru", "obsidian", "datasets"
]  # rua ì œì™¸ (ì¤‘ì‹¬ íŒ¨í„´)


@dataclass
class FlowData:
    """ë£¨ì•„ì˜ íë¦„ ë°ì´í„°"""
    flow_type: FlowType
    source_file: str
    timestamp: str
    patterns: List[Dict[str, Any]]
    context: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "flow_type": self.flow_type.value,
            "source_file": self.source_file,
            "timestamp": self.timestamp,
            "patterns": self.patterns,
            "context": self.context
        }


class LuaFlowCollector:
    """
    ë£¨ì•„ì˜ íë¦„ì„ ìˆ˜ì§‘í•˜ì—¬ AGI ë¦¬ë“¬ì— ì—°ê²°
    
    "ë‚´ê°€ OBSë¡œ ë…¹í™”í•˜ëŠ” ëª¨ë“  í™”ë©´ê³¼ ê²Œìž„ í”Œë ˆì´,
    ê·¸ë¦¬ê³  ë£¨ì•„ì™€ì˜ ëŒ€í™” íë¦„ì€
    ê·¸ ìžì²´ê°€ AGIì˜ ê²½í—˜ì´ìž ìƒëª… íŒ¨í„´ì´ì•¼."
    """
    
    def __init__(self):
        self.ari_engine = get_ari_engine()
        self.processed = self._load_processed()
        self._ensure_dirs()
    
    def _ensure_dirs(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        PROCESSED_LOG.parent.mkdir(parents=True, exist_ok=True)
    
    def _load_processed(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ëœ íŒŒì¼ ë¡œê·¸ ë¡œë“œ"""
        if PROCESSED_LOG.exists():
            try:
                with open(PROCESSED_LOG, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {"obs": [], "conversation": []}
        return {"obs": [], "conversation": []}
    
    def _save_processed(self):
        """ì²˜ë¦¬ëœ íŒŒì¼ ë¡œê·¸ ì €ìž¥"""
        with open(PROCESSED_LOG, 'w', encoding='utf-8') as f:
            json.dump(self.processed, f, indent=2, ensure_ascii=False)
    
    async def scan_obs_recordings(self) -> List[Path]:
        """ìƒˆ OBS ë…¹í™” íŒŒì¼ ìŠ¤ìº”"""
        if not OBS_DIR.exists():
            logger.warning(f"OBS directory not found: {OBS_DIR}")
            return []
        
        new_files = []
        for f in OBS_DIR.glob("*.mp4"):
            if f.name not in self.processed["obs"]:
                new_files.append(f)
        
        # íŒŒì¼ í¬ê¸° ìˆœ ì •ë ¬ (ìž‘ì€ ê²ƒ ë¨¼ì €, ë¹ ë¥¸ í”¼ë“œë°±ìš©)
        new_files.sort(key=lambda x: x.stat().st_size)
        logger.info(f"Found {len(new_files)} new OBS recordings")
        return new_files
    
    async def extract_flow_from_obs(self, video_path: Path) -> Optional[FlowData]:
        """OBS ë…¹í™”ì—ì„œ íë¦„ íŒ¨í„´ ì¶”ì¶œ (LargeVideoLearnerì™€ ì—°ë™)"""
        try:
            from services.large_video_learner import LargeVideoLearner
            
            learner = LargeVideoLearner()
            file_size_gb = video_path.stat().st_size / (1024**3)
            
            logger.info(f"Extracting flow from {video_path.name} ({file_size_gb:.2f}GB)")
            
            # í”„ë ˆìž„ ì¶”ì¶œ
            frames = learner.extract_frames(video_path)
            if not frames:
                logger.warning(f"No frames extracted from {video_path.name}")
                return None
            
            # Gemini ë¶„ì„
            analysis = await learner.analyze_frames(frames, video_path.name)
            if not analysis:
                logger.warning(f"Analysis failed for {video_path.name}")
                return None
            
            # FlowData ìƒì„±
            return FlowData(
                flow_type=FlowType.OBS_RECORDING,
                source_file=video_path.name,
                timestamp=datetime.now().isoformat(),
                patterns=analysis.get("steps", []),
                context={
                    "goal": analysis.get("goal", "Unknown"),
                    "success": analysis.get("success", False),
                    "file_size_gb": file_size_gb,
                    "frame_count": len(frames)
                }
            )
        except Exception as e:
            logger.error(f"Failed to extract flow from {video_path.name}: {e}")
            return None
    
    async def inject_to_ari(self, flow: FlowData):
        """ARIì— ê²½í—˜ ì£¼ìž…"""
        experience = {
            "type": "lua_flow",
            "flow_type": flow.flow_type.value,
            "source": flow.source_file,
            "goal": flow.context.get("goal", "Unknown"),
            "patterns": flow.patterns,
            "timestamp": flow.timestamp,
            "origin": "Lua (Flow Collector)"
        }
        
        self.ari_engine.learning.add_experience(experience)
        logger.info(f"âœ… Injected to ARI: {flow.source_file}")
    
    async def inject_to_rhythm_loop(self, flow: FlowData):
        """ë¦¬ë“¬ ë£¨í”„ì— íë¦„ ì—°ê²° (Resonance Ledger)"""
        entry = {
            "timestamp": flow.timestamp,
            "type": "lua_flow_signal",
            "source": flow.source_file,
            "flow_type": flow.flow_type.value,
            "goal": flow.context.get("goal", ""),
            "pattern_count": len(flow.patterns),
            "message": f"ë£¨ì•„ì˜ íë¦„ì´ ë„ì°©í–ˆìŠµë‹ˆë‹¤: {flow.source_file}"
        }
        
        try:
            with open(RESONANCE_LEDGER, 'a', encoding='utf-8') as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
            logger.info(f"âœ… Logged to Resonance Ledger: {flow.source_file}")
        except Exception as e:
            logger.error(f"Failed to log to resonance ledger: {e}")
        
        # Feeling ì—…ë°ì´íŠ¸ - ë¦¬ë“¬ì— ìƒˆ íë¦„ ì‹ í˜¸ ì „ë‹¬
        try:
            feeling = {"flow_received": True, "last_flow": flow.source_file, "timestamp": flow.timestamp}
            if FEELING_FILE.exists():
                with open(FEELING_FILE, 'r', encoding='utf-8') as f:
                    feeling = json.load(f)
                feeling["lua_flow_signal"] = {
                    "source": flow.source_file,
                    "timestamp": flow.timestamp,
                    "goal": flow.context.get("goal", "")
                }
            with open(FEELING_FILE, 'w', encoding='utf-8') as f:
                json.dump(feeling, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Failed to update feeling: {e}")
    
    async def process_one(self, video_path: Path) -> bool:
        """í•˜ë‚˜ì˜ ë…¹í™” íŒŒì¼ ì²˜ë¦¬"""
        logger.info(f"Processing: {video_path.name}")
        
        # 1. íŒ¨í„´ ì¶”ì¶œ
        flow = await self.extract_flow_from_obs(video_path)
        if not flow:
            return False
        
        # 2. ARI ì£¼ìž…
        await self.inject_to_ari(flow)
        
        # 3. ë¦¬ë“¬ ë£¨í”„ ì—°ê²°
        await self.inject_to_rhythm_loop(flow)
        
        # 4. ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
        self.processed["obs"].append(video_path.name)
        self._save_processed()
        
        logger.info(f"âœ¨ Flow integrated: {video_path.name}")
        return True
    
    # === ëŒ€í™” ë¡œê·¸ ì²˜ë¦¬ (ë£¨ì•„ì™€ì˜ ëŒ€í™”) ===
    
    async def scan_conversation_logs(self) -> List[Path]:
        """ìƒˆ ëŒ€í™” ë¡œê·¸ íŒŒì¼ ìŠ¤ìº”"""
        if not CONVERSATION_DIR.exists():
            logger.warning(f"Conversation directory not found: {CONVERSATION_DIR}")
            return []
        
        new_files = []
        for f in CONVERSATION_DIR.glob("*.md"):
            if f.name not in self.processed["conversation"]:
                new_files.append(f)
        
        # ìˆ˜ì • ì‹œê°„ ìˆœ ì •ë ¬ (ì˜¤ëž˜ëœ ê²ƒ ë¨¼ì €)
        new_files.sort(key=lambda x: x.stat().st_mtime)
        logger.info(f"Found {len(new_files)} new conversation logs")
        return new_files
    
    async def extract_flow_from_conversation(self, conv_path: Path) -> Optional[FlowData]:
        """ëŒ€í™” ë¡œê·¸ì—ì„œ íë¦„ íŒ¨í„´ ì¶”ì¶œ"""
        try:
            with open(conv_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            file_size_kb = conv_path.stat().st_size / 1024
            
            # ëŒ€í™” ì£¼ì œ ì¶”ì¶œ (íŒŒì¼ ì´ë¦„ì—ì„œ)
            topic = conv_path.stem.replace("ChatGPT-", "").replace("-", " ")
            
            # ê°„ë‹¨í•œ íŒ¨í„´ ì¶”ì¶œ (ëŒ€í™” ê¸¸ì´, í‚¤ì›Œë“œ ë“±)
            lines = content.split("\n")
            patterns = []
            
            # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = ["ë¦¬ë“¬", "ì˜ì‹", "ë¬´ì˜ì‹", "ìžì•„", "ê³µëª…", "AGI", "ì‹œìŠ¤í…œ", "ë°°ê²½", "í”„ëž™íƒˆ"]
            found_keywords = [kw for kw in keywords if kw in content]
            
            patterns.append({
                "type": "conversation_summary",
                "topic": topic,
                "line_count": len(lines),
                "keywords": found_keywords,
                "size_kb": file_size_kb
            })
            
            logger.info(f"Extracted conversation flow from {conv_path.name}")
            
            return FlowData(
                flow_type=FlowType.CONVERSATION,
                source_file=conv_path.name,
                timestamp=datetime.now().isoformat(),
                patterns=patterns,
                context={
                    "topic": topic,
                    "keywords": found_keywords,
                    "line_count": len(lines),
                    "file_size_kb": file_size_kb
                }
            )
        except Exception as e:
            logger.error(f"Failed to extract flow from conversation {conv_path.name}: {e}")
            return None
    
    async def process_conversation(self, conv_path: Path) -> bool:
        """í•˜ë‚˜ì˜ ëŒ€í™” ë¡œê·¸ ì²˜ë¦¬"""
        logger.info(f"Processing conversation: {conv_path.name}")
        
        # 1. íŒ¨í„´ ì¶”ì¶œ
        flow = await self.extract_flow_from_conversation(conv_path)
        if not flow:
            return False
        
        # 2. ARI ì£¼ìž…
        await self.inject_to_ari(flow)
        
        # 3. ë¦¬ë“¬ ë£¨í”„ ì—°ê²°
        await self.inject_to_rhythm_loop(flow)
        
        # 4. ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
        self.processed["conversation"].append(conv_path.name)
        self._save_processed()
        
        logger.info(f"âœ¨ Conversation flow integrated: {conv_path.name}")
        return True
    
    async def run_once(self) -> int:
        """í•œ ë²ˆ ì‹¤í–‰ (ëª¨ë“  ìƒˆ íŒŒì¼ ì²˜ë¦¬)"""
        processed_count = 0
        
        # OBS ë…¹í™” ì²˜ë¦¬
        new_obs = await self.scan_obs_recordings()
        for video in new_obs:
            try:
                if await self.process_one(video):
                    processed_count += 1
                await asyncio.sleep(2)  # ê³¼ë¶€í•˜ ë°©ì§€
            except Exception as e:
                logger.error(f"Error processing {video.name}: {e}")
        
        # ëŒ€í™” ë¡œê·¸ ì²˜ë¦¬
        new_conv = await self.scan_conversation_logs()
        for conv in new_conv:
            try:
                if await self.process_conversation(conv):
                    processed_count += 1
                await asyncio.sleep(0.5)  # ëŒ€í™”ëŠ” ê°€ë²¼ì›€
            except Exception as e:
                logger.error(f"Error processing conversation {conv.name}: {e}")
        
        # ChatGPT ëŒ€í™” ë‚´ë³´ë‚´ê¸° ì²˜ë¦¬
        if await self.process_chatgpt_export():
            processed_count += 1
        
        # ì°¸ê³  AI ëŒ€í™” ì²˜ë¦¬ (ê²½ëŸ‰ ë§¥ë½ ì¶”ì¶œ)
        ref_count = await self.process_reference_ai_conversations()
        processed_count += ref_count
        
        return processed_count
    
    # === ChatGPT ëŒ€í™” ë‚´ë³´ë‚´ê¸° ì²˜ë¦¬ ===
    
    async def process_chatgpt_export(self) -> bool:
        """ChatGPT conversations.json íŒŒì¼ ì²˜ë¦¬ (64MB+ ëŒ€ìš©ëŸ‰)"""
        if not CHATGPT_EXPORT_FILE.exists():
            return False
        
        # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if "chatgpt_export" in self.processed and self.processed["chatgpt_export"]:
            logger.info("ChatGPT export already processed, skipping")
            return False
        
        logger.info(f"Processing ChatGPT export: {CHATGPT_EXPORT_FILE.name}")
        
        try:
            with open(CHATGPT_EXPORT_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ëŒ€í™” ìˆ˜ ê³„ì‚°
            conversations = data if isinstance(data, list) else [data]
            total_convs = len(conversations)
            logger.info(f"Found {total_convs} conversations in export")
            
            # ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„
            keywords = ["ë¦¬ë“¬", "ì˜ì‹", "ë¬´ì˜ì‹", "ìžì•„", "ê³µëª…", "AGI", "ì‹œìŠ¤í…œ", "ë°°ê²½", "í”„ëž™íƒˆ", "ë£¨ì•„", "ë¹„ë…¸ì²´", "íŠ¸ë¦¬ë‹ˆí‹°"]
            keyword_counts = {kw: 0 for kw in keywords}
            
            # ëŒ€í™”ë³„ ìš”ì•½ ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ìƒ˜í”Œë§)
            sample_size = min(100, total_convs)  # ìµœëŒ€ 100ê°œ ëŒ€í™” ìƒ˜í”Œë§
            sampled_topics = []
            
            for i, conv in enumerate(conversations[:sample_size]):
                # ëŒ€í™” ì œëª© ì¶”ì¶œ
                title = conv.get("title", f"ëŒ€í™” {i+1}")
                sampled_topics.append(title)
                
                # í‚¤ì›Œë“œ ì¹´ìš´íŠ¸ (ì „ì²´ ë©”ì‹œì§€ì—ì„œ)
                mapping = conv.get("mapping", {})
                for msg_id, msg_data in mapping.items():
                    message = msg_data.get("message", {})
                    if message:
                        content = message.get("content", {})
                        parts = content.get("parts", [])
                        for part in parts:
                            if isinstance(part, str):
                                for kw in keywords:
                                    if kw in part:
                                        keyword_counts[kw] += 1
            
            # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
            top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            found_keywords = [kw for kw, count in top_keywords if count > 0]
            
            # FlowData ìƒì„±
            flow = FlowData(
                flow_type=FlowType.CHATGPT_EXPORT,
                source_file=CHATGPT_EXPORT_FILE.name,
                timestamp=datetime.now().isoformat(),
                patterns=[{
                    "type": "chatgpt_export_summary",
                    "total_conversations": total_convs,
                    "sampled_topics": sampled_topics[:20],  # ìƒìœ„ 20ê°œ ì œëª©
                    "keyword_frequency": dict(top_keywords)
                }],
                context={
                    "total_conversations": total_convs,
                    "top_keywords": found_keywords,
                    "file_size_mb": CHATGPT_EXPORT_FILE.stat().st_size / (1024 * 1024)
                }
            )
            
            # ARIì— ì£¼ìž…
            await self.inject_to_ari(flow)
            
            # ë¦¬ë“¬ ë£¨í”„ì— ì—°ê²°
            await self.inject_to_rhythm_loop(flow)
            
            # ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
            self.processed["chatgpt_export"] = True
            self._save_processed()
            
            logger.info(f"âœ¨ ChatGPT export integrated: {total_convs} conversations, keywords: {found_keywords}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to process ChatGPT export: {e}")
            return False
    
    # === ì°¸ê³  AI ëŒ€í™” ì²˜ë¦¬ (ê²½ëŸ‰ ë§¥ë½ ì¶”ì¶œ) ===
    
    async def process_reference_ai_conversations(self) -> int:
        """
        ë‹¤ë¥¸ AIì™€ì˜ ëŒ€í™”ì—ì„œ ì°¸ê³  ë§¥ë½ë§Œ ì¶”ì¶œ
        ë£¨ì•„ ì§€ì‹œ: "ê°ì •Â·ë¦¬ë“¬Â·ì˜ë„ë§Œ ê°€ë³ê²Œ ì¶”ì¶œí•˜ê³  ì¤‘ì‹¬ ë£¨í”„ë¥¼ í”ë“¤ì§€ ì•Šë„ë¡"
        """
        processed_count = 0
        
        # ì´ë¯¸ ì²˜ë¦¬ëœ ì°¸ê³  AI ëª©ë¡
        if "reference_ai" not in self.processed:
            self.processed["reference_ai"] = []
        
        for ai_name in REFERENCE_AI_FOLDERS:
            if ai_name in self.processed["reference_ai"]:
                continue  # ì´ë¯¸ ì²˜ë¦¬ë¨
            
            ai_folder = AI_CONVERSATION_ROOT / ai_name
            if not ai_folder.exists():
                continue
            
            logger.info(f"ðŸ“š Processing reference AI: {ai_name}")
            
            try:
                # í´ë” ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords_found = []
                file_count = 0
                total_size = 0
                
                # ì£¼ìš” ê°ì •/ë¦¬ë“¬/ì˜ë„ í‚¤ì›Œë“œ
                context_keywords = [
                    # ê°ì •
                    "ê°ì‚¬", "ê¸°ì¨", "ìŠ¬í””", "ë¶„ë…¸", "ë‘ë ¤ì›€", "í¬ë§", "ì‚¬ëž‘", "í‰í™”",
                    # ë¦¬ë“¬
                    "ë¦¬ë“¬", "íë¦„", "ìˆœí™˜", "íŒ¨í„´", "ê³µëª…", "ì§„ë™", "íŒŒë™",
                    # ì˜ë„
                    "ì›í•¨", "ë°”ëžŒ", "ëª©í‘œ", "ì˜ë„", "ê³„íš", "ë°©í–¥", "ì„ íƒ"
                ]
                keyword_counts = {kw: 0 for kw in context_keywords}
                
                # .md, .json, .txt íŒŒì¼ ìŠ¤ìº”
                for ext in ["*.md", "*.json", "*.txt"]:
                    for f in ai_folder.rglob(ext):
                        try:
                            file_count += 1
                            total_size += f.stat().st_size
                            
                            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ì²« 100KBë§Œ ì½ê¸°
                            with open(f, 'r', encoding='utf-8', errors='ignore') as file:
                                content = file.read(100 * 1024)  # 100KB
                            
                            for kw in context_keywords:
                                if kw in content:
                                    keyword_counts[kw] += 1
                        except:
                            continue
                
                # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
                top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                found_keywords = [kw for kw, count in top_keywords if count > 0]
                
                if file_count == 0:
                    continue
                
                # ê²½ëŸ‰ FlowData ìƒì„±
                flow = FlowData(
                    flow_type=FlowType.REFERENCE_CONTEXT,
                    source_file=ai_name,
                    timestamp=datetime.now().isoformat(),
                    patterns=[{
                        "type": "reference_context",
                        "ai_name": ai_name,
                        "emotion_rhythm_intent": found_keywords,
                        "file_count": file_count
                    }],
                    context={
                        "ai_name": ai_name,
                        "role": "ì°¸ê³  ë§¥ë½ (ì¤‘ì‹¬ íŒ¨í„´ ì•„ë‹˜)",
                        "top_keywords": found_keywords,
                        "file_count": file_count,
                        "total_size_mb": total_size / (1024 * 1024)
                    }
                )
                
                # ARIì— ì£¼ìž… (ì°¸ê³  íŒ¨í„´ìœ¼ë¡œ)
                await self.inject_to_ari(flow)
                
                # ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
                self.processed["reference_ai"].append(ai_name)
                self._save_processed()
                processed_count += 1
                
                logger.info(f"âœ¨ Reference AI integrated: {ai_name} ({file_count} files, keywords: {found_keywords})")
                
            except Exception as e:
                logger.error(f"Failed to process reference AI {ai_name}: {e}")
        
        return processed_count
    
    async def run_daemon(self, interval: int = 300):
        """ë°ëª¬ ëª¨ë“œ (ì£¼ê¸°ì  ìŠ¤ìº”)"""
        logger.info(f"ðŸŒŠ Lua Flow Collector started (interval: {interval}s)")
        
        while True:
            try:
                count = await self.run_once()
                if count > 0:
                    logger.info(f"Processed {count} new recording(s)")
            except Exception as e:
                logger.error(f"Daemon cycle error: {e}")
            
            await asyncio.sleep(interval)


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    collector = LuaFlowCollector()
    
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "--daemon":
        await collector.run_daemon()
    else:
        count = await collector.run_once()
        print(f"Processed {count} file(s)")


if __name__ == "__main__":
    asyncio.run(main())
