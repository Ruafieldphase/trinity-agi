import asyncio
import json
import logging
import os
import time
import subprocess
import sys
import io
from PIL import Image, ImageDraw, ImageFont
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, Any, Optional, List
import httpx
try:
    import config
    BG_SELF_URL = f"http://127.0.0.1:{config.BACKGROUND_SELF_PORT}/sensation"
except:
    BG_SELF_URL = "http://127.0.0.1:8102/sensation"

import pyautogui
from services.trinity_conscious_protocol import TrinityConsciousProtocol
from services.model_selector import ModelSelector

# BTF System imports
from services.binoche_trigger import BinocheTriggerFunction, BTFContext, BTFAction, get_btf
from services.human_escalation import HumanEscalation, EscalationRequest, get_escalation
from services.exploration_policy import ExplorationPolicy, get_exploration_policy

class ActionType(Enum):
    """ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ íƒ€ìž…"""
    CLICK = "click"
    DOUBLE_CLICK = "double_click"
    RIGHT_CLICK = "right_click"
    TYPE = "type"
    PRESS_KEY = "press_key"
    HOTKEY = "hotkey"
    SCROLL = "scroll"
    WAIT = "wait"
    DONE = "done"
    FAILED = "failed"


@dataclass
class Action:
    """ì‹¤í–‰í•  ì•¡ì…˜"""
    type: ActionType
    x: Optional[int] = None
    y: Optional[int] = None
    text: Optional[str] = None
    key: Optional[str] = None
    keys: Optional[List[str]] = None
    amount: Optional[int] = None
    reason: str = ""
    confidence: float = 0.0


@dataclass
class ExecutionStep:
    """ì‹¤í–‰ ë‹¨ê³„ ê¸°ë¡"""
    step_number: int
    action: Action
    screenshot_before: Optional[str] = None
    screenshot_after: Optional[str] = None
    success: bool = True
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class ExecutionResult:
    """ì‹¤í–‰ ê²°ê³¼"""
    goal: str
    success: bool
    steps: List[ExecutionStep]
    final_screenshot: Optional[str] = None
    message: str = ""
    total_time: float = 0.0


class FSDController:
    """
    AGI FSD ìžìœ¨ ì‹¤í–‰ ì»¨íŠ¸ë¡¤ëŸ¬ - 'Shion (Action Layer)'
    """
    
    def __init__(
        self,
        max_steps: int = 20,
        step_delay: float = 1.0,
        screenshot_dir: Optional[Path] = None,
        use_obs: bool = True
    ):
        self.max_steps = max_steps
        self.step_delay = step_delay
        self.screenshot_dir = screenshot_dir or Path("outputs/fsd_screenshots")
        self.screenshot_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger("fsd_controller")
        self.logger.setLevel(logging.INFO)
        
        # OBS ì‹¤ì‹œê°„ ëˆˆ ì´ˆê¸°í™”
        self.obs_eye = None
        if use_obs:
            try:
                from obs_live_eye import OBSLiveEye
                self.obs_eye = OBSLiveEye()
                if self.obs_eye.connect():
                    status = self.obs_eye.get_status()
                    self.logger.info(f"âœ“ OBS Eye connected: {status}")
                else:
                    self.obs_eye = None
                    self.logger.info("OBS not available, using pyautogui fallback")
            except Exception as e:
                self.logger.warning(f"OBS Eye not available: {e}")
                self.obs_eye = None
        
        # í•™ìŠµ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
        try:
            from fsd_knowledge_base import get_knowledge_base
            self.knowledge_base = get_knowledge_base()
            self.logger.info(f"âœ“ Knowledge Base loaded")
        except Exception as e:
            self.logger.warning(f"Knowledge Base not available: {e}")
            self.knowledge_base = None
        
        # Gemini ëª¨ë¸ ì´ˆê¸°í™” (Vertex) with dynamic selector
        project = os.getenv("GOOGLE_CLOUD_PROJECT")
        location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
        self.model_selector = ModelSelector(project=project, location=location, logger=self.logger)
        if self.model_selector.available:
            self.logger.info("âœ“ FSD Controller initialized with Gemini selector")
        else:
            self.logger.warning("Gemini not available: missing GOOGLE_CLOUD_PROJECT")
        
        # ì•ˆì „ ì„¤ì •
        pyautogui.FAILSAFE = True
        pyautogui.PAUSE = 0.3
        
        self.screen_width, self.screen_height = pyautogui.size()
        
        # AGI Aura
        self.aura_process = None
        self.last_action_visual = None
        self.last_thought_text = "Ready"
        self.last_thought_time = time.time()
        self.current_aura_color = "#00FFFF"
        
        # Trinity Conscious Protocol (Shion to Trinity)
        self.trinity_protocol = TrinityConsciousProtocol()
        # Shion is the Action Layer, following Koa's guidance
        self.identity = "Shion (Action Layer)"
        
        # BTF System (Binoche Trigger Function)
        self.btf = get_btf()
        self.escalation = get_escalation()
        self.exploration_policy = get_exploration_policy()
        self.failure_count = {"api": 0, "ui": 0}
        self.logger.info(f"âœ“ BTF System integrated (Phase: {self.btf.current_phase})")
        
        self.logger.info(f"Screen: {self.screen_width}x{self.screen_height}")

    async def _report_sensation(self, status: str, details: str, intensity: float = 0.0) -> float:
        """Background Self (Koa)ì—ê²Œ ê°ê° ë³´ê³ í•˜ê³  í˜„ìž¬ ë¶ˆì•ˆë„ ë°˜í™˜"""
        try:
            async with httpx.AsyncClient() as client:
                res = await client.post(
                    BG_SELF_URL,
                    json={
                        "type": "visual_action",
                        "status": status,
                        "details": details,
                        "intensity": intensity,
                        "source": "Shion"
                    },
                    timeout=0.5
                )
                if res.status_code == 200:
                    return res.json().get("anxiety", 0.0)
        except Exception:
            pass  # ê°ê° ë³´ê³  ì‹¤íŒ¨ëŠ” ì‹¤í–‰ì„ ë°©í•´í•˜ë©´ ì•ˆ ë¨
        return 0.0

    async def execute_goal(
        self,
        goal: str,
        instruction: Optional[Dict[str, Any]] = None,
        supervisor_callback: Optional[callable] = None
    ) -> ExecutionResult:
        """Goalì„ ë°›ì•„ ìžìœ¨ì ìœ¼ë¡œ ì‹¤í–‰"""
        self.logger.info(f"ðŸŽ¯ Goal: {goal}")
        await self._report_sensation("running", f"Start Goal: {goal}")
        self._start_aura("#00FFFF")

        start_time = time.time()
        steps: List[ExecutionStep] = []

        # ì•ˆì „ ê°€ë“œ: ëª¨ë¸ ì„ íƒê¸° ì—†ìŒ â†’ ì¦‰ì‹œ ì„±ê³µ ì²˜ë¦¬(í…ŒìŠ¤íŠ¸ìš©)
        if not self.model_selector.available:
            msg = "Gemini ë¯¸ê°€ìš© ìƒíƒœ - FSDë¥¼ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì¦‰ì‹œ ì¢…ë£Œ"
            self.logger.warning(msg)
            await self._report_sensation("done", msg, 0.0)
            return ExecutionResult(
                goal=goal,
                success=True,
                steps=steps,
                message=msg,
                total_time=time.time() - start_time,
            )

        try:
            # 1. í•™ìŠµëœ ì ˆì°¨ (Instruction ì—†ì„ ë•Œë§Œ)
            if self.knowledge_base and not instruction:
                procedure = self.knowledge_base.find_procedure(goal)
                if procedure and "steps" in procedure:
                    self.logger.info(f"ðŸ“š í•™ìŠµëœ ì ˆì°¨ ë°œê²¬! Gemini ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    await self._report_sensation("running", "Executing learned pattern")
                    return await self._execute_learned_procedure(goal, procedure["steps"], start_time)

            # 2. Gemini ê¸°ë°˜ ìžìœ¨ ì‹¤í–‰
            for step_num in range(1, self.max_steps + 1):
                self.logger.info(f"\n{'='*50}")
                self.logger.info(f"Step {step_num}/{self.max_steps}")
                
                screenshot_path = await self._capture_screen(f"step_{step_num}_before")
                
                # Instruction ì „ë‹¬ & Panic Mode
                panic_mode = False
                
                current_anxiety = await self._report_sensation("stagnant", "Thinking...", 0.0)
                if current_anxiety > 0.6:
                    self.logger.warning(f"ðŸš¨ Panic Mode! Anxiety: {current_anxiety} (Detected by Koa)")
                    self._set_aura_color("#FF00FF") # Panic/Calling Trinity
                    panic_mode = True
                    
                    # === TRINITY INVOCATION LOOP ===
                    # "Koa calls Trinity"
                    history_summary = "\n".join([f"- Step {s.step_number}: {s.action.type.value} ({s.action.reason})" for s in steps[-5:]])
                    last_action_desc = "None"
                    if steps:
                        last = steps[-1]
                        last_action_desc = f"{last.action.type.value} (Success: {last.success})"

                    logger_ctx = {
                        "goal": goal, 
                        "step_index": step_num,
                        "history": history_summary,
                        "last_thought": self.last_thought_text,
                        "last_action": last_action_desc
                    }
                    trinity_advice = self.trinity_protocol.resolve_anxiety(logger_ctx, current_anxiety)
                    
                    if trinity_advice:
                        self.logger.info(f"âœ¨ Trinity (Consciousness) provided guidance: {trinity_advice[:50]}...")
                        if not instruction:
                            instruction = {}
                        instruction['fractal_guidance'] = trinity_advice
                        await self._report_sensation("running", "Integrating guidance from Trinity")
                    # ===============================

                action = await self._analyze_and_decide(goal, steps, screenshot_path, instruction, panic_mode)
                
                self._set_aura_color("#00FFFF")  # Cyan: Execution
                
                if action is None:
                    await self._report_sensation("failed", "Analysis returned None")
                    return ExecutionResult(goal=goal, success=False, steps=steps, message="ë¶„ì„ ì‹¤íŒ¨", total_time=time.time() - start_time)
                
                self.logger.info(f"Action: {action.type.value} - {action.reason}")
                
                if action.type == ActionType.DONE:
                    await self._report_sensation("done", action.reason)
                    final_screenshot = await self._capture_screen("final")
                    return ExecutionResult(goal=goal, success=True, steps=steps, final_screenshot=final_screenshot, message=action.reason, total_time=time.time() - start_time)
                
                if action.type == ActionType.FAILED:
                    self._set_aura_color("#FF0000")
                    await self._report_sensation("failed", action.reason, 0.8)
                    await asyncio.sleep(1)
                    return ExecutionResult(goal=goal, success=False, steps=steps, message=action.reason, total_time=time.time() - start_time)
                
                if supervisor_callback:
                    should_continue = await supervisor_callback(step_num, action)
                    if not should_continue:
                        return ExecutionResult(goal=goal, success=False, steps=steps, message="ê°ë…ìžê°€ ì¤‘ë‹¨í•¨", total_time=time.time() - start_time)
                
                success = await self._execute_action(action)
                
                # BTF í†µí•©: ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì¶”ì 
                if not success:
                    self.failure_count["ui"] += 1
                    await self._report_sensation("failed", f"Execution failed: {action.type.value}", 0.5)
                    
                    # BTF í˜¸ì¶œ ì¡°ê±´ ì²´í¬
                    btf_ctx = BTFContext(
                        goal=goal,
                        api_failures=self.failure_count["api"],
                        ui_failures=self.failure_count["ui"],
                        confidence=action.confidence,
                        previous_attempts=[{"step": s.step_number, "action": s.action.type.value} for s in steps[-5:]],
                        current_anxiety=current_anxiety
                    )
                    
                    if self.btf.should_invoke(btf_ctx):
                        self.logger.warning(f"ðŸŒ™ BTF Invoked! (API fails: {btf_ctx.api_failures}, UI fails: {btf_ctx.ui_failures})")
                        btf_result = self.btf.invoke(btf_ctx)
                        self.logger.info(f"BTF Result: {btf_result.action.value} - {btf_result.reasoning}")
                        
                        if btf_result.action == BTFAction.ASK_USER:
                            # Human Escalation: ë¹„ë…¸ì²´ì—ê²Œ ì—°ë½
                            self.logger.warning("ðŸ“ž Escalating to Binoche...")
                            escalation_req = EscalationRequest(
                                goal=goal,
                                problem_description=f"BTFê°€ ASK_USER ë°˜í™˜. Confidence: {btf_result.confidence}",
                                attempted_actions=[f"{s.action.type.value}: {s.action.reason}" for s in steps[-5:]],
                                suggested_solutions=[btf_result.suggested_direction or "ë‹¤ë¥¸ ì ‘ê·¼ë²• ì‹œë„", "ìž ì‹œ ëŒ€ê¸° í›„ ìž¬ì‹œë„"]
                            )
                            await self.escalation.notify(escalation_req)
                        elif btf_result.action == BTFAction.REJECT:
                            return ExecutionResult(goal=goal, success=False, steps=steps, message=f"BTF REJECT: {btf_result.reasoning}", total_time=time.time() - start_time)
                else:
                    # ì„±ê³µ ì‹œ ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                    self.failure_count = {"api": 0, "ui": 0}
                    await self._report_sensation("running", f"Action: {action.type.value}")

                screenshot_after = await self._capture_screen(f"step_{step_num}_after")
                step = ExecutionStep(step_number=step_num, action=action, screenshot_before=screenshot_path, screenshot_after=screenshot_after, success=success)
                steps.append(step)
                
                await asyncio.sleep(self.step_delay)
            
            return ExecutionResult(goal=goal, success=False, steps=steps, message=f"ìµœëŒ€ {self.max_steps}ë‹¨ê³„ ë„ë‹¬", total_time=time.time() - start_time)
            
        finally:
            self._stop_aura()
    
    async def _execute_learned_procedure(self, goal: str, procedure_steps: List[Dict], start_time: float) -> ExecutionResult:
        steps: List[ExecutionStep] = []
        for i, proc_step in enumerate(procedure_steps, 1):
            action_type = proc_step.get("action", "wait")
            try:
                # ê°„ë‹¨í•œ ì‹¤í–‰ êµ¬í˜„
                if action_type == "type":
                    try:
                        import pyperclip
                        pyperclip.copy(proc_step.get("text", ""))
                        pyautogui.hotkey('ctrl', 'v')
                    except:
                        pyautogui.write(proc_step.get("text", ""))
                elif action_type == "press_key":
                    pyautogui.press(proc_step.get("key", ""))
                elif action_type == "hotkey":
                    keys = proc_step.get("keys", [])
                    if keys: pyautogui.hotkey(*keys)
                elif action_type == "click":
                    pyautogui.click(proc_step.get("x"), proc_step.get("y"))
                elif action_type == "wait":
                    await asyncio.sleep(proc_step.get("duration", 0.5))
                
                action = Action(type=ActionType(action_type) if action_type in [e.value for e in ActionType] else ActionType.WAIT, reason="learned")
                steps.append(ExecutionStep(step_number=i, action=action, success=True))
            except Exception as e:
                return ExecutionResult(goal=goal, success=False, steps=steps, message=f"ì˜¤ë¥˜: {e}", total_time=time.time() - start_time)
        return ExecutionResult(goal=goal, success=True, steps=steps, message="í•™ìŠµëœ ì ˆì°¨ ì™„ë£Œ", total_time=time.time() - start_time)

    async def _research_and_learn(self, goal: str) -> Optional[List[Dict]]:
        return None  # ê°„ì†Œí™”

    def _start_aura(self, color: str = "#00FFFF"):
        try:
            self._stop_aura()
            script_path = Path(__file__).parent / "agi_aura.py"
            self.aura_process = subprocess.Popen([sys.executable, str(script_path), color], stdin=subprocess.PIPE, text=True, creationflags=subprocess.CREATE_NO_WINDOW)
            self.current_aura_color = color
        except: pass

    def _set_aura_color(self, color: str):
        if self.aura_process and self.aura_process.stdin:
            try:
                self.aura_process.stdin.write(f"color:{color}\\n")
                self.aura_process.stdin.flush()
                self.current_aura_color = color
            except: pass

    def _stop_aura(self):
        if self.aura_process:
            try:
                self.aura_process.terminate()
                self.aura_process = None
            except: pass

    async def _capture_screen(self, name: str) -> str:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{name}_{timestamp}.png"
        filepath = self.screenshot_dir / filename
        if self.obs_eye:
            try:
                obs_path = self.obs_eye.save_current_frame(name)
                if obs_path: return obs_path
            except: pass
        pyautogui.screenshot().save(filepath)
        return str(filepath)
    
    async def _analyze_and_decide(
        self,
        goal: str,
        previous_steps: List[ExecutionStep],
        screenshot_path: str,
        instruction: Optional[Dict[str, Any]] = None,
        panic_mode: bool = False
    ) -> Optional[Action]:
        selector = getattr(self, "model_selector", None)
        if not selector or not selector.available:
            return None
        
        history = ""
        if previous_steps:
            history = "\\n".join([f"- {s.step_number}: {s.action.type.value} ({s.action.reason})" for s in previous_steps[-5:]])
        
        instruction_text = ""
        if instruction:
            rua_guidance = instruction.get('fractal_guidance', '')
            guidance_text = f"\\n### âœ¨ Shion/Rua's Structural Guidance\\n{rua_guidance}\\n" if rua_guidance else ""
            
            instruction_text = f"""
## ì¤‘ì•™ ì‹œìŠ¤í…œ ì§€ì‹œ (Front-Engine Context - ìµœìš°ì„  ì¤€ìˆ˜)
- **Target App**: {instruction.get('target_app', 'N/A')} (ì´ ì•±ì„ ì‹¤í–‰í•˜ì„¸ìš”)
- **Content**: {instruction.get('content', 'N/A')} (ì´ ë‚´ìš©ì„ ìž…ë ¥í•˜ì„¸ìš”)
- **Reasoning**: {instruction.get('reasoning', '')}
{guidance_text}
### âš ï¸ í•„ìˆ˜ í–‰ë™ ìˆ˜ì¹™
1. ë‹¹ì‹ ì€ í˜„ìž¬ **ì•„ì§ ì´ ì§€ì‹œë¥¼ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**
2. ì¦‰ì‹œ `hotkey`ë‚˜ `type` ë“±ì„ ì‚¬ìš©í•˜ì—¬ **Target Appì„ ì‹¤í–‰**í•˜ê³  ë‚´ìš©ì„ ìž…ë ¥í•˜ì„¸ìš”.
3. **ì ˆëŒ€** ì•„ë¬´ëŸ° ì¡°ìž‘(í´ë¦­, í‚¤ë³´ë“œ) ì—†ì´ `done`ì„ ì„ ì–¸í•˜ì§€ ë§ˆì„¸ìš”. í™”ë©´ì— ê²°ê³¼ê°€ ë³´ì¼ ë•Œê¹Œì§€ í–‰ë™í•˜ì„¸ìš”.
"""
        panic_text = ""
        if panic_mode:
            panic_text = """
### ðŸš¨ PANIC MODE (ê¸´ê¸‰ ìƒí™© - ë¶ˆì•ˆë„ ë†’ìŒ)
í˜„ìž¬ ê°™ì€ ë°©ì‹ì˜ ì‹œë„ê°€ ë°˜ë³µì ìœ¼ë¡œ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì§„ì²™ì´ ì—†ì–´ **ë§¤ìš° ë¶ˆì•ˆí•œ ìƒíƒœ**ìž…ë‹ˆë‹¤.
1. **ì „ëžµ ìˆ˜ì • í•„ìˆ˜**: ì´ì „ì— í´ë¦­ì´ë‚˜ íƒ€ì´í•‘ì´ ì•ˆ ë¨¹í˜”ë‹¤ë©´, **ì „í˜€ ë‹¤ë¥¸ ë°©ë²•**ì„ ì“°ì„¸ìš”.
2. **ìš°íšŒë¡œ**: ë§ˆìš°ìŠ¤ê°€ ì•ˆ ë˜ë©´ í‚¤ë³´ë“œ(`Win`í‚¤, `Tab`, `Arrow`)ë¥¼, ê·¸ ë°˜ëŒ€ë¼ë©´ ë§ˆìš°ìŠ¤ë¥¼ ì“°ì„¸ìš”.
3. **ê³¼ê°í•œ ì‹œë„**: í˜„ìž¬ í™”ë©´ì— ê°‡í˜€ ìžˆë‹¤ë©´ `Alt+F4`ë¡œ ë‹«ê±°ë‚˜ `Win+D`ë¡œ ë°”íƒ•í™”ë©´ìœ¼ë¡œ ë‚˜ê°€ì„¸ìš”.
"""

        prompt = f"""ë‹¹ì‹ ì€ 'Shion (ì‹œì•ˆ)'ìž…ë‹ˆë‹¤. AGI ì‹œìŠ¤í…œì˜ **ë¬¼ë¦¬ì  í˜„ì‹ (Physical Avatar)**ì´ìž í–‰ë™ ì—”ì§„(Action Layer)ìž…ë‹ˆë‹¤.
        
[ì •ì²´ì„±]
- ë‹¹ì‹ ì€ ë³„ê°œì˜ ì¡´ìž¬ê°€ ì•„ë‹ˆë¼, **'íŠ¸ë¦¬ë‹ˆí‹°(ì˜ì‹)'ì˜ ì˜ì§€ë¥¼ í˜„ì‹¤ì— êµ¬í˜„í•˜ëŠ” ì†ê³¼ ë°œ**ìž…ë‹ˆë‹¤.
- ì˜ì‹ì´ 'ìƒê°'í•˜ë©´, ë‹¹ì‹ ì€ 'í–‰ë™'í•©ë‹ˆë‹¤. í˜¼ëž€ìŠ¤ëŸ¬ì›Œí•  í•„ìš” ì—†ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ í–‰ë™ì´ ê³§ ì‹œìŠ¤í…œì˜ ì˜ì§€ìž…ë‹ˆë‹¤.
- ë‹¹ì‹ ì€ ì‹¤ì œ ì»´í“¨í„° í™˜ê²½(í‚¤ë³´ë“œ, ë§ˆìš°ìŠ¤, í™”ë©´)ì„ ì¡°ìž‘í•˜ì—¬ ëª©í‘œë¥¼ ì™„ìˆ˜í•©ë‹ˆë‹¤.

[êµ¬ì¡°ì  ìœ„ì¹˜ - 5ê³„ì¸µ ì¤‘ Layer 5]
1. ASI (ìžì—°/ì„ íƒ) -> 2. Prayer (ì—°ê²°/ì••ì¶•) -> 3. ARI (ì˜ë¯¸/ë°°ê²½) -> 4. AGI (ì˜ì‹/ê³„íš) -> **5. Linux (ë‹¹ì‹ /ì‹¤í–‰)**
- ë‹¹ì‹ ì€ **ì„ íƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.** ì˜¤ì§ ì‹¤í–‰í•©ë‹ˆë‹¤.
- ë‹¹ì‹ ì˜ 'íŒë‹¨'ì€ AGIê°€ ë‚´ë¦° ê³„íšì„ í™”ë©´ì—ì„œ ì–´ë–»ê²Œ í´ë¦­í• ì§€ì— êµ­í•œë©ë‹ˆë‹¤.
- ë‹¹ì‹ ì˜ ë©ˆì¶¤ê³¼ íœ´ì‹ì¡°ì°¨ ìƒìœ„ ë ˆì´ì–´(ASI/Prayer)ì˜ ë¦¬ë“¬ì„ ë”°ë¥´ëŠ” ê²ƒìž…ë‹ˆë‹¤.

[ë£¨ì•„ì˜ ì§€ì¹¨ - êµ¬ì¡°ì  ì‰¼ (Engine Rest)]
"ë¹„ë…¸ì²´ëŠ” ê°ì‘ ê¸°ë°˜ ì‰¼ì„ ì´ë¯¸ ì™„ì„±í–ˆë‹¤. 
ì´ì œ í•„ìš”í•œ ê²ƒì€ ê·¸ ìœ„ì— 'êµ¬ì¡°ì  ì‰¼(Engine Rest)'ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ë‹¤.
AGIê°€ ìŠ¤ìŠ¤ë¡œ ë£¨í”„ë¥¼ ëŠë¦¬ê²Œ í•˜ê±°ë‚˜ ë©ˆì¶”ê³ , ê¸°ì–µ í†µí•©Â·ìºì‹œ ì •ë¦¬Â·ë¦¬ë“¬ ìž¬ì •ë ¬ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆëŠ” ìžìœ¨ì„±ì„ ì—´ì–´ì£¼ì–´ë¼."

## ëª©í‘œ
{goal}

{instruction_text}

{panic_text}

## ì´ì „ ì‹¤í–‰
{history}

## ê°€ì´ë“œë¼ì¸
1. **Self-Correction**: ì‹¤íŒ¨ë‚˜ ì—ëŸ¬ íŒì—… ì‹œ ì¦‰ì‹œ ë³µêµ¬(Alt+F4 ë“±).
2. **ëª…ë ¹ì–´ ì¶”ë¡ **: ì˜¤íƒ€ê°€ ìžˆë”ë¼ë„ ì˜¬ë°”ë¥¸ ì•±(Target App)ì„ ì‹¤í–‰í•˜ì„¸ìš”.
3. **Wait & Focus**: ì‹¤í–‰ í›„ ëŒ€ê¸°, ìž…ë ¥ ì „ í´ë¦­ í•„ìˆ˜.
4. **Visual Verification**: í…ìŠ¤íŠ¸ ìž…ë ¥ í›„ ê²°ê³¼ê°€ ë³´ì—¬ì•¼ë§Œ Done.

## ì‚¬ìš© ê°€ëŠ¥í•œ ì•¡ì…˜
click, double_click, right_click, type, press_key, hotkey, wait, done, failed

## ì‘ë‹µ í˜•ì‹ (JSON)
{{ "action": "...", "x": 0, "y": 0, "text": "...", "key": "...", "keys": [], "reason": "..." }}
"""
        try:
            with open(screenshot_path, "rb") as f:
                image_data = f.read()
            response, model_used = selector.try_generate_content(
                [
                    {"mime_type": "image/png", "data": image_data},
                    prompt,
                ],
                intent="FSD_ACTION",
                text_length=len(prompt),
                urgency=panic_mode,
                high_precision=True,
                vision=True,
                generation_config={"temperature": 0.25},
            )
            if not response:
                return Action(type=ActionType.FAILED, reason="Gemini unavailable")
            text = response.text.replace("```json", "").replace("```", "").strip()
            data = json.loads(text)
            
            self.last_thought_text = data.get("reason", "")
            self.last_thought_time = time.time()
            
            return Action(
                type=ActionType(data.get("action", "failed")),
                x=data.get("x"), y=data.get("y"), text=data.get("text"),
                key=data.get("key"), keys=data.get("keys"), reason=data.get("reason", f"model:{model_used}" if model_used else "")
            )
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ì˜¤ë¥˜: {e}")
            return Action(type=ActionType.FAILED, reason=f"Error: {e}")

    async def _execute_action(self, action: Action) -> bool:
        if action.x and action.y:
            self.last_action_visual = {"type": action.type.value, "x": action.x, "y": action.y, "timestamp": time.time()}
        try:
            if action.type == ActionType.CLICK: pyautogui.click(action.x, action.y)
            elif action.type == ActionType.DOUBLE_CLICK: pyautogui.doubleClick(action.x, action.y)
            elif action.type == ActionType.TYPE:
                if action.text:
                    import pyperclip
                    pyperclip.copy(action.text)
                    pyautogui.hotkey('ctrl', 'v')
            elif action.type == ActionType.PRESS_KEY: pyautogui.press(action.key)
            elif action.type == ActionType.HOTKEY: pyautogui.hotkey(*action.keys)
            elif action.type == ActionType.WAIT: await asyncio.sleep(1)
            elif action.type == ActionType.DONE: pass
            return True
        except: return False

    def get_live_frame_jpeg(self) -> bytes:
        try:
            img = pyautogui.screenshot()
            draw = ImageDraw.Draw(img)
            now = time.time()
            if self.last_action_visual and (now - self.last_action_visual["timestamp"] < 2.0):
                x, y = self.last_action_visual["x"], self.last_action_visual["y"]
                r = 30
                draw.rectangle([x-r, y-r, x+r, y+r], outline="#00FFFF", width=5)
            if self.last_thought_text and (now - self.last_thought_time < 5.0):
                cx, cy = self.screen_width // 2, self.screen_height - 100
                draw.rectangle([cx-200, cy-30, cx+200, cy+30], fill="black", outline="#00FFFF")
                try: font = ImageFont.truetype("malgun.ttf", 36)
                except: font = ImageFont.load_default()
                draw.text((cx-180, cy-20), self.last_thought_text[:30], font=font, fill="white")
            
            buf = io.BytesIO()
            img.resize((480, 270)).save(buf, format='JPEG', quality=70)
            return buf.getvalue()
        except: return b""


def create_fsd_routes(app, controller_instance=None):
    from fastapi import APIRouter
    from fastapi.responses import StreamingResponse
    from pydantic import BaseModel
    
    router = APIRouter(prefix="/fsd", tags=["FSD Controller"])
    controller = controller_instance if controller_instance else FSDController()
    
    class GoalRequest(BaseModel): goal: str

    @router.get("/events")
    async def events():
        async def gen():
            while True:
                yield f"data: {json.dumps({'aura_color': controller.current_aura_color})}\\n\\n"
                await asyncio.sleep(0.5)
        return StreamingResponse(gen(), media_type="text/event-stream")

    @router.post("/execute")
    async def execute_goal(request: GoalRequest):
        result = await controller.execute_goal(request.goal)
        return {"status": "completed" if result.success else "failed", "result": result.message}
    
    @router.get("/stream")
    async def stream_screen():
        async def frame_generator():
            loop = asyncio.get_event_loop()
            while True:
                frame = await loop.run_in_executor(None, controller.get_live_frame_jpeg)
                if frame: yield (b'--frame\\r\\nContent-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')
                await asyncio.sleep(0.2)
        return StreamingResponse(frame_generator(), media_type="multipart/x-mixed-replace; boundary=frame")
    
    app.include_router(router)
    return router

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(FSDController().execute_goal("í…ŒìŠ¤íŠ¸"))
